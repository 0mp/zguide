Notes for The Guide

    
Chapter 6 topics:

- heartbeating, credit-based flow control, file transfer, serialization



- reliable pipelines, presence & discovery using UDP, bridging (like VTX), security, 
- contribution process
- who owns 0MQ
- forks and competitors - AMQP, Stomp, xs
- web interfaces - NullMQ



Using network buffers - such as 0MQ's queues - has a disadvantage when you are sending data to many readers. Your writer will block when the buffer is full. To avoid this, you can use non-blocking writes, and poll for sockets that are 'ready for writing'. But here's an alternative that dispenses with high-water marks and blocking writes. It's called "credit-based flow control".

The example below runs on 0MQ using the CZMQ API. The principles are portable to any messaging system. I'll let the code speak for itself.

[[code]]
//
//  Credit based flow control example
//
//  We start some clients that talk to a single server via a DEALER
//  to ROUTER setup. Clients say hello to the server and then start
//  to receive random data. The server sends as fast as it can, but
//  only within credit window created by client.
//
#include "czmq.h"

#define NBR_CLIENTS     1

//  The TRANSIT_TOTAL size defines the total data in transit,
//  covering 0MQ send and recv queues, TCP send and recv buffers,
//  and packets in flight on the network. The client starts by
//  sending TRANSIT_TOTAL credit to the server, and thereafter
//  sends TRANSIT_SLICE credit after receiving TRANSIT_SLICE bytes.

#define TRANSIT_TOTAL   1024 * 1024
#define TRANSIT_SLICE   TRANSIT_TOTAL / 4

//  We assert that the flow-control mechanism works by setting a
//  HWM on the server send queue, and sequencing messages. If the
//  queue hits HWM for any client, 0MQ will drop messages, as this
//  is the exception strategy for ROUTER sockets. The client can
//  detect this and abort.
//
//  Difficulty: 0MQ counts messages, not bytes. So HWM is not an
//  accurate measure. To solve this we batch small messages, and
//  fragment larger messages into blocks of FRAGMENT_SIZE octets.
//
//  For the example we simply generate FRAGMENT_SIZE messages.
//  In a more cynical test we would batch and fragment on sending.
//  But, once flow control works, we don't need the HWM at all.

#define FRAGMENT_SIZE   65536

//  Knowing the TRANSIT_TOTAL and the FRAGMENT_SIZE, we can set the
//  HWM to be (TRANSIT_TOTAL / FRAGMENT_SIZE).

#define SERVER_HWM      TRANSIT_TOTAL / FRAGMENT_SIZE

//  -------------------------------------------------------------------
//  Put a long integer to a network buffer

#define PUT_LONG(buffer, value) { \
    (buffer) [0] = (byte) (((value) >> 24) & 255); \
    (buffer) [1] = (byte) (((value) >> 16) & 255); \
    (buffer) [2] = (byte) (((value) >> 8)  & 255); \
    (buffer) [3] = (byte) (((value))       & 255); \
    }

//  Put a long long integer to the network buffer
#define PUT_LLONG(buffer, value) { \
    (buffer) [0] = (byte) (((value) >> 56) & 255); \
    (buffer) [1] = (byte) (((value) >> 48) & 255); \
    (buffer) [2] = (byte) (((value) >> 40) & 255); \
    (buffer) [3] = (byte) (((value) >> 32) & 255); \
    (buffer) [4] = (byte) (((value) >> 24) & 255); \
    (buffer) [5] = (byte) (((value) >> 16) & 255); \
    (buffer) [6] = (byte) (((value) >> 8)  & 255); \
    (buffer) [7] = (byte) (((value))       & 255); \
    }

//  Get a long integer from a network buffer

#define GET_LONG(buffer) \
      ((buffer) [0] << 24) \
    + ((buffer) [1] << 16) \
    + ((buffer) [2] << 8)  \
    +  (buffer) [3]

//  Get a long long integer from the network buffer
#define GET_LLONG(buffer) \
      ((int64_t) ((buffer) [0]) << 56) \
    + ((int64_t) ((buffer) [1]) << 48) \
    + ((int64_t) ((buffer) [2]) << 40) \
    + ((int64_t) ((buffer) [3]) << 32) \
    + ((int64_t) ((buffer) [4]) << 24) \
    + ((int64_t) ((buffer) [5]) << 16) \
    + ((int64_t) ((buffer) [6]) << 8) \
    +  (int64_t) ((buffer) [7])


//  -------------------------------------------------------------------
//  Client task

static void *
client_task (void *args)
{
    zctx_t *ctx = zctx_new ();
    void *dealer = zsocket_new (ctx, ZMQ_DEALER);
    zsocket_connect (dealer, "tcp://127.0.0.1:10001");

    //  Start by sending TRANSIT_TOTAL credit to server
    zframe_t *frame = zframe_new (NULL, 4);
    PUT_LONG (zframe_data (frame), TRANSIT_TOTAL);
    zframe_send (&frame, dealer, 0);

    //  Now consume and verify incoming messages and refresh
    //  credit asynchronously as needed
    int64_t expected_seq = 0;
    int received = 0;

    while (TRUE) {
        zmsg_t *msg = zmsg_recv (dealer);
        if (!msg)
            break;

        //  Message has two frames, sequence number and body
        zframe_t *sequence = zmsg_pop (msg);
        zframe_t *content = zmsg_pop (msg);
        assert (content);
        int64_t current_seq = GET_LLONG (zframe_data (sequence));
        if (current_seq != expected_seq) {
            printf ("E: server dropped %d messages, exit (%d/%d)\n",
                (int) (current_seq - expected_seq),
                (int) current_seq, (int) expected_seq);
            exit (1);
        }
        expected_seq++;

        //  Count received data, send top-up credit if needed
        received += zframe_size (content);
        if (received > TRANSIT_SLICE) {
            received -= TRANSIT_SLICE;
            zframe_t *frame = zframe_new (NULL, 4);
            PUT_LONG (zframe_data (frame), TRANSIT_SLICE);
            zframe_send (&frame, dealer, 0);
        }
        zframe_destroy (&sequence);
        zframe_destroy (&content);
        zmsg_destroy (&msg);

        //  Sleep for some random interval up to 100 msecs
        zclock_sleep (randof (10));
    }
    zctx_destroy (&ctx);
    return NULL;
}


//  -------------------------------------------------------------------
//  Server task

//  Clients are represented by this data structure
typedef struct {
    zframe_t *identity;
    int credit;
    int64_t sequence;
} client_t;

static void *
server_task (void *args)
{
    zctx_t *ctx = zctx_new ();
    void *router = zsocket_new (ctx, ZMQ_ROUTER);
    zsockopt_set_hwm (router, SERVER_HWM);
    zsocket_bind (router, "tcp://*:10001");

    //  We'll hold the clients on a simple list
    zlist_t *clients = zlist_new ();

    //  We're purely driven by input events
    while (1) {
        zmsg_t *msg = zmsg_recv (router);
        if (!msg)
            break;

        //  PROCESS CLIENT CREDITS
        //  -------------------------------------------------------
        //  Only message we accept from clients is a credit message
        //  - frame data is amount of credit as 4-byte net longint

        zframe_t *client_frame = zmsg_pop (msg);
        zframe_t *credit_frame = zmsg_pop (msg);
        assert (credit_frame);
        int credit = GET_LONG (zframe_data (credit_frame));
        zframe_destroy (&credit_frame);

        //  Look for pre-existing client with this identity
        client_t *client = (client_t *) zlist_first (clients);
        while (client) {
            if (zframe_eq (client->identity, client_frame))
                break;
            client = (client_t *) zlist_next (clients);
        }
        //  If this client is new, create an object and save it
        if (client == NULL) {
            client = (client_t *) zmalloc (sizeof (client_t));
            client->identity = client_frame;
            zlist_append (clients, client);
        }
        else
            zframe_destroy (&client_frame);

        //  Accumulate credit for this client
        client->credit += credit;
        zmsg_destroy (&msg);

        //  DISPATCH TO CLIENTS
        //  -------------------------------------------------------
        //  We now stream data to all clients with available credit
        //  until their credit is used up. We then wait for clients
        //  to send us new credit.

        //  Process entire client list in turn
        client = (client_t *) zlist_first (clients);
        while (client) {
            while (client->credit >= FRAGMENT_SIZE) {
                int msgsize = FRAGMENT_SIZE + randof (1000) - randof (1000);
                zframe_t *sequence = zframe_new (NULL, 8);
                zframe_t *content = zframe_new (NULL, msgsize);
                PUT_LLONG (zframe_data (sequence), client->sequence);
                client->sequence++;

                //  Send fragment of data to the client
                zframe_send (&client->identity, router,
                             ZFRAME_MORE + ZFRAME_REUSE);
                zframe_send (&sequence, router, ZFRAME_MORE);
                zframe_send (&content, router, 0);

                //  Discount credit
                client->credit -= msgsize;
            }
            client = (client_t *) zlist_next (clients);
        }
    }
    zctx_destroy (&ctx);
    return NULL;
}

int main (void)
{
    //  Create threads
    zctx_t *ctx = zctx_new ();
    printf ("I: starting server...\n");
    zthread_new (server_task, NULL);

    printf ("I: starting %d clients...\n", NBR_CLIENTS);
    int client_nbr;
    for (client_nbr = 0; client_nbr < NBR_CLIENTS; client_nbr++)
        zthread_new (client_task, NULL);

    while (!zctx_interrupted)
        sleep (1);

    zctx_destroy (&ctx);
    return 0;
}
[[/code]]








- pubsub ordering
    - N-to-N ordering guarantees
    - single-stream using a broker


- collection of patterns
    Least Recently Used
    Asynchronous Client-Server
    Lazy Pirate
    Simple Pirate
    Paranoid Pirate
    Clone

- how to make an error logging console

- how to make a TCP-to-0MQ bridge, with examples
http://lists.zeromq.org/pipermail/zeromq-dev/2011-March/010186.html
- FD_EVENT integration


- router trees
 (federation)

- cross-over router
    - send to, send from
    - flips addresses, gives you reply model

    //  If socket is a ROUTER, get identity first
    int socket_type;
    size_t type_size = sizeof (socket_type);
    zmq_getsockopt (socket, ZMQ_TYPE, &socket_type, &type_size);
    if (socket_type == ZMQ_ROUTER) {
        zmq_msg_t mesg;
        zmq_msg_init (&mesg);
        zmq_msg_recv (&mesg, socket, 0);
    }

- explain why callbacks are a pita
    - what thread do they run in?
    - how do you connect them to main app thread
    - how do you wait, do you sleep, call something, etc.?

- integrating 0MQ into TCP loops: http://lists.zeromq.org/pipermail/zeromq-dev/2011-March/010186.html

Then we can use this to start building some reusable pieces:

- timer device
- name service
- custom load-balancing
- custom publish-subscribe
- stateful publish-subscribe
- reliable request-reply
- logging device


+++ Presence Detection

- peers come and go
- if we want to route to them explicitly, we need to know if they are present
- heartbeating, liveness, etc.
- fresh data, failover, etc.
- purging old identities from routing tables
- example of eight robots and console
- robots come and go...


+++ A ZeroMQ Name Service

- name service
    translate logical name into connect or bind string
    service runs locally, connect via ipc://zns
    gets name updates asynchronously from central server
    also local zns lookup file
    using zpl syntax
    pubsub state / refresh example

how to map names?
    - XXX -> tcp://lo:5050 if I'm on server 1
    - XXX -> tcp://somename:5050
    -> does ZMQ do host lookup?  Doesn't seem like it...
    -> resolve host...


+++ Pipelines

- does PUSH block if there are no PULL sockets ready?
- how do maintain a queue?

+++ File Transfer

example of file transfer

+++ Generating Identities


+++ Setting Queue Limits

- setting queue limits prevents nodes from overflowing memory
- by default 0MQ does not set any limits
- example of application that will overflow memory
    - publish to subscribe socket but don't read
- now fix this by setting queue limit, ZMQ_HWM
- actual behaviour with automagic LWM
- how this works on different socket types
    - 'exception' on each socket type, from man page
- adding capacity for disk offload, ZMQ_SWAP
- creating persistent queues using identity
    - example of HWM 1 plus massive SWAP


+++ Reliable Request-Reply

We'll create a reliable request-reply application that uses XREQ and XREP and a simple resend mechanism.  When this works between two peers we'll show how it scales across a request-reply broker to effectively create edge-to-edge reliability.  We'll also open up the message format for request-reply and explore identities in horrible detail.

+++ Configuration Distribution

We'll look at how to dynamically configure a network of devices using a central configuration broker.

+++ Logging Subsystem

many applications
many subscribers
broker in the middle
persistent logfiles
replay via subscribe socket


+++ Failover and Recovery

We'll look at how to handle crashes in a distributed architecture.  For each pattern there is an ideal architecture, and we'll explore each of these with worked examples.

+++ Encrypted Publish-Subscribe

We'll look at how to secure pubsub data against snooping.  The actual technique involves out-of-band exchange of keys, symmetric encryption, and a broker that helps the thing along.  Hopefully all fairly easy to make, as usual, using 0MQ.

+++ Building a Multicast Bus

We'll now look at how the pgm: and epgm: protocols work.  With pgm:, the network switch basically acts as a hardware FORWARDER device.

++++ Customized Publish-Subscribe

- use identity to route message explicitly to A or B
- not using PUBSUB at all but XREP/????
    - limitations: no multicast, only TCP
    - how to scale with devices...

When a client activates, it chooses a random port that is not in use and creates a SUB socket listening for all traffic on it. The client then sends a message via REQ to the publisher containing the port that it is listening on. The publisher receives this message, acknowledges it, and creates a new pub socket specific to that client. All published events specific to this client go out that socket.

When the client deactivates, it sends a message to the publisher with the port to deactivate and close.

You end up creating a lot more PUB sockets on your server end and doing all of the filtering at the server. This sounds acceptable to you.

I didn't need to do this to avoid network bandwidth bottlenecks; I created this to enforce some security and entitlements.

+++ A Clock Device

We'll look at various ways of building a timer into a network.   A clock device sends out a signal (a message) at more or less precise intervals so that other nodes can use these signals for internal timing.

+++ Serializing Data

Examples of using Protocol Buffers and other options.

[!--
- ipc://name
 - connects two processes on a single box
 - supports all messaging patterns
 - typical use case is for multithreading apps
 - runs on Unix domain sockets (not available on Windows, OpenVMS)
 - permissions issues:

> Since I want to work under /tmp, this all had to be done programatically. My
> server now mkdir -p's a socket subdirectory and chmod 777's it. The server
> creates and binds the socket in that folder, and then chmod 777's it. The
> server must be run as root (which is fine for my project luckily). If it is
> run as a normal user, the client's still timeout.

- tcp://ipaddress:port
 - bind to address:port
 - bind to *:5555
 - localhost
  - also bind to interface: lo:port, eth1:port, etc.
  - Linux: eth1, eth2, eth3
  - Mac OS X: en1, en2, en3
  - Solaris: e1000g, etc.
 - connect to remote address: host:port

- pgm://address;multicastgroup:port
 - address can be interface name
 - requires decent hardware support
 - means enterprise level switches with IGNP snooping
 - some routers also support PGM
 - runs over IP, requires root privileges
 - more standard
 - rate-limited protocol, sender must define bandwidth
 - pgm is currently broken

- epgm://address;multicastgroup:port
 - encapsulated in UDP packets
 - requires decent hardware support
 - does not require root access
 - non-standard to pgm

- add peer example
   - exclusive lock on peer
   - for owning other party
   - solve reverse connection
 - e.g. to cross firewall
 - you need to add a bind to allow the client to accept a connection
 - could be usecase for EXCLUSIVE socket

XREQ is like PUSH+PULL, XREP is like PUSH+PULL+routing

--]


* How do we tunnel 0MQ connections over 0MQ connections?
    - e.g. to get 3-4 ports into DMZ via one single port
    - two devices, one reads, one writes
    - over other protocols: HTTP, SSH, etc...?
    - acts as split device, ...

I highly recommend that you try out the simpler topology and *verifying* that 0mq cannot keep up with your message rates when publishing all data to all clients. With smart topics the client can reject the data *very* fast. A 1 GB or 10 Gb switched network can also move quite a bit of data without a lot of blocking behavior in the hardware too. You may be able to save yourself a lot of unnecessary work, so just try it.



- explain in
- debugging message flow
    - using a queue device that prints out message parts
    - debugging versions of devices...

- heartbeating
    - set HWM to 1 message only

[!--

[!--
+++ The Wire-Level Protocol

- writing a thin client in JavaScript

+++ Building a Language Binding

+++ Tuning 0MQ

High performance multiple i/o threads
 - for HP applications, several threads
 - then, set affinity on sockets

+++ Contributing to 0MQ


+++ What's Missing from 0MQ



- handling crashing peers
- non-blocking send/recv
    EAGAIN
- reliable pub
    - pub connects to all subs
    - subs send request to pub
- reliable messaging over xreq / xrep
- how to do timer events
- there is no way to disconnect; second connection creates two
    - need to destroy the socket
- how to send multipart job and execute / cancel




Here is, I think, how to build an authenticated pubsub service using ØMQ.  The design handles authentication, arbitrary routing criteria, and flushing of dead client connections.

You will use ZMQ_XREQ at the client side and ZMQ_XREP at the service side.  There is a simple protocol between client and server that is loosely modelled on HTTP:

* Client sends messages containing: identity (ØMQ adds this automatically), authentication credentials, and subscription criteria.
* Server sends messages containing: identity (automatic), error/success code, and content.

The server has two main data structures:

* A connection table that stores authenticated identities and timestamps.  Each identity corresponds to a client connection.
* A routing table that maps message keys (e.g. topic value) to identities.  There are techniques for doing [http://www.zeromq.org/whitepapers:message-matching high speed message matching].



0MQ Quickstarter

- building & installing
- performance for language
- basic examples
- socket types
- transports
- main patterns
- problem solving
-> translated into different programming languages
