++ Chapter 2 - Intermediate Stuff

+++ The Zen of Zero

The Ø in 0MQ is all about tradeoffs. On the one hand this strange name lowers 0MQ's visibility on Google and Twitter.  On the other hand it annoys the heck out of some Danish folk who write us things like "ØMG røtfl", and "//Ø is not a funny looking zero!//" and "//Rødgrød med Fløde!//", which is apparently an insult that means "may your neighbours be the direct descendents of Grendel!"  Seems like a fair trade.

Originally the zero in 0MQ was meant as "zero broker" and (as close to) "zero latency" (as possible).  In the meantime it has come to cover different goals: zero-copy, zero administration, zero cost, zero waste.  More generally, "zero" refers to the culture of minimalism that permeates the project.  We add power by removing complexity rather than exposing new functionality.

+++ The Socket API

To be perfectly honest, 0MQ does a kind of switch-and-bait on you.  Which we don't apologize for, it's for your own good and hurts us more than it hurts you.  It presents a familiar BSD socket API but that hides a bunch of message-processing machines that will slowly fix your world-view about how to design and write distributed software.

Sockets are the de-facto standard API for network programming, as well as being useful for stopping your eyes from falling onto your cheeks.  One thing that makes 0MQ especially tasty to developers is that it uses a standard socket API.  Kudos to Martin Sustrik for pulling this idea off.  It turns "Message Oriented Middleware", a phrase guaranteed to send the whole room off to Catatonia, into "Extra Spicy Sockets!" which leaves us with a strange craving for pizza, and a desire to know more.

Like a nice pepperoni pizza, 0MQ sockets are easy to digest.  Sockets have a life in four parts, just like BSD sockets:

* Creating and destroying sockets, which go together to form a karmic circle of socket life (see zmq_socket(3), zmq_close(3)).
* Configuring sockets by setting options on them and checking them if necessary (see zmq_setsockopt(3), zmq_getsockopt(3)).
* Plugging sockets onto the network topology by creating 0MQ connections to and from them (see zmq_bind(3), zmq_connect(3)).
* Using the sockets to carry data by writing and receiving messages on them (see zmq_send(3), zmq_recv(3)).

Which looks like this:

[[code type="C"]]
void *mousetrap;

//  Create socket for catching mice
mousetrap = zmq_socket (context, ZMQ_PULL);

//  Configure the socket
int64_t jawsize = 10000;
zmq_setsockopt (mousetrap, ZMQ_HWM, &jawsize, sizeof jawsize);

//  Plug socket into mouse hole
zmq_connect (mousetrap, "tcp://192.168.55.221:5001");

//  Wait for juicy mouse to arrive
zmq_msg_t mouse;
mouse = zmq_msg_init (&mouse);
zmq_recv (mousetrap, &mouse, 0);
//  Destroy the mouse
zmq_msg_close (&mouse);

//  Destroy the socket
zmq_close (mousetrap);
[[/code]]

Note that sockets are always void pointers, and messages (which we'll come to very soon) are structures.  So in C you pass sockets as-such, but you pass addresses of messages in all functions that work with messages, like zmq_send(3) and zmq_recv(3).  As a mnemonic, realize that "in 0MQ all ur sockets are belong to us", but messages are things you actually own in your code.

Creating, destroying, and configuring sockets works as you'd expect for any object.  But remember that 0MQ is an asynchronous, elastic fabric.  This has some impact on how we plug sockets into the network topology, and how we use the sockets after that.

+++ Plugging Sockets Into the Topology

To create a connection between two nodes you use zmq_bind(3) in one node, and zmq_connect(3) in the other.   As a general rule of thumb, the node which does zmq_bind(3) is a "server", sitting on a well-known network address, and the node which does zmq_connect(3) is a "client", with unknown or arbitrary network addresses.  Thus we say that we "bind a socket to an endpoint" and "connect a socket to an endpoint", the endpoint being that well-known network address.

0MQ connections are somewhat different from old-fashioned TCP connections.  The main notable differences are:

* They go across an arbitrary transport (inproc:, ipc:, tcp:, pgm: or epgm:).  See zmq_inproc(7), zmq_ipc(7), zmq_tcp(7), zmq_pgm(7), and zmq_epgm(7).
* They exist when a client does zmq_connect(3) to an endpoint, whether or not a server has already done zmq_bind(3) to that endpoint.
* They are asynchronous, and have queues that magically exist where and when needed.
* They may express a certain "messaging pattern", according to the type of socket used at each end.
* One socket may have many outgoing and many incoming connections.
* There is no zmq_accept() method.  When a socket is bound to an endpoint it automatically starts accepting connections.
* Your application code cannot work with these connections directly; they are encapsulated under the socket.

Many architectures follow some kind of client-server model, where the server is the component that is most stable, and the clients are the components that are most dynamic, i.e. they come and go the most.  There are sometimes issues of addressing: servers will be visible to clients, but not necessarily vice-versa.  So mostly it's obvious which node should be doing zmq_bind(3) (the server) and which should be doing zmq_connect(3) (the client).  It also depends on the kind of sockets you're using, with some exceptions for unusual network architectures.  We'll look at socket types later.

Now, imagine we start the client //before// we start the server.  In traditional networking we get a big red Fail flag.  But 0MQ lets us start and stop pieces arbitrarily.  As soon as the client node does zmq_connect(3) the connection exists and that node can start to write messages to the socket.  At some stage (hopefully before messages queue up so much that they start to get discarded, or the client blocks), the server comes alive, does a zmq_bind(3) and 0MQ starts to deliver messages.

A server node can bind to many endpoints and it can do this using a single socket.  This means it will accept connections across different transports:

[[code type="C"]]
zmq_bind (socket, "tcp://*:5555");
zmq_bind (socket, "tcp://*:9999");
zmq_bind (socket, "ipc://myserver");
[[/code]]

You cannot bind to the same endpoint twice, that will cause an exception.

Each time a client node does a zmq_connect(3) to any of these endpoints, the server node's socket gets another connection.  There is no inherent limit to how many connections a socket can have.  A client node can also connect to many endpoints using a single socket.

In most cases, which node acts as client, and which as server, is about network topology rather than message flow.  However, there //are// cases (resending when connections are broken) where the same socket type will behave differently if it's a server or if it's a client.

What this means is that you should always think in terms of "servers" as stable parts of your topology, with more-or-less fixed endpoint addresses, and "clients" as dynamic parts that come and go.  Then, design your application around this model.  The chances that it will "just work" are much better like that.

Sockets have types.  The socket type defines the semantics of the socket, its policies for routing messages inwards and outwards, queueing, etc.  You can connect certain types of socket together, e.g. a publisher socket and a subscriber socket.  Sockets work together in "messaging patterns".  We'll look at this in more detail later.

It's the ability to connect sockets in these different ways that gives 0MQ its basic power as a message queuing system.  There are layers on top of this, such as devices and topic routing, which we'll get to later.  But essentially, with 0MQ you define your network architecture by plugging pieces together like a child's construction toy.

+++ Using Sockets to Carry Data

To send and receive messages you use the zmq_send(3) and zmq_recv(3) methods.  The names are conventional but 0MQ's I/O model is different enough from TCP's model that you will need time to get your head around it.

[[code type="textdiagram"]]
             +------------+
             |            |
             |    Node    |
             |            |
             +------------+
             |   Socket   |
             \------------/
                   ^
                   |
                1 to 1
                   |
                   v
             /------------\
             |   Socket   |
             +------------+
             |            |
             |    Node    |
             |            |
             +------------+


   Figure # - TCP sockets are 1 to 1
[[/code]]

Let's look at the main differences between TCP sockets and 0MQ sockets when it comes to carrying data:

* 0MQ sockets carry messages, rather than bytes (as in TCP) or frames (as in UDP).  A message is a length-specified blob of binary data.  We'll come to messages shortly, their design is optimized for performance and thus somewhat tricky to understand.
* 0MQ sockets do their I/O in a background thread.  This means that messages arrive in a local input queue, and are sent from a local output queue, no matter what your application is busy doing.  These are configurable memory queues, by the way.
* 0MQ sockets can, depending on the socket type, be connected to (or from, it's the same) many other sockets.  Where TCP emulates a one-to-one phone call, 0MQ implements one-to-many (like a radio broadcast), many-to-many (like a post office), many-to-one (like a mail box), and even one-to-one.
* 0MQ sockets can send to many endpoints (creating a fan-in model), or receive from many endpoints (creating a fan-out model).

[[code type="textdiagram"]]
        +------------+           +------------+
        |            |           |            |
        |    Node    |           |    Node    |
        |            |           |            |
        +------------+           +------------+
        |   Socket   |           |   Socket   |
        \------------/           \------------/
             | |                        :
     1 to N  | +------------------------+
     Fan out |                          |
             +------------------------+ | N to 1
             |                        | | Fan in
             v                        v v
        /------------\           /------------\
        |   Socket   |           |   Socket   |
        +------------+           +------------+
        |            |           |            |
        |    Node    |           |    Node    |
        |            |           |            |
        +------------+           +------------+


           Figure # - 0MQ sockets are N to N
[[/code]]

So writing a message to a socket may send the message to one or many other places at once, and conversely, one socket will collect messages from all connections sending messages to it.  The zmq_recv(3) method uses a fair-queuing algorithm so each sender gets an even chance.

The zmq_send(3) method does not actually send the message to the socket connection(s).  It queues the message so that the I/O thread can send it asynchronously.  It does not block except in some exception cases.  So the message is not necessarily sent when zmq_send(3) returns to your application.  If you created a message using zmq_msg_init_data(3) you cannot reuse the data or free it, otherwise the I/O thread will rapidly find itself writing overwritten or unallocated garbage.  This is a common mistake for beginners.  We'll see a little later how to properly work with messages.

+++ I/O Threads

We said that 0MQ does I/O in a background thread.  One I/O thread (for all sockets) is sufficient for all but the most extreme applications.  This is the magic '1' that we use when creating a context, meaning "use one I/O thread":

[[code type="C"]]
void *context;
context = zmq_init (1);
[[/code]]

There is a major difference between a 0MQ application and a conventional networked application, which is that you don't create one socket per connection. One socket handles all incoming and outcoming connections for a particular point of work.  E.g. when you publish to a thousand subscribers, it's via one socket.  When you distribute work among twenty services, it's via one socket.  When you collect data from a thousand web applications, it's via one socket.

This has a fundamental impact on how you write applications.  A traditional networked application has one process or one thread per remote connection, and that process or thread handles one socket.  0MQ lets you collapse this entire structure into a single thread, and then break it up as necessary for scaling.

+++ The Messaging Patterns

Underneath the brown paper wrapping of 0MQ's socket API lies the world of messaging patterns.  If you have a background in enterprise messaging, these will be vaguely familiar.  But to most 0MQ newcomers they are a surprise, we're so used to the TCP paradigm where a socket represents another node.

Let's recap briefly what 0MQ does for you.  It delivers blobs of data (messages) to nodes, quickly and efficiently.  You can map nodes to threads, processes, or boxes.  It gives your applications a single socket API to work with, no matter what the actual transport (like in-process, inter-process, TCP, or multicast).  It automatically reconnects to peers as they come and go.  It queues messages at both sender and receiver, as needed.  It manages these queues carefully to ensure processes don't run out of memory, overflowing to disk when appropriate.  It handles socket errors.  It does all I/O in background threads.  It uses lock-free techniques for talking between nodes, so there are never locks, waits, semaphores, or deadlocks.

But cutting through that, it routes and queues messages according to precise recipes called //patterns//.  It is these patterns that provide 0MQ's intelligence.  They encapsulate our hard-earned experience of the best ways to distribute data and work.  0MQ's patterns are hard-coded but future versions may allow user-definable patterns.

0MQ patterns are implemented by pairs of sockets with matching types.  In other words, to understand 0MQ patterns you need to understand socket types and how they work together.  Mostly this just takes learning, there is little that is obvious at this level.

The basic 0MQ patterns are:

* **Request-reply**, which connects a set of clients to a set of services.  This is a remote procedure call and task distribution pattern.
* **Publish-subscribe**, which connects a set of publishers to a set of subscribers.  This is a data distribution pattern.
* **Pipeline**, connects nodes in a fan-out / fan-in pattern that can have multiple steps, and loops.  This is a parallel task distribution and collection pattern.

We looked at each of these in the first chapter.  There's one more pattern that people tend to try to use when they still think of 0MQ in terms of traditional TCP sockets:

* **Exclusive pair**, which connects two sockets in an exclusive pair.  This is a low-level pattern for specific, advanced use cases.  We'll see an example at the end of this chapter.

The zmq_socket(3) [http://api.0MQ.org/zmq_socket.html man page] is fairly clear about the patterns, it's worth reading several times until it starts to make sense.  We'll look at each pattern and the use cases it covers.

These are the socket combinations that are valid for a connect-bind pair (either side can bind):

* PUB and SUB
* REQ and REP
* REQ and XREP
* XREQ and REP
* XREQ and XREP
* PUSH and PULL
* PAIR and PAIR

Any other combination will produce undocumented and unreliable results and future versions of 0MQ will probably return errors if you try them.  You can and will of course combine other socket types //via code//, i.e. read from one socket type and write to another.

+++ Messages and Zero-copy

On the wire, 0MQ messages are blobs of any size from zero upwards, fitting in memory.  You do your own serialization using Google Protocol Buffers, XDR, JSON, or whatever else your applications need to speak.  It's wise to choose a data representation that is portable and fast, but you can make your own decisions about trade-offs.

In memory, 0MQ messages are zmq_msg_t structures (or classes depending on your language). This structure has a pointer to the data, and does reference counts, and provides a hook for 0MQ to free the data when it's not needed any longer.

In high-performance applications (hundreds of thousands of messages per second), copying data can make things go slower.  0MQ's message API is therefore focused on zero-copy.  This means you can send and receive messages directly from and to application buffers without copying data.  Given that 0MQ sends messages in the background, zero-copy needs some extra sauce.

I'll say right away that 0MQ's message API is more complex than it could be.  We don't need zero-copy all the time (or even most of the time) but we pay for it by not being able to just send blocks of data directly to and from sockets.  0MQ/3.0 will probably fix this.

Anyhow since we're describing 0MQ/2.x, here are the basic ground rules for using 0MQ messages, first without doing zero copy:

* You create and pass around zmq_msg_t objects, not blocks of data.

* To read a message you use zmq_msg_init(3) to create an empty message, and then you pass that to zmq_recv(3).

* To write a message from new data, you use zmq_msg_init_size(3) to create a message and at the same time allocate a block of data of some size.  You then fill that data, and pass the message to zmq_send(3).  This is the simplest, but not the fastest method.

* To release (not destroy) a message you call zmq_msg_close(3).  This drops a reference, and eventually 0MQ will destroy the message.

* To access the message content you use zmq_msg_data(3), zmq_msg_size(3), zmq_msg_copy(3) and zmq_msg_move(3).

**Note than when you have passed a message to zmq_send(3), 0MQ will clear the message, i.e. set the data to empty.  You cannot send the same message twice, and you cannot access the message data after sending it.**

To do zero copy you use zmq_msg_init_data(3) to create a message that refers to a block of data already allocated on the heap with malloc(), and then you pass that to zmq_send(3).  When you create the message you also pass a function that 0MQ will call to free the block of data, when it has finished sending the message.  This is the simplest example, assuming 'buffer' is a block of 1000 bytes allocated on the heap:

[[code type="C"]]
void my_free (void *data, void *hint)
{
    free (data);
}
//  Send message from buffer, which we allocate and 0MQ will free for us
zmq_msg_t message;
zmq_msg_init_data (&message, buffer, 1000, my_free, NULL);
zmq_send (socket, &message, 0);
[[/code]]

0MQ also supports //multipart// messages, which let you handle a list of blobs as a single message.  This goes further than we need for basic use, and we'll cover multipart messages in the Advanced Concepts section.

Some other things that are worth knowing about messages:

* 0MQ sends and receives them atomically, i.e. you get a whole message, or you don't get it at all.
* 0MQ does not send a message right away but at some indeterminate later time.
* You can send zero-length messages, e.g. for sending a signal from one thread to another.
* A message must fit in memory.  If you want to send files of arbitrary sizes, you should break them into pieces and send each piece as a message.
* You must call zmq_msg_close(3) when finished with a message, in languages that don't automatically destroy objects when a scope closes.

When you start using 0MQ, a classic error is that messages arrive but are garbled.  It's a beginner's mistake: the sending thread is freeing or reusing the message data before 0MQ has finished sending it, so 0MQ sends garbage.  If you're using zmq_msg_init_data(3), realize that the data is not copied.  if your thread does not own the data, and therefore it cannot pass a free function as in the example above, then you must do this:

[[code type="C"]]
//  Send a message from buffer, which we do not own
zmq_msg_t message;
zmq_msg_init_size (&message, 1000);
mempcy (zmq_msg_data (&message), buffer, 1000);
zmq_send (socket, &message, 0);
[[/code]]

+++ Handling Multiple Sockets

In all the examples so far, the main loop of most examples has been:

# wait for message on socket
# process message
# repeat

What if we want to read from multiple sockets at the same time?  The simplest way is to connect one socket to multiple endpoints and get 0MQ to do the fanin for us.  This is legal if the remote endpoints are in the same pattern but it would be illegal to e.g. connect a PULL socket to a PUB endpoint.  Fun, but illegal.  If you start mixing patterns you break future scalability.

The right way is to use zmq_poll(3).  An even better way might be to wrap zmq_poll(3) in a framework that turns it into a nice event-driven //reactor//. This is perhaps the best solution of all, and we'll make a simple reactor later, but it's significantly more work than we want to cover here.

Let's start with a dirty hack, partly for the fun of not doing it right, but mainly because it lets me show you how to do non-blocking socket reads.  Here is a simple example of reading from two sockets using non-blocking reads.  This rather confused program acts both as a subscriber to weather updates, and a worker for parallel tasks:

[[code type="C" title="Multiple socket reader" name="msreader"]]
[[/code]]

The cost of this approach is some additional latency on the first message (the sleep at the end of the loop, when there are no waiting messages to process).  This would be a problem in applications where sub-millisecond latency was vital.  Also, you need to check the documentation for nanosleep() or whatever function you use to make sure it does not busy-loop.

You can treat the sockets fairly by reading first from one, then the second rather than prioritizing them as we did in this example.  This is called "fair-queuing", something that 0MQ does automatically when one socket receives messages from more than one source.

Now let's see the same little senseless application done right, using zmq_poll(3):

[[code type="C" title="Multiple socket poller" name="mspoller"]]
[[/code]]

+++ Multipart Messages

Multipart messages are messages consisting of more than one part (technically, frames).  0MQ uses these for some particular cases that we'll look at in the Advanced Concepts chapter.  What we'll look at now is how to send, and how to receive multipart messages.

Here is how we send the frames in a multipart message (each frame is a message object):

[[code]]
zmq_send (socket, &message, ZMQ_SNDMORE);
...
zmq_send (socket, &message, ZMQ_SNDMORE);
...
zmq_send (socket, &message, 0);
[[/code]]

Here is how we receive and process all the parts in a message, be it single part or multipart:

[[code]]
zmq_msg_t message;
int64_t more;
size_t more_size;

while (1) {
    zmq_msg_init (&message);
    zmq_recv (socket, &message, 0);
    //  Process the message part
    zmq_msg_close (&message);
    more_size = sizeof (more);
    zmq_getsockopt (socket, ZMQ_RCVMORE, &more, &more_size);
    if (!more)
        break;      //  Last message part
}
[[/code]]

Some things to know about multipart messages:

* You will receive all parts of a message, or none at all.
* You will receive all parts of a message whether or not you check the RCVMORE option.
* On sending, 0MQ queues message parts in memory until the last is received, then sends them all.
* There is no way to cancel a partially sent message, except by closing the socket.

+++ Handling Errors and ETERM

0MQ's error handling philosophy is a mix of fail-fast and resilience.  Processes, we believe, should be as vulnerable as possible to internal errors, and as robust as possible against external attacks and errors.  To give an analogy, a living cell will self-destruct if it detects a single internal error, yet it will resist attack from the outside by all means possible.  Assertions, which pepper the 0MQ code, are absolutely vital to robust code, they just have to be on the right side of the cellular wall.  And there should be such a wall.  If it is unclear whether a fault is internal or external, that is a design flaw that needs to be fixed.

In C, assertions stop the application immediately with an error.  In other languages you may get exceptions or halts.

When 0MQ detects an external faults it returns an error to the calling code.  In some rare cases it drops messages silently, if there is no obvious strategy for recovering from the error.  In a few places 0MQ still asserts on external faults, but these are considered bugs.

In most of the C examples we've seen so far there's been no error handling.  **Real code should do error handling on every single 0MQ call**.  If you're using a language binding other than C, the binding may handle errors for you.  In C you do need to do this yourself.  There are some simple rules, starting with POSIX conventions:

* Methods that create objects will return NULL in case they fail.
* Other methods will return 0 on success and other values (mostly -1) on an exceptional condition (usually failure).
* The error code is provided in {{errno}} or zmq_errno(3).
* A descriptive error text for logging is provided by zmq_strerror(3).

There are two main exceptional conditions that you may want to handle as non-fatal:

* When a thread calls zmq_recv(3) with the NOBLOCK option and there is no waiting data.  0MQ will return -1 and set errno to EAGAIN.
* When a thread calls zmq_term(3) and other threads are doing blocking work.  The zmq_term(3) call closes the context and all blocking calls exit with -1, and errno set to ETERM.

What this boils down to is that in most cases you can wrap 0MQ calls in assertions, like this:

[[code type="C"]]
    context = zmq_init (1);
    assert (context);
    socket = zmq_socket (context, ZMQ_REP);
    assert (socket);
    assert (!zmq_bind (socket, "tcp://*:5555"));
[[/code]]

Let's see how to shut down a process cleanly.  We'll take the parallel pipeline example from the previous section.  If we've start a whole lot of workers in the background, we now want to kill them when the batch is finished.  Let's do this by sending a kill message to the workers.  The best place to do this is the sink, since it really knows when the batch is done.

How do we connect the sink to the workers?  The PUSH/PULL sockets are one-way only.  The standard 0MQ answer is: create a new socket flow for each type of problem you need to solve.  We'll use a publish-subscribe model to send kill messages to the workers:

* The sink creates a PUB socket on a new endpoint.
* Workers bind their input socket to this endpoint.
* When the sink detects the end of the batch it sends a kill message on its PUB socket.
* When a worker detects this kill message, it exits.

It doesn't take much new code in the sink:

[[code]]
    control = zmq_socket (context, ZMQ_PUB);
    zmq_bind (control, "tcp://*:5559");

    ...
    //  Send kill signal to workers
    zmq_msg_init_data (&message, "KILL", 5);
    zmq_send (control, &message, 0);
    zmq_msg_close (&message);
[[/code]]

[[code type="textdiagram"]]
                +-------------+
                |             |
                |  Ventilator |
                |             |
                +-------------+
                |    PUSH     |
                \-------------/
                       |
                     tasks
                       |
       +---------------+---------------+
       |               |               |
       |     /=--------|-----+=--------|-----+------\
     task    |       task    |        task   |      :
       |     |         |     |         |     |      |
       v     v         v     v         v     v      |
   /------+-----\  /------+-----\  /------+-----\   |
   | PULL | SUB |  | PULL | SUB |  | PULL | SUB |   |
   +------+-----+  +------+-----+  +------+-----+   |
   |            |  |            |  |            |   |
   |   Worker   |  |   Worker   |  |   Worker   |   |
   |            |  |            |  |            |   |
   +------------+  +------------+  +------------+   |
   |    PUSH    |  |    PUSH    |  |    PUSH    |   |
   \------------/  \------------/  \------------/   |
         |               |               |          |
       result          result          result       |
         |               |               |          |
         +---------------+---------------+          |
                         |                          |
                      results                       |
                         |                          |
                         v                          |
                  /-------------\                   |
                  |    PULL     |                   |
                  +-------------+                   |
                  |             |                   |
                  |    Sink     |                   |
                  |             |                   |
                  +-------------+                   |
                  |     PUB     |                   |
                  \-------------/                   |
                         |                          |
                    KILL signal                     |
                         |                          |
                         \--------------------------/


     Figure # - Parallel Pipeline with Kill signalling
[[/code]]

Here is the worker process, which manages two sockets (a PULL socket getting tasks, and a SUB socket getting control commands) using the zmq_poll(3) technique we saw earlier:

[[code type="C" title="Parallel task worker with kill signalling" name="taskwork2"]]
[[/code]]

Here is the modified sink application.  When it's finished collecting results it broadcasts a KILL message to all workers:

[[code type="C" title="Parallel task sink with kill signalling" name="tasksink2"]]
[[/code]]

+++ Devilish Devices

We now introduce a class of 0MQ applications called "devices".  Devices are anything that sit between your real applications.  We usually start building any 0MQ application as a set of nodes on a network with the nodes talking to each other:

[[code type="textdiagram"]]
               +--------+
               |        |
               |  Node  |
               |        |
               +--------+
               | Socket |
               \--------/
                   |
                   |
             +-----+-----+
             |           |
             |           |
        /--------\   /--------\
        | Socket |   | Socket |
        +--------+   +--------+
        |        |   |        |
        |  Node  |   |  Node  |
        |        |   |        |
        +--------+   +--------+


Figure # - Small scale 0MQ application
[[/code]]

And then we stretch the application across a wider network, placing devices in specific places and scaling up the number of nodes:

[[code type="textdiagram"]]
                +--------+
                |        |
                |  Node  |
                |        |
                +--------+
                | Socket |
                \--------/
                    |
                    |
        +-----------+-----------+
        |           |           |
        |           |           |
    /--------\  /--------\  /--------\
    | Socket |  | Socket |  | Socket |
    +--------+  +--------+  +--------+
    |        |  |        |  |        |
    |  Node  |  |  Node  |  | Device |
    |        |  |        |  |        |
    +--------+  +--------+  +--------+
                            | Socket |
                            \--------/
                                |
                                |
                          +-----+-----+
                          |           |
                          |           |
                     /--------\   /--------\
                     | Socket |   | Socket |
                     +--------+   +--------+
                     |        |   |        |
                     |  Node  |   |  Node  |
                     |        |   |        |
                     +--------+   +--------+


    Figure # - Larger scale 0MQ application
[[/code]]

0MQ devices:

* Generally run without state or persistence.
* Are applications that read and write 0MQ sockets in particular ways.
* Usually connect one 'frontend' socket to one 'backend' socket.
* May in complex cases work with three or more sockets.
* Run as stand-alone processes or within threads.

Devices are a confusing concept to the new 0MQ user, and the best way as always is to look at real use cases and see how we solve them.

++++ A Publish-Subscribe Proxy Server

It is a common requirement to stretch a publish-subscribe architecture over more than one network segment or transport.  Perhaps there are a group of subscribers sitting at a remote location.  Perhaps we want to publish to local subscribers via multicast, and to remote subscribers via TCP.

We're going to write a simple proxy server that sits in between a publisher and a set of subscribers, bridging two networks.  This is perhaps the simplest case of a useful device.  The device has two sockets, a frontend facing subscribers on the external network and a backend facing the internal network, where the weather server is sitting.  It subscribes to the weather service on the backend socket, and republishes its data on the frontend socket:

[[code type="C" title="Weather update proxy" name="wuproxy"]]
[[/code]]

We call this a //proxy// because it acts as a subscriber to publishers, and acts as a publisher to subscribers. That means you can slot this device into an existing network without affecting it (of course the new subscribers need to know to speak to the proxy).

[[code type="textdiagram"]]

                   +-----------+
                   |           |
                   | Publisher |
                   |           |
                   +-----------+
                   |    PUB    |
                   \-----------/
                       bind
             tcp://192.168.55.210:5556
                         |
                         |
        +----------------+----------------+
        |                |                |
        |                |                |
     connect           connect            |
  /------------\   /------------\       connect
  |    SUB     |   |    SUB     |   /------------\
  +------------+   +------------+   |    SUB     |
  |            |   |            |   +------------+
  | Subscriber |   | Subscriber |   |            |
  |            |   |            |   | Forwarder  |
  +------------+   +------------+   |            |
                                    +------------+
   Internal network                 |    PUB     |
   ---------------------------------\------------/--------
   External network                      bind
                                  tcp://10.1.1.0:8100
                                          |
                                          |
                                 +--------+--------+
                                 |                 |
                                 |                 |
                              connect           connect
                           /------------\    /------------\
                           |    SUB     |    |    SUB     |
                           +------------+    +------------+
                           |            |    |            |
                           | Subscriber |    | Subscriber |
                           |            |    |            |
                           +------------+    +------------+


             Figure # - Forwarder proxy device
[[/code]]

Note that this application is multipart safe.  It correctly detects multipart messages and sends them as it read them.  If we did not set the SNDMORE option on outgoing multipart data, the final recipient would get a corrupted message.  You should always make your devices multipart safe so that there is no risk they will corrupt the data they switch.

++++ A Request-Reply Broker

Let's explore how to solve a problem of scale by writing a little message queuing broker in 0MQ.  We'll look at the request-reply pattern for this case.

In the Hello World client-server application we have one client that talks to one service.  However in real cases we usually need to allow multiple services as well as multiple clients.  This lets us scale up the power of the service (many threads or processes or boxes rather than just one).  The only constraint is that services must be stateless, all state being in the request or in some shared storage such as a database.

There are two ways to connect multiple clients to multiple servers.  The brute-force way is to connect each client socket to multiple service endpoints.  One client socket can connect to multiple service sockets, and requests are load-balanced among these services.  Let's say you connect a client socket to three service endpoints, A, B, and C.  The client makes requests R1, R2, R3, R4.  R1 and R4 go to service A, R2 goes to B, and R3 goes to service C.

[[code type="textdiagram"]]
               +-----------+
               |           |
               |   Client  |
               |           |
               +-----------+
               |    REQ    |
               \-----------/
                     |
               R1, R2, R3, R4
                     |
       +-------------+-------------+
       |             |             |
    R1, R4           R2            R3
       |             |             |
       v             v             v
  /---------\   /---------\   /---------\
  |   REP   |   |   REP   |   |   REP   |
  +---------+   +---------+   +---------+
  |         |   |         |   |         |
  | Service |   | Service |   | Service |
  |    A    |   |    B    |   |    C    |
  |         |   |         |   |         |
  +---------+   +---------+   +---------+


   Figure # - Load balancing of requests
[[/code]]

This design lets you add more clients cheaply.  You can also add more services.  Each client will load-balance its requests to the services.  But each client has to know the service topology.  If you have 100 clients and then you decide to add three more services, you need to reconfigure and restart 100 clients in order for the clients to know about the three new services.

That's clearly not the kind of thing we want to be doing at 3am when our supercomputing cluster has run out of resources and we desperately need to add a couple of hundred new service nodes.  Too many stable pieces are like liquid concrete: knowledge is distributed and the more stable pieces you have, the more effort it is to change the topology.  What we want is something sitting in between clients and services that centralizes all knowledge of the topology.  Ideally, we should be able to add and remove services or clients at any time without touching any other part of the topology.

So we'll write a little message queuing broker that gives us this flexibility.  The broker binds to two endpoints, a frontend for clients and a backend for services.  It then uses zmq_poll(3) to monitor these two sockets for activity and when it has some, it shuttles messages between its two sockets.  It doesn't actually manage any queues explicitly -- 0MQ does that automatically on each socket.

When you use REQ to talk to REP you get a strictly synchronous request-reply dialog.  The client sends a request, the service reads the request and sends a reply.  The client then reads the reply.  If either the client or the service try to do anything else (e.g. sending two requests in a row without waiting for a response) they will get an error.

But our broker has to be non-blocking.  Obviously we can use zmq_poll(3) to wait for activity on either socket, but we can't use REP and REQ.

Luckily there are non-blocking versions of these two sockets, called XREQ and XREP.  These "extended request/reply" sockets let you stretch request-reply across intermediate nodes, such as our message queuing broker.

When we stretch request-reply, REQ talks to XREP and XREQ talks to REP.  In between the XREQ and XREP we have to have code (like our broker) that pulls messages off the one socket and shoves them onto the other:

[[code type="textdiagram"]]
  +---------+   +---------+   +---------+
  |   REQ   |   |   REQ   |   |   REQ   |
  \---------/   \---------/   \---------/
       |             |             |
       |             |             |
       +-------------+-------------+
                     |
                     |
               /-----------\
               |   XREP    |
               +-----------+
               |   code    |
               +-----------+
               |   XREQ    |
               \-----------/
                     |
                     |
       +-------------+-------------+
       |             |             |
       |             |             |
  /---------\   /---------\   /---------\
  |   REP   |   |   REP   |   |   REP   |
  +---------+   +---------+   +---------+


    Figure # - Extending request-reply
[[/code]]

The request-reply broker binds to two endpoints, one for clients to connect to (the frontend socket) and one for services to connect to (the backend).  To test this broker, you will want to change your services so they connect to the backend socket.  Here are a Python client and service that show what I mean:

[[code type="Python" title="Request-reply client" name="rrclient"]]
[[/code]]

[[code type="Python" title="Request-reply service" name="rrserver"]]
[[/code]]

And here is the broker, in C.  You will see that it's multipart safe:

[[code type="C" title="Request-reply broker" name="rrbroker"]]
[[/code]]

Using a request-reply broker makes your client-server architectures easier to scale since clients don't see services, and services don't see clients.  The only stable node is the device in the middle:

[[code type="textdiagram"]]
  +---------+   +---------+   +---------+
  |         |   |         |   |         |
  | Client  |   | Client  |   | Client  |
  |         |   |         |   |         |
  +---------+   +---------+   +---------+
  |   REQ   |   |   REQ   |   |   REQ   |
  \---------/   \---------/   \---------/
    connect       connect       connect
       |             |             |
       |             |             |
    request       request       request
       |             |             |
       +-------------+-------------+
                     |
               fair-queuing
                     |
                     v
                   bind
               /-----------\
               |   XREP    |
               +-----------+
               |           |
               |   Broker  |
               |           |
               +-----------+
               |   XREQ    |
               \-----------/
                   bind
                     |
              load balancing
                     |
       +-------------+-------------+
       |             |             |
    request       request       request
       |             |             |
       v             v             v
    connect       connect       connect
  /---------\   /---------\   /---------\
  |   REP   |   |   REP   |   |   REP   |
  +---------+   +---------+   +---------+
  |         |   |         |   |         |
  | Service |   | Service |   | Service |
  |    A    |   |    B    |   |    C    |
  |         |   |         |   |         |
  +---------+   +---------+   +---------+


      Figure # - Request-reply broker
[[/code]]

++++ Built-in Devices

0MQ provides some built-in devices, though most advanced users write their own devices.  The built-in devices are:

* QUEUE, which is like the request-reply broker.
* FORWARDER, which is like the pub-sub proxy server.
* STREAMER, which is like FORWARDER but for pipeline flows.

To start a device, you call zmq_queue(3) and pass it two sockets, one for the frontend and one for the backend:

[[code]]
zmq_device (ZMQ_QUEUE, frontend, backend);
[[/code]]

Which if you start a QUEUE device is exactly like plugging the main body of the request-reply broker into your code at that spot.  You need to create the sockets, bind or connect them, and possibly configure them, before calling zmq_device(3).  It is trivial to do.  Here is the request-reply broker re-written to call QUEUE and rebadged as an expensive-sounding "message queue":

[[code type="C" title="Message queue broker" name="msgqueue"]]
[[/code]]

The built-in devices do proper error handling, whereas the examples we have shown don't.  Since you can configure the sockets as you need to, before starting the device, it's worth using the built-in devices when you can.

++++ Patience is Power

If you're like most 0MQ users, at this stage your mind is starting to think, "what kind of evil stuff can I do if I plug random socket types into devices?"

The short answer is: don't do it.  The longer answer is: don't do it //yet//.  You can mix socket types but the results are going to be weird.  Some socket types like REQ and REP send multipart data, whereas others do not.  Later on we'll look at exotic devices that do things like connect XREP to XREP.

For now, these are the combinations you should stick to:

* XREP to XREQ for queue devices.
* SUB to PUB for forwarder devices.
* PULL to PUSH for streamer devices.

+++ Multithreading with 0MQ

0MQ is perhaps the nicest way ever to write multithreaded applications.  Whereas as 0MQ sockets require some readjustment if you are used to traditional sockets, 0MQ multithreading will take everything you know about writing multithreading applications, throw it into a heap in the garden, pour gasoline over it, and set it alite.

**We don't use mutexes, locks, or any other form of inter-thread communication except messages sent across 0MQ sockets.**

0MQ uses native OS threads rather than virtual "green" threads.  The advantage is that you don't need to learn any new threading API, and that 0MQ threads map cleanly to your operating system.  You can use standard tools like Intel's ThreadChecker to see what your application is doing.  The disadvantages are that your code, when it for instance starts new threads, won't be portable, and that if you have a huge number of threads (thousands), some operating systems will get stressed.

You do need to follow some rules to write multithreaded code with 0MQ:

* You MUST NOT try to access the same data from multiple threads.
* You MUST create a 'context' object for your process, and pass that to all threads.  The context collects 0MQ's state.  To create a connection across the inproc: transport, both server and client thread must share the same context object.
* You MUST NOT share sockets between threads, not even when using proper exclusion mechanisms like semaphores, locks or mutexes.  0MQ will in some future make it possible to move sockets to different threads.  Right now the thread which creates a socket is the only thread that may use it.
* You MAY use semaphores, locks and mutexes //if you really have to// (but NOT to share sockets among threads).  However we'd consider that bad practice and potentially dangerous

If you follow these rules, you can quite easily split threads into separate processes, when you need to.  Application logic can sit in threads, processes, boxes: whatever your scale needs.

Let's turn our old Hello World server into something more capable.  The original server was a single thread.  If the work per request is low, that's fine: one ØMQ thread can run at full speed on a CPU core, with no waits, doing an awful lot of work.  But realistic servers have to do non-trivial work per request.  A single core may not be enough when 10,000 clients hit the server all at once.  So a realistic server must starts multiple worker threads.  It then accepts requests as fast as it can, and distributes these to its worker threads.  The worker threads grind through the work, and eventually send their replies back.

You can of course do all this using a queue device and external worker processes, but often it's easier to start one process that gobbles up sixteen cores, than sixteen processes, each gobbling up one core.  Further, running workers as threads will cut out a network hop, latency, and network traffic.

The multithreaded version of the Hello World service basically collapses the queue device and workers into a single process:

[[code type="C" title="Multithreaded Service" name="mtserver"]]
[[/code]]

All the code should be recognizable to you by now.  How it works:

* The server starts a set of worker threads.  Each worker thread creates a REP socket and then processes requests on this socket.  Worker threads are just like single-threaded servers.  The only differences are the transport (inproc: instead of tcp:), and the bind-connect direction.

* The server creates an XREP (extended reply) socket to talk to clients and binds this to its external interface (over tcp:).
* The server creates an XREQ (extended request) socket to talk to the workers and binds this to its internal interface (over inproc:).
* The server starts a QUEUE device that connects the two sockets.  The QUEUE device keeps a single queue for incoming requests, and distributes those out to workers.  It also routes replies back to their origin.

Here the 'work' is just a one-second pause.  We could do anything in the workers, including talking to other nodes.  This is what the multithreaded server looks like in terms of ØMQ sockets and nodes.  Note how the request-reply chain is {{REQ-XREP-queue-XREQ-REP}}:

[[code type="textdiagram"]]
                 +------------+
                 |            |
                 |   Client   |
                 |            |
                 +------------+
                 |    REQ     |
                 \------------/
                     |    ^
                     |    |
                "Hello"  "World"
                     |    |
  /------------------|----|------------------\
  |                  v    |                  :
  |              /------------\              |
  |              |    XREP    |              |
  |              +------------+              |
  |              |            |              |
  |              |   Server   |              |
  |              |            |              |
  |              +------------+              |
  |              |            |              |
  |              |   Queue    |              |
  |              |   device   |              |
  |              |            |              |
  |              +------------+              |
  |              |    XREQ    |              |
  |              \------------/              |
  |                    ^                     |
  |                    |                     |
  |        +-----------+-----------+         |
  |        |           |           |         |
  |        v           v           v         |
  |    /--------\  /--------\  /--------\    |
  |    |  REP   |  |  REP   |  |  REP   |    |
  |    +--------+  +--------+  +--------+    |
  |    |        |  |        |  |        |    |
  |    | Worker |  | Worker |  | Worker |    |
  |    |        |  |        |  |        |    |
  |    +--------+  +--------+  +--------+    |
  |                                          |
  \------------------------------------------/


 Figure # - Multithreaded server
[[/code]]

+++ Thread Coordination

When you start making multithreaded applications with 0MQ, you'll hit the question of how to coordinate your threads.  Though you might be tempted to insert 'sleep' statements, or use multithreading techniques such as semaphores or mutexes, **the only mechanism that you should use are 0MQ messages**.  Here is a simple example showing three threads that signal each other when they are ready.

[[code type="textdiagram"]]
        +------------+
        |            |
        |   Step 1   |
        |            |
        +------------+
        |    PAIR    |
        \------------/
              |
              |
            Ready!
              |
              v
        /------------\
        |    PAIR    |
        +------------+
        |            |
        |   Step 2   |
        |            |
        +------------+
        |    PAIR    |
        \------------/
              |
              |
            Ready!
              |
              v
        /------------\
        |    PAIR    |
        +------------+
        |            |
        |   Step 3   |
        |            |
        +------------+

 Figure # - The Relay Race
[[/code]]

In this example we use PAIR sockets over the //inproc:// transport:

[[code type="C" title="Multithreaded relay" name="mtrelay"]]
[[/code]]

This is the first time we've shown an example using PAIR sockets.  Why use PAIR?  Other socket combinations might seem to work but they all have side-effects that could interfere with signalling:

* You can use PUSH for the sender and PULL for the receiver.  This looks simple and will work, but remember that PUSH will load-balance messages to all available receivers.  If you by accident start two receivers (e.g. you already have one running and you start a second), you'll "lose" half of your signals.  PAIR has the advantage of refusing more than one connection, the pair is //exclusive//.

* You can use XREQ for the sender and XREP for the receiver.  XREP however sticks an "identity" part in front of your message, meaning your zero-size signal turns into a multipart message.  If you don't transmit data except the signal itself, that won't matter.  If however you decide to send data, you will suddenly find XREP providing you with "wrong" messages.  XREQ also load-balances, giving the same risk as PUSH.

* You can use PUB for the sender and SUB for the receiver.  This will correctly deliver you messages exactly as you sent them and PUB does not load-balance as PUSH or XREQ do.  However you need to configure the subscriber with an empty subscription, which is annoying.  Worse, the reliability of the PUB-SUB link is timing dependent and messages can get lost if the SUB socket is connecting while the PUB socket is sending its message.

For these reasons, PAIR makes the best choice for coordination between specific threads.

+++ Node Coordination

When you want to coordinate nodes, PAIR sockets won't work well any more.  This is one of the few areas where the strategies for threads and nodes are different.  Principally nodes come and go whereas threads are stable.  PAIR sockets do not automatically reconnect if the remote node goes away and comes back.

The second significant difference between threads and nodes is that you typically have a fixed number of threads but a more variable number of nodes.  Let's take one of our earlier scenarios (the weather server and clients) and use node coordination to ensure that subscribers don't lose data when starting up.

This is how the application will work:

* The publisher knows in advance how many subscribers it expects.  This is just a magic number it gets from somewhere.
* The publisher starts up and waits for all subscribers to connect.  This is the node coordination part.  Each subscriber subscribes and then tells the publisher it's ready via another socket.
* When the publisher has all subscribers connected, it starts to publish data.

In this case we'll use a REQ-REP socket flow to synchronize subscribers and publisher.  Here is the publisher:

[[code type="C" title="Synchronized publisher" name="syncpub"]]
[[/code]]

[[code type="textdiagram"]]
            +----------------+
            |                |
            |   Publisher    |
            |                |
            +--------+-------+
            |  PUB   |  REQ  |
            \--------+-------/
                |      ^   |
                |      |   |
                |     (1)  |
               (3)     |   |
                |      |  (2)
                |      |   |
                v      |   v
            /----------------\
            |  SUB   |  REP  |
            +----------------+
            |                |
            |   Subscriber   |
            |                |
            +----------------+


  Figure # - Pub-Sub Synchronization
[[/code]]

And here is the subscriber:

[[code type="C" title="Synchronized subscriber" name="syncsub"]]
[[/code]]

This Linux shell script will start ten subscribers and then the publisher:

[[code]]
echo "Starting subscribers..."
for a in 1 2 3 4 5 6 7 8 9 10; do
    syncsub &
done
echo "Starting publisher..."
syncpub
[[/code]]

Which gives us this satisfying output:

[[code]]
Starting subscribers...
Starting publisher...
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
[[/code]]

[!-- TODO: remove this note when 0MQ/2.1 is released --]
Note that we do {{sleep (1);}} before exiting the publisher.  This is a hack that gets around 0MQ/2.0's design, which discards messages that have not yet been sent, if you exit the program too soon.  If you are using 0MQ/2.1 you can remove this sleep statement.  There is no way in 0MQ/2.0 to properly exit the publisher without such a sleep.

+++ A Bare Necessity

ØMQ is like a box of pieces that plug together, the only limitation being your imagination and sobriety.

The scalable elastic architecture you get should be an eye-opener.  You might need a coffee or two first.  Don't make the mistake I made once and buy exotic German coffee labeled //Entkoffeiniert//.  That does not mean "Delicious".  Scalable elastic architectures are not a new idea - [http://en.wikipedia.org/wiki/Flow-based_programming flow-based programming] and languages like [http://www.erlang.org/ Erlang] already worked like this - but ØMQ makes it easier to use than ever before.

As [http://permalink.gmane.org/gmane.network.zeromq.devel/2145 Gonzo Diethelm said], '//My gut feeling is summarized in this sentence: "if ØMQ didn't exist, it would be necessary to invent it". Meaning that I ran into ØMQ after years of brain-background processing, and it made instant sense... ØMQ simply seems to me a "bare necessity" nowadays.//'
