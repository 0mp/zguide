[[include :csi:include:css-brackets]]
[[div style="overflow:hidden"]]
[[div style="float:left"]]
++ Chapter Two - Intermediate Stuff
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

In Chapter One we took ØMQ for a drive, with some basic examples of the main ØMQ patterns: request-reply, publish-subscribe, and pipeline.  In this chapter we're going to get our hands dirty and start to learn how to use these tools in real programs.

We'll cover:

* How to create and work with ØMQ sockets.
* How to send and receive messages on sockets.
* How to build your apps around ØMQ's asynchronous I/O model.
* How to handle multiple sockets in one thread.
* How to handle fatal and non-fatal errors properly.
* How to shutdown a ØMQ application cleanly.
* How to send and receive multipart messages.
* How to forward messages across networks.
* How to build a simple message queuing broker.
* How to write multithreaded applications with ØMQ.
* How to use ØMQ to signal between threads.
* How to use ØMQ to coordinate a network of nodes.
* How to create durable sockets using socket identities.
* How to create and use message envelopes for publish-subscribe.
* How to do make durable subscribers that can recover from crashes.
* Using the high-water mark (HWM) to protect against memory overflows.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ The Zen of Zero
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

The Ø in ØMQ is all about tradeoffs. On the one hand this strange name lowers ØMQ's visibility on Google and Twitter.  On the other hand it annoys the heck out of some Danish folk who write us things like "ØMG røtfl", and "//Ø is not a funny looking zero!//" and "//Rødgrød med Fløde!//", which is apparently an insult that means "may your neighbours be the direct descendents of Grendel!"  Seems like a fair trade.

Originally the zero in ØMQ was meant as "zero broker" and (as close to) "zero latency" (as possible).  In the meantime it has come to cover different goals: zero administration, zero cost, zero waste.  More generally, "zero" refers to the culture of minimalism that permeates the project.  We add power by removing complexity rather than exposing new functionality.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ The Socket API
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

To be perfectly honest, ØMQ does a kind of switch-and-bait on you.  Which we don't apologize for, it's for your own good and hurts us more than it hurts you.  It presents a familiar BSD socket API but that hides a bunch of message-processing machines that will slowly fix your world-view about how to design and write distributed software.

Sockets are the de-facto standard API for network programming, as well as being useful for stopping your eyes from falling onto your cheeks.  One thing that makes ØMQ especially tasty to developers is that it uses a standard socket API.  Kudos to Martin Sustrik for pulling this idea off.  It turns "Message Oriented Middleware", a phrase guaranteed to send the whole room off to Catatonia, into "Extra Spicy Sockets!" which leaves us with a strange craving for pizza, and a desire to know more.

Like a nice pepperoni pizza, ØMQ sockets are easy to digest.  Sockets have a life in four parts, just like BSD sockets:

* Creating and destroying sockets, which go together to form a karmic circle of socket life (see [http://api.zeromq.org/zmq_socket.html zmq_socket(3)], [http://api.zeromq.org/zmq_close.html zmq_close(3)]).

* Configuring sockets by setting options on them and checking them if necessary (see [http://api.zeromq.org/zmq_setsockopt.html zmq_setsockopt(3)], [http://api.zeromq.org/zmq_getsockopt.html zmq_getsockopt(3)]).

* Plugging sockets onto the network topology by creating ØMQ connections to and from them (see [http://api.zeromq.org/zmq_bind.html zmq_bind(3)], [http://api.zeromq.org/zmq_connect.html zmq_connect(3)]).

* Using the sockets to carry data by writing and receiving messages on them (see [http://api.zeromq.org/zmq_send.html zmq_send(3)], [http://api.zeromq.org/zmq_recv.html zmq_recv(3)]).

Which looks like this:

[[code type="C"]]
void *mousetrap;

//  Create socket for catching mice
mousetrap = zmq_socket (context, ZMQ_PULL);

//  Configure the socket
int64_t jawsize = 10000;
zmq_setsockopt (mousetrap, ZMQ_HWM, &jawsize, sizeof jawsize);

//  Plug socket into mouse hole
zmq_connect (mousetrap, "tcp://192.168.55.221:5001");

//  Wait for juicy mouse to arrive
zmq_msg_t mouse;
zmq_msg_init (&mouse);
zmq_recv (mousetrap, &mouse, 0);
//  Destroy the mouse
zmq_msg_close (&mouse);

//  Destroy the socket
zmq_close (mousetrap);
[[/code]]

Note that sockets are always void pointers, and messages (which we'll come to very soon) are structures.  So in C you pass sockets as-such, but you pass addresses of messages in all functions that work with messages, like [http://api.zeromq.org/zmq_send.html zmq_send(3)] and [http://api.zeromq.org/zmq_recv.html zmq_recv(3)].  As a mnemonic, realize that "in ØMQ all ur sockets are belong to us", but messages are things you actually own in your code.

Creating, destroying, and configuring sockets works as you'd expect for any object.  But remember that ØMQ is an asynchronous, elastic fabric.  This has some impact on how we plug sockets into the network topology, and how we use the sockets after that.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Plugging Sockets Into the Topology
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

To create a connection between two nodes you use [http://api.zeromq.org/zmq_bind.html zmq_bind(3)] in one node, and [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] in the other.   As a general rule of thumb, the node which does [http://api.zeromq.org/zmq_bind.html zmq_bind(3)] is a "server", sitting on a well-known network address, and the node which does [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] is a "client", with unknown or arbitrary network addresses.  Thus we say that we "bind a socket to an endpoint" and "connect a socket to an endpoint", the endpoint being that well-known network address.

ØMQ connections are somewhat different from old-fashioned TCP connections.  The main notable differences are:

* They go across an arbitrary transport (inproc:, ipc:, tcp:, pgm: or epgm:).  See [http://api.zeromq.org/zmq_inproc.html zmq_inproc(7)], [http://api.zeromq.org/zmq_ipc.html zmq_ipc(7)], [http://api.zeromq.org/zmq_tcp.html zmq_tcp(7)], [http://api.zeromq.org/zmq_pgm.html zmq_pgm(7)], and [http://api.zeromq.org/zmq_epgm.html zmq_epgm(7)].

* They exist when a client does [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] to an endpoint, whether or not a server has already done [http://api.zeromq.org/zmq_bind.html zmq_bind(3)] to that endpoint.

* They are asynchronous, and have queues that magically exist where and when needed.

* They may express a certain "messaging pattern", according to the type of socket used at each end.

* One socket may have many outgoing and many incoming connections.

* There is no zmq_accept() method.  When a socket is bound to an endpoint it automatically starts accepting connections.

* Your application code cannot work with these connections directly; they are encapsulated under the socket.

Many architectures follow some kind of client-server model, where the server is the component that is most stable, and the clients are the components that are most dynamic, i.e. they come and go the most.  There are sometimes issues of addressing: servers will be visible to clients, but not necessarily vice-versa.  So mostly it's obvious which node should be doing [http://api.zeromq.org/zmq_bind.html zmq_bind(3)] (the server) and which should be doing [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] (the client).  It also depends on the kind of sockets you're using, with some exceptions for unusual network architectures.  We'll look at socket types later.

Now, imagine we start the client //before// we start the server.  In traditional networking we get a big red Fail flag.  But ØMQ lets us start and stop pieces arbitrarily.  As soon as the client node does [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] the connection exists and that node can start to write messages to the socket.  At some stage (hopefully before messages queue up so much that they start to get discarded, or the client blocks), the server comes alive, does a [http://api.zeromq.org/zmq_bind.html zmq_bind(3)] and ØMQ starts to deliver messages.

A server node can bind to many endpoints and it can do this using a single socket.  This means it will accept connections across different transports:

[[code type="C"]]
zmq_bind (socket, "tcp://*:5555");
zmq_bind (socket, "tcp://*:9999");
zmq_bind (socket, "ipc://myserver.ipc");
[[/code]]

You cannot bind to the same endpoint twice, that will cause an exception.

Each time a client node does a [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] to any of these endpoints, the server node's socket gets another connection.  There is no inherent limit to how many connections a socket can have.  A client node can also connect to many endpoints using a single socket.

In most cases, which node acts as client, and which as server, is about network topology rather than message flow.  However, there //are// cases (resending when connections are broken) where the same socket type will behave differently if it's a server or if it's a client.

What this means is that you should always think in terms of "servers" as stable parts of your topology, with more-or-less fixed endpoint addresses, and "clients" as dynamic parts that come and go.  Then, design your application around this model.  The chances that it will "just work" are much better like that.

Sockets have types.  The socket type defines the semantics of the socket, its policies for routing messages inwards and outwards, queueing, etc.  You can connect certain types of socket together, e.g. a publisher socket and a subscriber socket.  Sockets work together in "messaging patterns".  We'll look at this in more detail later.

It's the ability to connect sockets in these different ways that gives ØMQ its basic power as a message queuing system.  There are layers on top of this, such as devices and topic routing, which we'll get to later.  But essentially, with ØMQ you define your network architecture by plugging pieces together like a child's construction toy.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Using Sockets to Carry Data
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

To send and receive messages you use the [http://api.zeromq.org/zmq_send.html zmq_send(3)] and [http://api.zeromq.org/zmq_recv.html zmq_recv(3)] methods.  The names are conventional but ØMQ's I/O model is different enough from TCP's model that you will need time to get your head around it.

[[=image http://github.com/imatix/zguide/raw/master/images/fig10.png]]

Let's look at the main differences between TCP sockets and ØMQ sockets when it comes to carrying data:

* ØMQ sockets carry messages, rather than bytes (as in TCP) or frames (as in UDP).  A message is a length-specified blob of binary data.  We'll come to messages shortly, their design is optimized for performance and thus somewhat tricky to understand.

* ØMQ sockets do their I/O in a background thread.  This means that messages arrive in a local input queue, and are sent from a local output queue, no matter what your application is busy doing.  These are configurable memory queues, by the way.

* ØMQ sockets can, depending on the socket type, be connected to (or from, it's the same) many other sockets.  Where TCP emulates a one-to-one phone call, ØMQ implements one-to-many (like a radio broadcast), many-to-many (like a post office), many-to-one (like a mail box), and even one-to-one.

* ØMQ sockets can send to many endpoints (creating a fan-out model), or receive from many endpoints (creating a fan-in model).

[[=image http://github.com/imatix/zguide/raw/master/images/fig11.png]]

So writing a message to a socket may send the message to one or many other places at once, and conversely, one socket will collect messages from all connections sending messages to it.  The [http://api.zeromq.org/zmq_recv.html zmq_recv(3)] method uses a fair-queuing algorithm so each sender gets an even chance.

The [http://api.zeromq.org/zmq_send.html zmq_send(3)] method does not actually send the message to the socket connection(s).  It queues the message so that the I/O thread can send it asynchronously.  It does not block except in some exception cases.  So the message is not necessarily sent when [http://api.zeromq.org/zmq_send.html zmq_send(3)] returns to your application.  If you created a message using [http://api.zeromq.org/zmq_msg_init_data.html zmq_msg_init_data(3)] you cannot reuse the data or free it, otherwise the I/O thread will rapidly find itself writing overwritten or unallocated garbage.  This is a common mistake for beginners.  We'll see a little later how to properly work with messages.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Unicast Transports
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ provides a set of unicast transports (inproc, ipc, and tcp) and multicast transports (epgm, pgm).  Multicast is an advanced technique that we'll come to later.  Don't even start using it unless you know that your fanout ratios will make 1-to-N unicast impossible.

For most common cases, use **tcp**, which is a //disconnected TCP// transport.  It is elastic, portable, and fast enough for most cases.  We call this 'disconnected' because ØMQ's {{tcp}} transport doesn't require that the endpoint exists before you connect to it.  Clients and servers can connect and bind at any time, can go and come back, and it remains transparent to applications.

The inter-process transport, **ipc**, is like {{tcp}} except that it is abstracted from the LAN, so you don't need to specify IP addresses or domain names.  This makes it better for some purposes, and we use it quite often in the examples in this book.  ØMQ's {{ipc}} transport is disconnected, like {{tcp}}.  It has one limitation: it does not work on Windows.  This may be fixed in future versions of ØMQ.  By convention we use endpoint names with an ".ipc" extension to avoid potential conflict with other file names.  On UNIX systems, if you use {{ipc}} endpoints you need to create these with appropriate permissions otherwise they may not be shareable between processes running under different user ids.

The inter-thread transport, **inproc**, is a connected signaling transport.  It is much faster than {{tcp}} or {{ipc}}.  This transport has a specific limitation compared to {{ipc}} and {{tcp}}: **you must do bind before connect**.  This is something future versions of ØMQ may fix, but for today it has some impact on the way you use {{inproc}} sockets.  When we use this in examples, we carefully bind to each endpoint before connecting to it.  If we connect first, and then bind, the connect will return an error, and if we ignore that error, the recv will block.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ ØMQ is Not a Neutral Carrier
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

A common question that newcomers to ØMQ ask (it's one I asked myself) is something like, "//how do I write a XYZ server in ØMQ?//"  For example, "how do I write an HTTP server in ØMQ?"

The implication is that if we use normal sockets to carry HTTP requests and responses, we should be able to use ØMQ sockets to do the same, only much faster and better.

Sadly the answer is "this is not how it works".  ØMQ is not a neutral carrier, it imposes a framing on the transport protocols it uses.  This framing is not compatible with existing protocols, which tend to use their own framing.  For example, here is an HTTP request, and a ØMQ request, both over TCP/IP:

[[=image http://github.com/imatix/zguide/raw/master/images/fig12.png]]

Where the HTTP request uses CR-LF as its simplest framing delimiter, and ØMQ uses a length-specified frame:

[[=image http://github.com/imatix/zguide/raw/master/images/fig13.png]]

So you could write a HTTP-like protocol using ØMQ, using for example the request-reply socket pattern.  But it would not be HTTP.

There is however a good answer to the question, "how can I make profitable use of ØMQ when making my new XYZ server?"  You need to implement whatever protocol you want to speak in any case, but you can connect that protocol server (which can be extremely thin) to a ØMQ backend that does the real work.  The beautiful part here is that you can then extend your backend with code in any language, running locally or remotely, as you wish.  Zed Shaw's [http://www.mongrel2.org Mongrel2] web server is a great example of such an architecture.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ I/O Threads
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We said that ØMQ does I/O in a background thread.  One I/O thread (for all sockets) is sufficient for all but the most extreme applications.  This is the magic '1' that we use when creating a context, meaning "use one I/O thread":

[[code type="C"]]
void *context;
context = zmq_init (1);
[[/code]]

There is a major difference between a ØMQ application and a conventional networked application, which is that you don't create one socket per connection. One socket handles all incoming and outcoming connections for a particular point of work.  E.g. when you publish to a thousand subscribers, it's via one socket.  When you distribute work among twenty services, it's via one socket.  When you collect data from a thousand web applications, it's via one socket.

This has a fundamental impact on how you write applications.  A traditional networked application has one process or one thread per remote connection, and that process or thread handles one socket.  ØMQ lets you collapse this entire structure into a single thread, and then break it up as necessary for scaling.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ The Messaging Patterns
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Underneath the brown paper wrapping of ØMQ's socket API lies the world of messaging patterns.  If you have a background in enterprise messaging, these will be vaguely familiar.  But to most ØMQ newcomers they are a surprise, we're so used to the TCP paradigm where a socket represents another node.

Let's recap briefly what ØMQ does for you.  It delivers blobs of data (messages) to nodes, quickly and efficiently.  You can map nodes to threads, processes, or boxes.  It gives your applications a single socket API to work with, no matter what the actual transport (like in-process, inter-process, TCP, or multicast).  It automatically reconnects to peers as they come and go.  It queues messages at both sender and receiver, as needed.  It manages these queues carefully to ensure processes don't run out of memory, overflowing to disk when appropriate.  It handles socket errors.  It does all I/O in background threads.  It uses lock-free techniques for talking between nodes, so there are never locks, waits, semaphores, or deadlocks.

But cutting through that, it routes and queues messages according to precise recipes called //patterns//.  It is these patterns that provide ØMQ's intelligence.  They encapsulate our hard-earned experience of the best ways to distribute data and work.  ØMQ's patterns are hard-coded but future versions may allow user-definable patterns.

ØMQ patterns are implemented by pairs of sockets with matching types.  In other words, to understand ØMQ patterns you need to understand socket types and how they work together.  Mostly this just takes learning, there is little that is obvious at this level.

The basic ØMQ patterns are:

* **Request-reply**, which connects a set of clients to a set of services.  This is a remote procedure call and task distribution pattern.

* **Publish-subscribe**, which connects a set of publishers to a set of subscribers.  This is a data distribution pattern.

* **Pipeline**, connects nodes in a fan-out / fan-in pattern that can have multiple steps, and loops.  This is a parallel task distribution and collection pattern.

We looked at each of these in the first chapter.  There's one more pattern that people tend to try to use when they still think of ØMQ in terms of traditional TCP sockets:

* **Exclusive pair**, which connects two sockets in an exclusive pair.  This is a low-level pattern for specific, advanced use-cases.  We'll see an example at the end of this chapter.

The [http://api.zeromq.org/zmq_socket.html zmq_socket(3)] man page is fairly clear about the patterns, it's worth reading several times until it starts to make sense.  We'll look at each pattern and the use-cases it covers.

These are the socket combinations that are valid for a connect-bind pair (either side can bind):

* PUB and SUB
* REQ and REP
* REQ and XREP
* XREQ and REP
* XREQ and XREP
* XREQ and XREQ
* XREP and XREP
* PUSH and PULL
* PAIR and PAIR

Any other combination will produce undocumented and unreliable results and future versions of ØMQ will probably return errors if you try them.  You can and will of course bridge other socket types //via code//, i.e. read from one socket type and write to another.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Working with Messages
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

On the wire, ØMQ messages are blobs of any size from zero upwards, fitting in memory.  You do your own serialization using Google Protocol Buffers, XDR, JSON, or whatever else your applications need to speak.  It's wise to choose a data representation that is portable and fast, but you can make your own decisions about trade-offs.

In memory, ØMQ messages are zmq_msg_t structures (or classes depending on your language).  Here are the basic ground rules for using ØMQ messages in C:

* You create and pass around zmq_msg_t objects, not blocks of data.

* To read a message you use [http://api.zeromq.org/zmq_msg_init.html zmq_msg_init(3)] to create an empty message, and then you pass that to [http://api.zeromq.org/zmq_recv.html zmq_recv(3)].

* To write a message from new data, you use [http://api.zeromq.org/zmq_msg_init_size.html zmq_msg_init_size(3)] to create a message and at the same time allocate a block of data of some size.  You then fill that data using memcpy, and pass the message to [http://api.zeromq.org/zmq_send.html zmq_send(3)].

* To release (not destroy) a message you call [http://api.zeromq.org/zmq_msg_close.html zmq_msg_close(3)].  This drops a reference, and eventually ØMQ will destroy the message.

* To access the message content you use [http://api.zeromq.org/zmq_msg_data.html zmq_msg_data(3)].  To know how much data the message contains, use [http://api.zeromq.org/zmq_msg_size.html zmq_msg_size(3)].

* Do not use [http://api.zeromq.org/zmq_msg_move.html zmq_msg_move(3)], [http://api.zeromq.org/zmq_msg_copy.html zmq_msg_copy(3)], or [http://api.zeromq.org/zmq_msg_init_data.html zmq_msg_init_data(3)] unless you read the man pages and know precisely why you need these.

Here is a typical chunk of code working with messages, which should be familiar if you have been paying attention.  This is from the zhelpers.h file we use in all the examples:

[[code]]
//  Receive ØMQ string from socket and convert into C string
static char *
s_recv (void *socket) {
    zmq_msg_t message;
    zmq_msg_init (&message);
    zmq_recv (socket, &message, 0);
    int size = zmq_msg_size (&message);
    char *string = malloc (size + 1);
    memcpy (string, zmq_msg_data (&message), size);
    zmq_msg_close (&message);
    string [size] = 0;
    return (string);
}

//  Convert C string to ØMQ string and send to socket
static int
s_send (void *socket, char *string) {
    int rc;
    zmq_msg_t message;
    zmq_msg_init_size (&message, strlen (string));
    memcpy (zmq_msg_data (&message), string, strlen (string));
    rc = zmq_send (socket, &message, 0);
    assert (!rc);
    zmq_msg_close (&message);
    return (rc);
}
[[/code]]

You can easily extend this code to send and receive blobs of arbitrary length.

**Note than when you have passed a message to zmq_send(3), ØMQ will clear the message, i.e. set the size to zero. You cannot send the same message twice, and you cannot access the message data after sending it.**

If you want to send the same message more than once, create a second message, initialize it using [http://api.zeromq.org/zmq_msg_init.html zmq_msg_init(3)] and then use [http://api.zeromq.org/zmq_msg_copy.html zmq_msg_copy(3)] to create a copy of the first message.  This does not copy the data but the reference.  You can then send the message twice (or more, if you create more copies) and the message will only be finally destroyed when the last copy is sent or closed.

ØMQ also supports //multipart// messages, which let you handle a list of blobs as a single message.  This is widely used in real applications and we'll look at that later in this chapter and in Chapter Three.

Some other things that are worth knowing about messages:

* ØMQ sends and receives them atomically, i.e. you get a whole message, or you don't get it at all.

* ØMQ does not send a message right away but at some indeterminate later time.

* You can send zero-length messages, e.g. for sending a signal from one thread to another.

* A message must fit in memory.  If you want to send files of arbitrary sizes, you should break them into pieces and send each piece as a separate message.

* You must call [http://api.zeromq.org/zmq_msg_close.html zmq_msg_close(3)] when finished with a message, in languages that don't automatically destroy objects when a scope closes.

And to be necessarily repetitive, do not use [http://api.zeromq.org/zmq_msg_init_data.html zmq_msg_init_data(3)], yet.  This is a zero-copy method and guaranteed to create trouble for you.  There are far more important things to learn about ØMQ before you start to worry about shaving off microseconds.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Handling Multiple Sockets
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

In all the examples so far, the main loop of most examples has been:

# wait for message on socket
# process message
# repeat

What if we want to read from multiple sockets at the same time?  The simplest way is to connect one socket to multiple endpoints and get ØMQ to do the fanin for us.  This is legal if the remote endpoints are in the same pattern but it would be illegal to e.g. connect a PULL socket to a PUB endpoint.  Fun, but illegal.  If you start mixing patterns you break future scalability.

The right way is to use [http://api.zeromq.org/zmq_poll.html zmq_poll(3)].  An even better way might be to wrap [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] in a framework that turns it into a nice event-driven //reactor//. This is perhaps the best solution of all, and we'll make a simple reactor later, but it's significantly more work than we want to cover here.

Let's start with a dirty hack, partly for the fun of not doing it right, but mainly because it lets me show you how to do non-blocking socket reads.  Here is a simple example of reading from two sockets using non-blocking reads.  This rather confused program acts both as a subscriber to weather updates, and a worker for parallel tasks:

[[code type="C" title="Multiple socket reader" name="msreader"]]
//
//  Reading from multiple sockets
//  This version uses a simple recv loop
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    //  Prepare our context and sockets
    void *context = zmq_init (1);

    //  Connect to task ventilator
    void *receiver = zmq_socket (context, ZMQ_PULL);
    zmq_connect (receiver, "tcp://localhost:5557");

    //  Connect to weather server
    void *subscriber = zmq_socket (context, ZMQ_SUB);
    zmq_connect (subscriber, "tcp://localhost:5556");
    zmq_setsockopt (subscriber, ZMQ_SUBSCRIBE, "10001 ", 6);

    //  Process messages from both sockets
    //  We prioritize traffic from the task ventilator
    while (1) {
        //  Process any waiting tasks
        int rc;
        for (rc = 0; !rc; ) {
            zmq_msg_t task;
            zmq_msg_init (&task);
            if ((rc = zmq_recv (receiver, &task, ZMQ_NOBLOCK)) == 0) {
                //  process task
            }
            zmq_msg_close (&task);
        }
        //  Process any waiting weather updates
        for (rc = 0; !rc; ) {
            zmq_msg_t update;
            zmq_msg_init (&update);
            if ((rc = zmq_recv (subscriber, &update, ZMQ_NOBLOCK)) == 0) {
                //  process weather update
            }
            zmq_msg_close (&update);
        }
        //  No activity, so sleep for 1 msec
        struct timespec t;
        t.tv_sec = 0;
        t.tv_nsec = 1000000;
        nanosleep (&t, NULL);
    }
    //  We never get here but clean up anyhow
    zmq_close (receiver);
    zmq_close (subscriber);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/msreader.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/msreader.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/msreader.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/msreader.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/msreader.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/msreader.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/msreader.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/msreader.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/msreader.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/msreader.hs Haskell]//
**[*http://github.com/imatix/zguide/blob/master/examples/Java/msreader.java Java]**
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/msreader.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/msreader.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/msreader.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/msreader.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/msreader.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/msreader.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/msreader.rb Ruby]//
[[/collapsible]]
[[/>]]

The cost of this approach is some additional latency on the first message (the sleep at the end of the loop, when there are no waiting messages to process).  This would be a problem in applications where sub-millisecond latency was vital.  Also, you need to check the documentation for nanosleep() or whatever function you use to make sure it does not busy-loop.

You can treat the sockets fairly by reading first from one, then the second rather than prioritizing them as we did in this example.  This is called "fair-queuing", something that ØMQ does automatically when one socket receives messages from more than one source.

Now let's see the same little senseless application done right, using [http://api.zeromq.org/zmq_poll.html zmq_poll(3)]:

[[code type="C" title="Multiple socket poller" name="mspoller"]]
//
//  Reading from multiple sockets
//  This version uses zmq_poll()
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  Connect to task ventilator
    void *receiver = zmq_socket (context, ZMQ_PULL);
    zmq_connect (receiver, "tcp://localhost:5557");

    //  Connect to weather server
    void *subscriber = zmq_socket (context, ZMQ_SUB);
    zmq_connect (subscriber, "tcp://localhost:5556");
    zmq_setsockopt (subscriber, ZMQ_SUBSCRIBE, "10001 ", 6);

    //  Initialize poll set
    zmq_pollitem_t items [] = {
        { receiver, 0, ZMQ_POLLIN, 0 },
        { subscriber, 0, ZMQ_POLLIN, 0 }
    };
    //  Process messages from both sockets
    while (1) {
        zmq_msg_t message;
        zmq_poll (items, 2, -1);
        if (items [0].revents & ZMQ_POLLIN) {
            zmq_msg_init (&message);
            zmq_recv (receiver, &message, 0);
            //  Process task
            zmq_msg_close (&message);
        }
        if (items [1].revents & ZMQ_POLLIN) {
            zmq_msg_init (&message);
            zmq_recv (subscriber, &message, 0);
            //  Process weather update
            zmq_msg_close (&message);
        }
    }
    //  We never get here
    zmq_close (receiver);
    zmq_close (subscriber);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/mspoller.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/mspoller.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/mspoller.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/mspoller.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/mspoller.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/mspoller.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/mspoller.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/mspoller.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/mspoller.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/mspoller.hs Haskell]//
**[*http://github.com/imatix/zguide/blob/master/examples/Java/mspoller.java Java]**
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/mspoller.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/mspoller.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/mspoller.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/mspoller.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/mspoller.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/mspoller.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/mspoller.rb Ruby]//
[[/collapsible]]
[[/>]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Handling Errors and ETERM
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ's error handling philosophy is a mix of fail-fast and resilience.  Processes, we believe, should be as vulnerable as possible to internal errors, and as robust as possible against external attacks and errors.  To give an analogy, a living cell will self-destruct if it detects a single internal error, yet it will resist attack from the outside by all means possible.  Assertions, which pepper the ØMQ code, are absolutely vital to robust code, they just have to be on the right side of the cellular wall.  And there should be such a wall.  If it is unclear whether a fault is internal or external, that is a design flaw that needs to be fixed.

In C, assertions stop the application immediately with an error.  In other languages you may get exceptions or halts.

When ØMQ detects an external faults it returns an error to the calling code.  In some rare cases it drops messages silently, if there is no obvious strategy for recovering from the error.  In a few places ØMQ still asserts on external faults, but these are considered bugs.

In most of the C examples we've seen so far there's been no error handling.  **Real code should do error handling on every single ØMQ call**.  If you're using a language binding other than C, the binding may handle errors for you.  In C you do need to do this yourself.  There are some simple rules, starting with POSIX conventions:

* Methods that create objects will return NULL in case they fail.

* Other methods will return 0 on success and other values (mostly -1) on an exceptional condition (usually failure).

* The error code is provided in {{errno}} or [http://api.zeromq.org/zmq_errno.html zmq_errno(3)].

* A descriptive error text for logging is provided by [http://api.zeromq.org/zmq_strerror.html zmq_strerror(3)].

There are two main exceptional conditions that you may want to handle as non-fatal:

* When a thread calls [http://api.zeromq.org/zmq_recv.html zmq_recv(3)] with the NOBLOCK option and there is no waiting data.  ØMQ will return -1 and set errno to EAGAIN.

* When a thread calls [http://api.zeromq.org/zmq_term.html zmq_term(3)] and other threads are doing blocking work.  The [http://api.zeromq.org/zmq_term.html zmq_term(3)] call closes the context and all blocking calls exit with -1, and errno set to ETERM.

What this boils down to is that in most cases you can use assertions on ØMQ calls, like this:

[[code type="C"]]
    void *context = zmq_init (1);
    assert (context);
    void *socket = zmq_socket (context, ZMQ_REP);
    assert (socket);
    int rc;
    rc = zmq_bind (socket, "tcp://*:5555");
    assert (rc == 0);
[[/code]]

In the first version of this code I put the assert() call around the function.  Not a good idea, since an optimized build will turn all assert() macros to null and happily wallop those functions.  Use a return code, and assert the return code.

Let's see how to shut down a process cleanly.  We'll take the parallel pipeline example from the previous section.  If we've start a whole lot of workers in the background, we now want to kill them when the batch is finished.  Let's do this by sending a kill message to the workers.  The best place to do this is the sink, since it really knows when the batch is done.

How do we connect the sink to the workers?  The PUSH/PULL sockets are one-way only.  The standard ØMQ answer is: create a new socket flow for each type of problem you need to solve.  We'll use a publish-subscribe model to send kill messages to the workers:

* The sink creates a PUB socket on a new endpoint.
* Workers bind their input socket to this endpoint.
* When the sink detects the end of the batch it sends a kill to its PUB socket.
* When a worker detects this kill message, it exits.

It doesn't take much new code in the sink:

[[code]]
    void *control = zmq_socket (context, ZMQ_PUB);
    zmq_bind (control, "tcp://*:5559");
    ...
    //  Send kill signal to workers
    zmq_msg_init_data (&message, "KILL", 5);
    zmq_send (control, &message, 0);
    zmq_msg_close (&message);
[[/code]]

[[=image http://github.com/imatix/zguide/raw/master/images/fig14.png]]

Here is the worker process, which manages two sockets (a PULL socket getting tasks, and a SUB socket getting control commands) using the [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] technique we saw earlier:

[[code type="C" title="Parallel task worker with kill signalling" name="taskwork2"]]
//
//  Task worker - design 2
//  Adds pub-sub flow to receive and respond to kill signal
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  Socket to receive messages on
    void *receiver = zmq_socket (context, ZMQ_PULL);
    zmq_connect (receiver, "tcp://localhost:5557");

    //  Socket to send messages to
    void *sender = zmq_socket (context, ZMQ_PUSH);
    zmq_connect (sender, "tcp://localhost:5558");

    //  Socket for control input
    void *controller = zmq_socket (context, ZMQ_SUB);
    zmq_connect (controller, "tcp://localhost:5559");
    zmq_setsockopt (controller, ZMQ_SUBSCRIBE, "", 0);

    //  Process messages from receiver and controller
    zmq_pollitem_t items [] = {
        { receiver, 0, ZMQ_POLLIN, 0 },
        { controller, 0, ZMQ_POLLIN, 0 }
    };
    //  Process messages from both sockets
    while (1) {
        zmq_msg_t message;
        zmq_poll (items, 2, -1);
        if (items [0].revents & ZMQ_POLLIN) {
            zmq_msg_init (&message);
            zmq_recv (receiver, &message, 0);

            //  Process task
            int workload;           //  Workload in msecs
            struct timespec t;
            sscanf ((char *) zmq_msg_data (&message), "%d", &workload);
            t.tv_sec = 0;
            t.tv_nsec = workload * 1000000;

            //  Do the work
            nanosleep (&t, NULL);

            //  Send results to sink
            zmq_msg_init (&message);
            zmq_send (sender, &message, 0);

            //  Simple progress indicator for the viewer
            printf (".");
            fflush (stdout);

            zmq_msg_close (&message);
        }
        //  Any waiting controller command acts as 'KILL'
        if (items [1].revents & ZMQ_POLLIN)
            break;                      //  Exit loop
    }
    //  Finished
    zmq_close (receiver);
    zmq_close (sender);
    zmq_close (controller);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/taskwork2.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/taskwork2.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/taskwork2.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/taskwork2.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/taskwork2.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/taskwork2.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/taskwork2.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/taskwork2.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/taskwork2.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/taskwork2.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/taskwork2.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/taskwork2.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/taskwork2.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/taskwork2.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/taskwork2.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/taskwork2.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/taskwork2.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/taskwork2.rb Ruby]//
[[/collapsible]]
[[/>]]

Here is the modified sink application.  When it's finished collecting results it broadcasts a KILL message to all workers:

[[code type="C" title="Parallel task sink with kill signalling" name="tasksink2"]]
//
//  Task sink - design 2
//  Adds pub-sub flow to send kill signal to workers
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  Socket to receive messages on
    void *receiver = zmq_socket (context, ZMQ_PULL);
    zmq_bind (receiver, "tcp://*:5558");

    //  Socket for worker control
    void *controller = zmq_socket (context, ZMQ_PUB);
    zmq_bind (controller, "tcp://*:5559");

    //  Wait for start of batch
    char *string = s_recv (receiver);
    free (string);

    //  Start our clock now
    struct timeval tstart;
    gettimeofday (&tstart, NULL);

    //  Process 100 confirmations
    int task_nbr;
    for (task_nbr = 0; task_nbr < 100; task_nbr++) {
        char *string = s_recv (receiver);
        free (string);
        if ((task_nbr / 10) * 10 == task_nbr)
            printf (":");
        else
            printf (".");
        fflush (stdout);
    }
    //  Calculate and report duration of batch
    struct timeval tend, tdiff;
    gettimeofday (&tend, NULL);

    if (tend.tv_usec < tstart.tv_usec) {
        tdiff.tv_sec = tend.tv_sec - tstart.tv_sec - 1;
        tdiff.tv_usec = 1000000 + tend.tv_usec - tstart.tv_usec;
    }
    else {
        tdiff.tv_sec = tend.tv_sec - tstart.tv_sec;
        tdiff.tv_usec = tend.tv_usec - tstart.tv_usec;
    }
    int total_msec = tdiff.tv_sec * 1000 + tdiff.tv_usec / 1000;
    printf ("Total elapsed time: %d msec\n", total_msec);

    //  Send kill signal to workers
    s_send (controller, "KILL");

    //  Finished
    sleep (1);              //  Give 0MQ time to deliver

    zmq_close (receiver);
    zmq_close (controller);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/tasksink2.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/tasksink2.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/tasksink2.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/tasksink2.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/tasksink2.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/tasksink2.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/tasksink2.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/tasksink2.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/tasksink2.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/tasksink2.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/tasksink2.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/tasksink2.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/tasksink2.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/tasksink2.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/tasksink2.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/tasksink2.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/tasksink2.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/tasksink2.rb Ruby]//
[[/collapsible]]
[[/>]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Multipart Messages
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ lets us compose a message out of several frames, giving us a 'multipart message'.  Realistic applications use multipart messages heavily, especially to make "envelopes".  We'll look at them later.  What we'll learn now is simply how to safely (but blindly) read and write multipart messages because otherwise the devices we write won't work with applications that use multipart messages.

When you work with multipart messages, each part is a zmq_msg item.  E.g. if you are sending a message with five parts, you must construct, send, and destroy five zmq_msg items.  You can do this in advance (and store the zmq_msg items in an array or structure), or as you send them, one by one.

Here is how we send the frames in a multipart message (we receive each frame into a message object):

[[code]]
zmq_send (socket, &message, ZMQ_SNDMORE);
...
zmq_send (socket, &message, ZMQ_SNDMORE);
...
zmq_send (socket, &message, 0);
[[/code]]

Here is how we receive and process all the parts in a message, be it single part or multipart:

[[code]]
while (1) {
    zmq_msg_t message;
    zmq_msg_init (&message);
    zmq_recv (socket, &message, 0);
    //  Process the message part
    zmq_msg_close (&message);
    int64_t more;
    size_t more_size = sizeof (more);
    zmq_getsockopt (socket, ZMQ_RCVMORE, &more, &more_size);
    if (!more)
        break;      //  Last message part
}
[[/code]]

Some things to know about multipart messages:

* When you send a multipart message, the first part (and all following parts) are only sent when you send the final part.

* If you are using [http://api.zeromq.org/zmq_poll.html zmq_poll(3)], when you receive the first part of a message, all the rest have also arrived.

* You will receive all parts of a message, or none at all.

* Each part of a message is a separate zmq_msg item.

* You will receive all parts of a message whether or not you check the RCVMORE option.

* On sending, ØMQ queues message parts in memory until the last is received, then sends them all.

* There is no way to cancel a partially sent message, except by closing the socket.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Intermediates and Devices
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Any connected set hits a complexity curve as the number of set members increases.  A small number of members can all know about each other but as the set gets larger, the cost to each member of knowing all other interesting members grows linearly, and the overall cost of connecting members grows factorially.  The solution is to break sets into smaller ones, and use intermediates to connect the sets.

This pattern is extremely common in the real world and is why our societies and economies are filled with intermediaries who have no other real function than to reduce the complexity and scaling costs of larger networks.  Intermediaries are typically called wholesalers, distributors, managers, etc.

A ØMQ network like any cannot grow beyond a certain size without needing intermediaries.  In ØMQ, we call these "devices".  When we use ØMQ we usually start building our applications as a set of nodes on a network with the nodes talking to each other, without intermediaries:

[[=image http://github.com/imatix/zguide/raw/master/images/fig15.png]]

And then we extend the application across a wider network, placing devices in specific places and scaling up the number of nodes:

[[=image http://github.com/imatix/zguide/raw/master/images/fig16.png]]

ØMQ devices generally connect a set of 'frontend' sockets to a set of 'backend' sockets, though there are no strict design rules.  They ideally run with no state, so that it becomes possible to stretch applications over as many intermediates as needed.  You can run them as threads within a process, or as stand-alone processes.  ØMQ provides some very basic devices but you will in practice develop your own.

ØMQ devices can do intermediation of addresses, services, queues, or any other abstraction you care to define above the message and socket layers.  Different messaging patterns have different complexity issues and need different kinds of intermediation.  For example, request-reply works well with queue and service abstractions, while publish-subscribe works well with streams or topics.

What's interesting about ØMQ as compared to traditional centralized brokers is that you can place devices precisely where you need them, and they can do the optimal intermediation.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ A Publish-Subscribe Proxy Server
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

It is a common requirement to extend a publish-subscribe architecture over more than one network segment or transport.  Perhaps there are a group of subscribers sitting at a remote location.  Perhaps we want to publish to local subscribers via multicast, and to remote subscribers via TCP.

We're going to write a simple proxy server that sits in between a publisher and a set of subscribers, bridging two networks.  This is perhaps the simplest case of a useful device.  The device has two sockets, a frontend facing the internal network, where the weather server is sitting, and a backend facing subscribers on the external network.  It subscribes to the weather service on the frontend socket, and republishes its data on the backend socket:

[[code type="C" title="Weather update proxy" name="wuproxy"]]
//
//  Weather proxy device
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  This is where the weather server sits
    void *frontend = zmq_socket (context, ZMQ_SUB);
    zmq_connect (frontend, "tcp://192.168.55.210:5556");

    //  This is our public endpoint for subscribers
    void *backend  = zmq_socket (context, ZMQ_PUB);
    zmq_bind (backend, "tcp://10.1.1.0:8100");

    //  Subscribe on everything
    zmq_setsockopt (frontend, ZMQ_SUBSCRIBE, "", 0);

    //  Shunt messages out to our own subscribers
    while (1) {
        while (1) {
            zmq_msg_t message;
            int64_t more;

            //  Process all parts of the message
            zmq_msg_init (&message);
            zmq_recv (frontend, &message, 0);
            size_t more_size = sizeof (more);
            zmq_getsockopt (frontend, ZMQ_RCVMORE, &more, &more_size);
            zmq_send (backend, &message, more? ZMQ_SNDMORE: 0);
            zmq_msg_close (&message);
            if (!more)
                break;      //  Last message part
        }
    }
    zmq_close (frontend);
    zmq_close (backend);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/wuproxy.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/wuproxy.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/wuproxy.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/wuproxy.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/wuproxy.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/wuproxy.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/wuproxy.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/wuproxy.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/wuproxy.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/wuproxy.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/wuproxy.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/wuproxy.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/wuproxy.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/wuproxy.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/wuproxy.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/wuproxy.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/wuproxy.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/wuproxy.rb Ruby]//
[[/collapsible]]
[[/>]]

We call this a //proxy// because it acts as a subscriber to publishers, and acts as a publisher to subscribers. That means you can slot this device into an existing network without affecting it (of course the new subscribers need to know to speak to the proxy).

[[=image http://github.com/imatix/zguide/raw/master/images/fig17.png]]

Note that this application is multipart safe.  It correctly detects multipart messages and sends them as it read them.  If we did not set the SNDMORE option on outgoing multipart data, the final recipient would get a corrupted message.  You should always make your devices multipart safe so that there is no risk they will corrupt the data they switch.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ A Request-Reply Broker
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Let's explore how to solve a problem of scale by writing a little message queuing broker in ØMQ.  We'll look at the request-reply pattern for this case.

In the Hello World client-server application we have one client that talks to one service.  However in real cases we usually need to allow multiple services as well as multiple clients.  This lets us scale up the power of the service (many threads or processes or boxes rather than just one).  The only constraint is that services must be stateless, all state being in the request or in some shared storage such as a database.

There are two ways to connect multiple clients to multiple servers.  The brute-force way is to connect each client socket to multiple service endpoints.  One client socket can connect to multiple service sockets, and requests are load-balanced among these services.  Let's say you connect a client socket to three service endpoints, A, B, and C.  The client makes requests R1, R2, R3, R4.  R1 and R4 go to service A, R2 goes to B, and R3 goes to service C.

[[=image http://github.com/imatix/zguide/raw/master/images/fig18.png]]

This design lets you add more clients cheaply.  You can also add more services.  Each client will load-balance its requests to the services.  But each client has to know the service topology.  If you have 100 clients and then you decide to add three more services, you need to reconfigure and restart 100 clients in order for the clients to know about the three new services.

That's clearly not the kind of thing we want to be doing at 3am when our supercomputing cluster has run out of resources and we desperately need to add a couple of hundred new service nodes.  Too many stable pieces are like liquid concrete: knowledge is distributed and the more stable pieces you have, the more effort it is to change the topology.  What we want is something sitting in between clients and services that centralizes all knowledge of the topology.  Ideally, we should be able to add and remove services or clients at any time without touching any other part of the topology.

So we'll write a little message queuing broker that gives us this flexibility.  The broker binds to two endpoints, a frontend for clients and a backend for services.  It then uses [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] to monitor these two sockets for activity and when it has some, it shuttles messages between its two sockets.  It doesn't actually manage any queues explicitly -- ØMQ does that automatically on each socket.

When you use REQ to talk to REP you get a strictly synchronous request-reply dialog.  The client sends a request, the service reads the request and sends a reply.  The client then reads the reply.  If either the client or the service try to do anything else (e.g. sending two requests in a row without waiting for a response) they will get an error.

But our broker has to be non-blocking.  Obviously we can use [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] to wait for activity on either socket, but we can't use REP and REQ.

Luckily there are non-blocking versions of these two sockets, called XREQ and XREP.  These "extended request/reply" sockets let you extend request-reply across intermediate nodes, such as our message queuing broker.

When we extend request-reply, REQ talks to XREP and XREQ talks to REP.  In between the XREQ and XREP we have to have code (like our broker) that pulls messages off the one socket and shoves them onto the other:

[[=image http://github.com/imatix/zguide/raw/master/images/fig19.png]]

The request-reply broker binds to two endpoints, one for clients to connect to (the frontend socket) and one for services to connect to (the backend).  To test this broker, you will want to change your services so they connect to the backend socket.  Here are a Python client and service that show what I mean:

[[code type="Python" title="Request-reply client" name="rrclient"]]
#
#   Request-reply client in Python
#   Connects REQ socket to tcp://localhost:5559
#   Sends "Hello" to server, expects "World" back
#
import zmq

#  Prepare our context and sockets
context = zmq.Context()
socket = context.socket(zmq.REQ)
socket.connect("tcp://localhost:5559")

#  Do 10 requests, waiting each time for a response
for request in range(1,10):
    socket.send("Hello")
    message = socket.recv()
    print "Received reply ", request, "[", message, "]"
[[/code]]
[[>]]
examples/Python/rrclient.py
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rrclient.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rrclient.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rrclient.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rrclient.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/rrclient.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rrclient.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rrclient.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rrclient.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rrclient.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rrclient.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rrclient.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rrclient.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rrclient.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rrclient.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rrclient.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rrclient.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rrclient.rb Ruby]//
[[/collapsible]]
[[/>]]

[[code type="Python" title="Request-reply service" name="rrserver"]]
#
#   Request-reply service in Python
#   Connects REP socket to tcp://localhost:5560
#   Expects "Hello" from client, replies with "World"
#
import zmq

context = zmq.Context()
socket = context.socket(zmq.REP)
socket.connect("tcp://localhost:5560")

while True:
    message = socket.recv()
    print "Received request: ", message
    socket.send("World")
[[/code]]
[[>]]
examples/Python/rrserver.py
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rrserver.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rrserver.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rrserver.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rrserver.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/rrserver.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rrserver.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rrserver.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rrserver.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rrserver.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rrserver.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rrserver.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rrserver.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rrserver.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rrserver.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rrserver.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rrserver.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rrserver.rb Ruby]//
[[/collapsible]]
[[/>]]

And here is the broker, in C.  You will see that it's multipart safe:

[[code type="C" title="Request-reply broker" name="rrbroker"]]
//
//  Simple request-reply broker
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    //  Prepare our context and sockets
    void *context = zmq_init (1);
    void *frontend = zmq_socket (context, ZMQ_XREP);
    void *backend  = zmq_socket (context, ZMQ_XREQ);
    zmq_bind (frontend, "tcp://*:5559");
    zmq_bind (backend,  "tcp://*:5560");

    //  Initialize poll set
    zmq_pollitem_t items [] = {
        { frontend, 0, ZMQ_POLLIN, 0 },
        { backend,  0, ZMQ_POLLIN, 0 }
    };
    //  Switch messages between sockets
    while (1) {
        zmq_msg_t message;
        int64_t more;           //  Multipart detection

        zmq_poll (items, 2, -1);
        if (items [0].revents & ZMQ_POLLIN) {
            while (1) {
                //  Process all parts of the message
                zmq_msg_init (&message);
                zmq_recv (frontend, &message, 0);
                size_t more_size = sizeof (more);
                zmq_getsockopt (frontend, ZMQ_RCVMORE, &more, &more_size);
                zmq_send (backend, &message, more? ZMQ_SNDMORE: 0);
                zmq_msg_close (&message);
                if (!more)
                    break;      //  Last message part
            }
        }
        if (items [1].revents & ZMQ_POLLIN) {
            while (1) {
                //  Process all parts of the message
                zmq_msg_init (&message);
                zmq_recv (backend, &message, 0);
                size_t more_size = sizeof (more);
                zmq_getsockopt (backend, ZMQ_RCVMORE, &more, &more_size);
                zmq_send (frontend, &message, more? ZMQ_SNDMORE: 0);
                zmq_msg_close (&message);
                if (!more)
                    break;      //  Last message part
            }
        }
    }
    //  We never get here but clean up anyhow
    zmq_close (frontend);
    zmq_close (backend);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/rrbroker.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rrbroker.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rrbroker.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rrbroker.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rrbroker.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/rrbroker.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rrbroker.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rrbroker.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rrbroker.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rrbroker.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rrbroker.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rrbroker.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rrbroker.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rrbroker.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rrbroker.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rrbroker.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/rrbroker.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rrbroker.rb Ruby]//
[[/collapsible]]
[[/>]]

Using a request-reply broker makes your client-server architectures easier to scale since clients don't see services, and services don't see clients.  The only stable node is the device in the middle:

[[=image http://github.com/imatix/zguide/raw/master/images/fig20.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Built-in Devices
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ provides some built-in devices, though most advanced users write their own devices.  The built-in devices are:

* QUEUE, which is like the request-reply broker.
* FORWARDER, which is like the pub-sub proxy server.
* STREAMER, which is like FORWARDER but for pipeline flows.

To start a device, you call [http://api.zeromq.org/zmq_device.html zmq_device(3)] and pass it two sockets, one for the frontend and one for the backend:

[[code]]
zmq_device (ZMQ_QUEUE, frontend, backend);
[[/code]]

Which if you start a QUEUE device is exactly like plugging the main body of the request-reply broker into your code at that spot.  You need to create the sockets, bind or connect them, and possibly configure them, before calling [http://api.zeromq.org/zmq_device.html zmq_device(3)].  It is trivial to do.  Here is the request-reply broker re-written to call QUEUE and rebadged as an expensive-sounding "message queue" (people have charged houses for code that did less):

[[code type="C" title="Message queue broker" name="msgqueue"]]
//
//  Simple message queuing broker
//  Same as request-reply broker but using QUEUE device
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  Socket facing clients
    void *frontend = zmq_socket (context, ZMQ_XREP);
    zmq_bind (frontend, "tcp://*:5559");

    //  Socket facing services
    void *backend = zmq_socket (context, ZMQ_XREQ);
    zmq_bind (backend, "tcp://*:5560");

    //  Start built-in device
    zmq_device (ZMQ_QUEUE, frontend, backend);

    //  We never get here...
    zmq_close (frontend);
    zmq_close (backend);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/msgqueue.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/msgqueue.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/msgqueue.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/msgqueue.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/msgqueue.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/msgqueue.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/msgqueue.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/msgqueue.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/msgqueue.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/msgqueue.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/msgqueue.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/msgqueue.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/msgqueue.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/msgqueue.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/msgqueue.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/msgqueue.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/msgqueue.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/msgqueue.rb Ruby]//
[[/collapsible]]
[[/>]]

The built-in devices do proper error handling, whereas the examples we have shown don't.  Since you can configure the sockets as you need to, before starting the device, it's worth using the built-in devices when you can.

If you're like most ØMQ users, at this stage your mind is starting to think, "//what kind of evil stuff can I do if I plug random socket types into devices?//"  The short answer is: don't do it.  You can mix socket types but the results are going to be weird.  So stick to using XREP/XREQ for queue devices, SUB/PUB for forwarders and PULL/PUSH for streamers.

When you start to need other combinations, it's time to write your own devices.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Multithreading with ØMQ
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ is perhaps the nicest way ever to write multithreaded (MT) applications.  Whereas as ØMQ sockets require some readjustment if you are used to traditional sockets, ØMQ multithreading will take everything you know about writing MT applications, throw it into a heap in the garden, pour gasoline over it, and set it alite.  It's a rare book that deserves burning, but most books on concurrent programming do.

To make utterly perfect MT programs (and I mean that literally) **we don't need mutexes, locks, or any other form of inter-thread communication except messages sent across ØMQ sockets.**

By "perfect" MT programs I mean code that's easy to write and understand, that works with one technology in any language and on any operating system, and that scales across any number of CPUs with zero wait states and no point of diminishing returns.

If you've spent years learning tricks to make your MT code work at all, let alone rapidly, with locks and semaphores and critical sections, you will be disgusted when you realize it was all for nothing.  If there's one lesson we've learned from 30+ years of concurrent programming it is: //just don't share state//. It's like two drunkards trying to share a beer.  It doesn't matter if they're good buddies. Sooner or later they're going to get into a fight. And the more drunkards you add to the pavement, the more they fight each other over the beer.  The tragic majority of MT applications look like drunken bar fights.

The list of weird problems that you need to fight as you write classic shared-state MT code would be hillarious if it didn't translate directly into stress and risk, as code that seems to work suddenly fails under stress.  Here is a list of "11 Likely Problems In Your Multithreaded Code" from a large firm with experience in buggy code: forgotten synchronization, incorrect granularity, read and write tearing, lock-free reordering, lock convoys, two-step dance, and priority inversion.

Yeah, we also counted seven, not eleven.  That's not the point though.  The point is, do you really want that code running the power grid or stock market to start getting two-step lock convoys at 3pm on a busy Thursday?  Who cares what the terms actually mean.  This is not what turned us on to programming, fighting ever more complex side-effects with ever more complex hacks.

Some widely used metaphors, despite being the basis for billion-dollar industries, are fundamentally broken, and shared state concurrency is one of them.  Code that wants to scale without limit does it like the Internet does, by sending messages and sharing nothing except a common contempt for broken programming metaphors.

You do need to follow some rules to write MT code with ØMQ:

* You MUST NOT try to access the same data from multiple threads.

* You MUST create a 'context' object for your process, and pass that to all threads.  The context collects ØMQ's state.  To create a connection across the inproc: transport, both server and client thread must share the same context object.

* You MUST NOT share sockets between threads, not even when using proper exclusion mechanisms like semaphores, locks or mutexes.  ØMQ will in some future make it possible to move sockets to different threads.  Right now the thread which creates a socket is the only thread that may use it.

* If you have to work with existing drunkard code, find a way to isolate yourself from it.  Don't use mutexes, semaphores, critical sections, but use ØMQ messages.

If you need to start more than one device in an application, for example, you will want to run each in their own thread.  It is easy to make the error of creating the device sockets in one thread, and then passing the sockets to the device in another thread.  This may appear to work but will fail randomly.  Remember: //You cannot use sockets except in the thread that created them.//

If you follow these rules, you can quite easily split threads into separate processes, when you need to.  Application logic can sit in threads, processes, boxes: whatever your scale needs.

ØMQ uses native OS threads rather than virtual "green" threads.  The advantage is that you don't need to learn any new threading API, and that ØMQ threads map cleanly to your operating system.  You can use standard tools like Intel's ThreadChecker to see what your application is doing.  The disadvantages are that your code, when it for instance starts new threads, won't be portable, and that if you have a huge number of threads (thousands), some operating systems will get stressed.

Let's see how this works in pratice.  We'll turn our old Hello World server into something more capable.  The original server was a single thread.  If the work per request is low, that's fine: one ØMQ thread can run at full speed on a CPU core, with no waits, doing an awful lot of work.  But realistic servers have to do non-trivial work per request.  A single core may not be enough when 10,000 clients hit the server all at once.  So a realistic server must starts multiple worker threads.  It then accepts requests as fast as it can, and distributes these to its worker threads.  The worker threads grind through the work, and eventually send their replies back.

You can of course do all this using a queue device and external worker processes, but often it's easier to start one process that gobbles up sixteen cores, than sixteen processes, each gobbling up one core.  Further, running workers as threads will cut out a network hop, latency, and network traffic.

The MT version of the Hello World service basically collapses the queue device and workers into a single process:

[[code type="C" title="Multithreaded Service" name="mtserver"]]
//
//  Multithreaded Hello World server
//
#include "zhelpers.h"

static void *
worker_routine (void *context) {
    //  Socket to talk to dispatcher
    void *receiver = zmq_socket (context, ZMQ_REP);
    zmq_connect (receiver, "inproc://workers");

    while (1) {
        char *string = s_recv (receiver);
        printf ("Received request: [%s]\n", string);
        free (string);
        //  Do some 'work'
        sleep (1);
        //  Send reply back to client
        s_send (receiver, "World");
    }
    return (NULL);
}

int main () {
    //  Prepare our context and sockets
    void *context = zmq_init (1);

    //  Socket to talk to clients
    void *clients = zmq_socket (context, ZMQ_XREP);
    zmq_bind (clients, "tcp://*:5555");

    //  Socket to talk to workers
    void *workers = zmq_socket (context, ZMQ_XREQ);
    zmq_bind (workers, "inproc://workers");

    //  Launch pool of worker threads
    int thread_nbr;
    for (thread_nbr = 0; thread_nbr != 5; thread_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_routine, context);
    }
    //  Connect work threads to client threads via a queue
    zmq_device (ZMQ_QUEUE, clients, workers);

    //  We never get here but clean up anyhow
    zmq_close (clients);
    zmq_close (workers);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/mtserver.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/mtserver.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/mtserver.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/mtserver.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/mtserver.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/mtserver.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/mtserver.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/mtserver.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/mtserver.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/mtserver.hs Haskell]//
**[*http://github.com/imatix/zguide/blob/master/examples/Java/mtserver.java Java]**
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/mtserver.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/mtserver.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/mtserver.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/mtserver.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/mtserver.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/mtserver.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/mtserver.rb Ruby]//
[[/collapsible]]
[[/>]]

All the code should be recognizable to you by now.  How it works:

* The server starts a set of worker threads.  Each worker thread creates a REP socket and then processes requests on this socket.  Worker threads are just like single-threaded servers.  The only differences are the transport (inproc: instead of tcp:), and the bind-connect direction.

* The server creates an XREP (extended reply) socket to talk to clients and binds this to its external interface (over tcp:).

* The server creates an XREQ (extended request) socket to talk to the workers and binds this to its internal interface (over inproc:).

* The server starts a QUEUE device that connects the two sockets.  The QUEUE device keeps a single queue for incoming requests, and distributes those out to workers.  It also routes replies back to their origin.

Here the 'work' is just a one-second pause.  We could do anything in the workers, including talking to other nodes.  This is what the MT server looks like in terms of ØMQ sockets and nodes.  Note how the request-reply chain is {{REQ-XREP-queue-XREQ-REP}}:

[[=image http://github.com/imatix/zguide/raw/master/images/fig21.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Thread Coordination
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

When you start making multithreaded applications with ØMQ, you'll hit the question of how to coordinate your threads.  Though you might be tempted to insert 'sleep' statements, or use multithreading techniques such as semaphores or mutexes, **the only mechanism that you should use are ØMQ messages**.  Remember the story of The Drunkards and the Beer Bottle.  Here is a simple example showing three threads that signal each other when they are ready.

[[=image http://github.com/imatix/zguide/raw/master/images/fig22.png]]

In this example we use PAIR sockets over the //inproc:// transport:

[[code type="C" title="Multithreaded relay" name="mtrelay"]]
//
//  Multithreaded relay
//
#include "zhelpers.h"

static void *
step1 (void *context) {
    //  Signal downstream to step 2
    void *sender = zmq_socket (context, ZMQ_PAIR);
    zmq_connect (sender, "inproc://step2");
    s_send (sender, "");

    return NULL;
}

static void *
step2 (void *context) {
    //  Bind to inproc: endpoint, then start upstream thread
    void *receiver = zmq_socket (context, ZMQ_PAIR);
    zmq_bind (receiver, "inproc://step2");
    pthread_t thread;
    pthread_create (&thread, NULL, step1, context);

    //  Wait for signal
    char *string = s_recv (receiver);
    free (string);

    //  Signal downstream to step 3
    void *sender = zmq_socket (context, ZMQ_PAIR);
    zmq_connect (sender, "inproc://step3");
    s_send (sender, "");

    return NULL;
}

int main () {
    void *context = zmq_init (1);

    //  Bind to inproc: endpoint, then start upstream thread
    void *receiver = zmq_socket (context, ZMQ_PAIR);
    zmq_bind (receiver, "inproc://step3");
    pthread_t thread;
    pthread_create (&thread, NULL, step2, context);

    //  Wait for signal
    char *string = s_recv (receiver);
    free (string);

    printf ("Test successful!\n");
    zmq_close (receiver);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/mtrelay.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/mtrelay.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/mtrelay.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/mtrelay.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/mtrelay.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/mtrelay.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/mtrelay.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/mtrelay.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/mtrelay.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/mtrelay.hs Haskell]//
**[*http://github.com/imatix/zguide/blob/master/examples/Java/mtrelay.java Java]**
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/mtrelay.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/mtrelay.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/mtrelay.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/mtrelay.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/mtrelay.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/mtrelay.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/mtrelay.rb Ruby]//
[[/collapsible]]
[[/>]]

This is the first time we've shown an example using PAIR sockets.  Why use PAIR?  Other socket combinations might seem to work but they all have side-effects that could interfere with signalling:

* You can use PUSH for the sender and PULL for the receiver.  This looks simple and will work, but remember that PUSH will load-balance messages to all available receivers.  If you by accident start two receivers (e.g. you already have one running and you start a second), you'll "lose" half of your signals.  PAIR has the advantage of refusing more than one connection, the pair is //exclusive//.

* You can use XREQ for the sender and XREP for the receiver.  XREP however wraps your message in an "envelope", meaning your zero-size signal turns into a multipart message.  If you don't care about the data, and treat anything as a valid signal, and if you don't read more than once from the socket, that won't matter.  If however you decide to send real data, you will suddenly find XREP providing you with "wrong" messages.  XREQ also load-balances, giving the same risk as PUSH.

* You can use PUB for the sender and SUB for the receiver.  This will correctly deliver you messages exactly as you sent them and PUB does not load-balance as PUSH or XREQ do.  However you need to configure the subscriber with an empty subscription, which is annoying.  Worse, the reliability of the PUB-SUB link is timing dependent and messages can get lost if the SUB socket is connecting while the PUB socket is sending its message.

For these reasons, PAIR makes the best choice for coordination between specific threads.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Node Coordination
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

When you want to coordinate nodes, PAIR sockets won't work well any more.  This is one of the few areas where the strategies for threads and nodes are different.  Principally nodes come and go whereas threads are stable.  PAIR sockets do not automatically reconnect if the remote node goes away and comes back.

The second significant difference between threads and nodes is that you typically have a fixed number of threads but a more variable number of nodes.  Let's take one of our earlier scenarios (the weather server and clients) and use node coordination to ensure that subscribers don't lose data when starting up.

This is how the application will work:

* The publisher knows in advance how many subscribers it expects.  This is just a magic number it gets from somewhere.

* The publisher starts up and waits for all subscribers to connect.  This is the node coordination part.  Each subscriber subscribes and then tells the publisher it's ready via another socket.

* When the publisher has all subscribers connected, it starts to publish data.

In this case we'll use a REQ-REP socket flow to synchronize subscribers and publisher.  Here is the publisher:

[[code type="C" title="Synchronized publisher" name="syncpub"]]
//
//  Synchronized publisher
//
#include "zhelpers.h"

//  We wait for 10 subscribers
#define SUBSCRIBERS_EXPECTED  10

int main () {
    void *context = zmq_init (1);

    //  Socket to talk to clients
    void *publisher = zmq_socket (context, ZMQ_PUB);
    zmq_bind (publisher, "tcp://*:5561");

    //  Socket to receive signals
    void *syncservice = zmq_socket (context, ZMQ_REP);
    zmq_bind (syncservice, "tcp://*:5562");

    //  Get synchronization from subscribers
    int subscribers = 0;
    while (subscribers < SUBSCRIBERS_EXPECTED) {
        //  - wait for synchronization request
        char *string = s_recv (syncservice);
        free (string);
        //  - send synchronization reply
        s_send (syncservice, "");
        subscribers++;
    }
    //  Now broadcast exactly 1M updates followed by END
    int update_nbr;
    for (update_nbr = 0; update_nbr < 1000000; update_nbr++)
        s_send (publisher, "Rhubarb");

    s_send (publisher, "END");

    sleep (1);              //  Give 0MQ/2.0.x time to flush output
    zmq_close (publisher);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/syncpub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/syncpub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/syncpub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/syncpub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/syncpub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/syncpub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/syncpub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/syncpub.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/syncpub.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/syncpub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/syncpub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/syncpub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/syncpub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/syncpub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/syncpub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/syncpub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/syncpub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/syncpub.rb Ruby]//
[[/collapsible]]
[[/>]]

[[=image http://github.com/imatix/zguide/raw/master/images/fig23.png]]

And here is the subscriber:

[[code type="C" title="Synchronized subscriber" name="syncsub"]]
//
//  Synchronized subscriber
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  First, connect our subscriber socket
    void *subscriber = zmq_socket (context, ZMQ_SUB);
    zmq_connect (subscriber, "tcp://localhost:5561");
    zmq_setsockopt (subscriber, ZMQ_SUBSCRIBE, "", 0);

    //  Second, synchronize with publisher
    void *syncclient = zmq_socket (context, ZMQ_REQ);
    zmq_connect (syncclient, "tcp://localhost:5562");

    //  - send a synchronization request
    s_send (syncclient, "");

    //  - wait for synchronization reply
    char *string = s_recv (syncclient);
    free (string);

    //  Third, get our updates and report how many we got
    int update_nbr = 0;
    while (1) {
        char *string = s_recv (subscriber);
        if (strcmp (string, "END") == 0) {
            free (string);
            break;
        }
        free (string);
        update_nbr++;
    }
    printf ("Received %d updates\n", update_nbr);

    zmq_close (subscriber);
    zmq_close (syncclient);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/syncsub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/syncsub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/syncsub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/syncsub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/syncsub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/syncsub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/syncsub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/syncsub.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/syncsub.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/syncsub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/syncsub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/syncsub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/syncsub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/syncsub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/syncsub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/syncsub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/syncsub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/syncsub.rb Ruby]//
[[/collapsible]]
[[/>]]

This Linux shell script will start ten subscribers and then the publisher:

[[code]]
echo "Starting subscribers..."
for a in 1 2 3 4 5 6 7 8 9 10; do
    syncsub &
done
echo "Starting publisher..."
syncpub
[[/code]]

Which gives us this satisfying output:

[[code]]
Starting subscribers...
Starting publisher...
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
Received 1000000 updates
[[/code]]

[!-- TODO: remove this note when ØMQ/2.1 is released --]
Note that we do {{sleep (1);}} before exiting the publisher.  This is a hack that gets around ØMQ/2.0's design, which discards messages that have not yet been sent, if you exit the program too soon.  If you are using ØMQ/2.1 you can remove this sleep statement.  There is no way in ØMQ/2.0 to properly exit the publisher without such a sleep.

The example assumes that the SUB connect will be finished by the time the REQ/REP dialog is complete.  However there are no //guarantees// that outbound connects are ordered.  If using a single IO thread, and connecting to the same hostname, it is highly likely.  If you are connecting via two different network routes, i.e. to two different interfaces or hostnames, one route could be arbitrarily slower than the other.

A more robust model would be:

* Publisher opens PUB socket and starts sending "Hello" messages (not data).
* Subscribers connect SUB socket and when they receive a Hello message they tell the publisher via a REQ/REP socket pair.
* When the publisher has had all the necessary confirmations, it starts to send real data.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Zero Copy
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We teased you in Chapter One, when you were still a ØMQ newbie, about zero-copy.  If you survived this far, you are probabably ready to use zero-copy.  However, remember that there are many roads to Hell, and premature optimization is not the most enjoyable nor profitable one, by far.  In English, trying to do zero-copy properly while your architecture is not perfect is a waste of time and will make things worse, not better.

ØMQ's message API lets you can send and receive messages directly from and to application buffers without copying data.  Given that ØMQ sends messages in the background, zero-copy needs some extra sauce.

To do zero-copy you use [http://api.zeromq.org/zmq_msg_init_data.html zmq_msg_init_data(3)] to create a message that refers to a block of data already allocated on the heap with malloc(), and then you pass that to [http://api.zeromq.org/zmq_send.html zmq_send(3)].  When you create the message you also pass a function that ØMQ will call to free the block of data, when it has finished sending the message.  This is the simplest example, assuming 'buffer' is a block of 1000 bytes allocated on the heap:

[[code type="C"]]
void my_free (void *data, void *hint)
{
    free (data);
}
//  Send message from buffer, which we allocate and ØMQ will free for us
zmq_msg_t message;
zmq_msg_init_data (&message, buffer, 1000, my_free, NULL);
zmq_send (socket, &message, 0);
[[/code]]

There is no way to do zero-copy on receive: ØMQ delivers you a buffer that you can store as long as you wish but it will not write data directly into application buffers.

On writing, ØMQ's multipart messages work nicely together with zero-copy.  In traditional messaging you need to marshal different buffers together into one buffer that you can send.  That means copying data.  With ØMQ, you can send multiple buffers coming from different sources as individual message parts.  We send each field as a length-delimited frame.  To the application it looks like a series of send and recv calls.  But internally the multiple parts get written to the network and read back with single system calls, so it's very efficient.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Transient vs. Durable Sockets
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

The concept of a durable socket is one more of those surprisingly obvious ØMQ inventions that when you see, you wonder why no-one ever thought of it before.  In classic networking, sockets are API objects, and their lifespan is never longer than the code that uses them.  But if you look at a socket you see that it collects a bunch of resources - network buffers - and at some stage, a ØMQ user asked, "isn't there some way these could hang around if my program crashes, so I can get them back?"

This turns out to be very useful.  It's not foolproof, but it gives ØMQ a kind of "better than a kick in the nuts" reliability, particularly useful for pubsub cases.  We'll look at that shortly.

Here is the general model of two sockets happily chatting about the weather, and who kissed who and where and when precisely, cause I heard something different, at the last staff party, not to mention did you see that new family up the road who do they think they are with that car and what's with the prices at the shops these days don't they know it's a crisis?

[[=image http://github.com/imatix/zguide/raw/master/images/fig24.png]]

What durable sockets give you is the promise that the ØMQ //transmit buffer// is kept alive as long as the sender exists.  If the receiver crashes, it will lose its receive buffer, and the network will lose its I/O buffers, but the sender can continue to push data into its transmit buffer and the receiver can then pick this up.

Note that ØMQ's transmit and receive buffers are invisible and automatic, just like TCP's buffers are.

All the sockets we've used so far were transient.  To turn a transient socket into a durable one you give it an explicit //identity//.  All ØMQ sockets have identities but by default they are generated 'unique universal identifiers' (UUIDs) that the peer uses to recall who it's talking to.

Behind the scenes, and invisibly to you, when one socket connects to another, the two sockets exchange identities.  Normally sockets don't tell their peers their identity, so peers invent random identities for each other:

[[=image http://github.com/imatix/zguide/raw/master/images/fig25.png]]

But a socket can also tell the other its identity, and then the next time the two meet, it'll be "so as I was saying what I heard was quite different but anyhow you know how it goes at the office, they're all tattletales I'd never say anything about anyone that wasn't true or at least based on a sure thing".

[[=image http://github.com/imatix/zguide/raw/master/images/fig26.png]]

Here's how you set the identity of a socket, to create a durable socket:

[[code]]
zmq_setsockopt (socket, ZMQ_IDENTITY, "Lucy", 4);
[[/code]]

Some comments on setting a socket identity:

* If you want to set an identity you must do it //before// connecting or binding the socket.

* Identities are binary strings: identities starting with a zero byte are reserved for ØMQ use.

* Do not use the same identity for more than one socket.  On ØMQ/2.0.9 and earlier versions this will cause an assertion failure in the //other// socket.  Yes, that's a bug, and yes, it'll be fixed.

* Do not modify the identity of a socket after binding, or if you restart a peer, this will cause an assertion failure in sockets connected to it.  Yes, that's also a bug and yes, it'll also be fixed.

* Do not use random identities in applications that create lots of sockets.  What this will do is cause lots and lots of durable sockets to pile up, eventually crashing nodes.

* If you need to know the identity of the //peer// you got a message from, only the XREP socket does this for you automatically.  For any other socket type you must send the address explicitly, as a message part.

See [http://api.zeromq.org/zmq_setsockopt.html zmq_setsockopt(3)] for a summary of the ZMQ_IDENTITY socket option.  Note that the [http://api.zeromq.org/zmq_getsockopt.html zmq_getsockopt(3)] method gives you the identity of the socket you are working with, //not// any peer it might be connected to.

About these assertion failures, ØMQ shouldn't assert when it's misused, only when it thinks it's actually hitting an internal bug.  Code should be full of assertions that //never// fail.  There are ongoing discussions about how these situations should be handled.  As a general rule, if you get assertions while using ØMQ, report them to the zeromq-dev list.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Pubsub Message Envelopes
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We've looked briefly at multipart messages.  Let's now look at their main use-case, which is //message envelopes//.  An envelope is a way of safely packaging up data with an address, without touching the data itself.

In the pubsub pattern, the envelope at least holds the subscription key for filtering but you can also add the sender identity in the envelope.

If you want to use pubsub envelopes, you make them yourself.  It's optional, and in previous pubsub examples we didn't do this.  Using a pubsub envelope is a little more work for simple cases but it's cleaner especially for real cases, where the key and the data are naturally separate things.  It's also faster, if you are writing the data directly from an application buffer.

Here is what a publish-subscribe message with an envelope looks like:

[[=image http://github.com/imatix/zguide/raw/master/images/fig27.png]]


Recall that pubsub matches messages based on the prefix.  Putting the key into a separate frame makes the matching very obvious, since there is no chance an application will accidentally match on part of the data.

Here is a minimalist example of how pubsub envelopes look in code.  This publisher sends messages of two types, A and B.  The envelope holds the message type:

[[code type="C" title="Pubsub envelope publisher" name="psenvpub"]]
//
//  Pubsub envelope publisher
//  Note that the zhelpers.h file also provides s_sendmore
//
#include "zhelpers.h"

int main () {
    //  Prepare our context and publisher
    void *context = zmq_init (1);
    void *publisher = zmq_socket (context, ZMQ_PUB);
    zmq_bind (publisher, "tcp://*:5563");

    while (1) {
        //  Write two messages, each with an envelope and content
        s_sendmore (publisher, "A");
        s_send (publisher, "We don't want to see this");
        s_sendmore (publisher, "B");
        s_send (publisher, "We would like to see this");
        sleep (1);
    }
    //  We never get here but clean up anyhow
    zmq_close (publisher);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/psenvpub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/psenvpub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/psenvpub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/psenvpub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/psenvpub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/psenvpub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/psenvpub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/psenvpub.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/psenvpub.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/psenvpub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/psenvpub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/psenvpub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/psenvpub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/psenvpub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/psenvpub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/psenvpub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/psenvpub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/psenvpub.rb Ruby]//
[[/collapsible]]
[[/>]]

The subscriber only wants messages of type B:

[[code type="C" title="Pubsub envelope subscriber" name="psenvsub"]]
//
//  Pubsub envelope subscriber
//
#include "zhelpers.h"

int main () {
    //  Prepare our context and subscriber
    void *context = zmq_init (1);
    void *subscriber = zmq_socket (context, ZMQ_SUB);
    zmq_connect (subscriber, "tcp://localhost:5563");
    zmq_setsockopt (subscriber, ZMQ_SUBSCRIBE, "B", 1);

    while (1) {
        //  Read envelope with address
        char *address = s_recv (subscriber);
        //  Read message contents
        char *contents = s_recv (subscriber);
        printf ("[%s] %s\n", address, contents);
        free (address);
        free (contents);
    }
    //  We never get here but clean up anyhow
    zmq_close (subscriber);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/psenvsub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/psenvsub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/psenvsub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/psenvsub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/psenvsub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/psenvsub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/psenvsub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/psenvsub.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/psenvsub.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/psenvsub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/psenvsub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/psenvsub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/psenvsub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/psenvsub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/psenvsub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/psenvsub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/psenvsub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/psenvsub.rb Ruby]//
[[/collapsible]]
[[/>]]

When you run the two programs, the subscriber should show you this:

[[code]]
[B] We would like to see this
[B] We would like to see this
[B] We would like to see this
[B] We would like to see this
...
[[/code]]

This examples shows that the subscription filter rejects or accepts the entire multipart message (key plus data).  You won't get part of a multipart message, ever.

If you subscribe to multiple publishers and you want to know their identity so that you can send them data via another socket (and this is a fairly typical use-case), you create a three-part message:

[[=image http://github.com/imatix/zguide/raw/master/images/fig28.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Making a (Semi-)Durable Subscriber
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Identities work on all socket types.  If you have a PUB and a SUB socket, and the subscriber gives the publisher its identity, the publisher holds onto data until it can deliver it to the subscriber.

This is both wonderful and terrible at the same time.  It's wonderful because it means updates can wait for you in the publisher's transmit buffer, until you connect and collect them.  It's terrible because by default this will rapidly kill a publisher and lock up your system.

**If you use durable subscriber sockets (i.e. if you set the identity on a SUB socket) you //must// also guard against queue explosion by using the //high water mark// or HWM, on the publisher socket.**

If you want to prove this, take the wuclient and wuserver from Chapter One, and add this line to the wuclient before it connects:

[[code]]
    zmq_setsockopt (subscriber, ZMQ_IDENTITY, "Hello", 5);
[[/code]]

Build and run the two programs.  It all looks normal.  But keep an eye on the memory used by the publisher, and you'll see that as the subscriber finishes, the publisher memory grows and grows.  If you restart the subscriber, the publisher queues stop growing.  As soon as the subscriber goes away, they grow again.  It'll rapidly overwhelm your system.

We'll first look at how to do this, and then at how to do it properly.  Here are a publisher and subscriber that use the 'node coordination' technique from Chapter Two to synchronize.  The publisher then sends ten messages, waiting a second between each one.  That wait is for you to kill the subscriber using Ctrl-C, wait a few seconds, and restart it.

Here's the publisher:

[[code type="C" title="Durable publisher" name="durapub"]]
//
//  Publisher for durable subscriber
//
#include "zhelpers.h"

int main () {
    void *context = zmq_init (1);

    //  Subscriber tells us when it's ready here
    void *sync = zmq_socket (context, ZMQ_PULL);
    zmq_bind (sync, "tcp://*:5564");

    //  We send updates via this socket
    void *publisher = zmq_socket (context, ZMQ_PUB);
    zmq_bind (publisher, "tcp://*:5565");

    //  Wait for synchronization request
    char *string = s_recv (sync);
    free (string);

    //  Now broadcast exactly 10 updates with pause
    int update_nbr;
    for (update_nbr = 0; update_nbr < 10; update_nbr++) {
        char string [20];
        sprintf (string, "Update %d", update_nbr);
        s_send (publisher, string);
        sleep (1);
    }
    s_send (publisher, "END");

    sleep (1);              //  Give 0MQ/2.0.x time to flush output

    zmq_close (sync);
    zmq_close (publisher);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/durapub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/durapub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/durapub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/durapub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/durapub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/durapub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/durapub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/durapub.erl Erlang]//
**[*http://github.com/imatix/zguide/blob/master/examples/Go/durapub.go Go]**
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/durapub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/durapub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/durapub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/durapub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/durapub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/durapub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/durapub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/durapub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/durapub.rb Ruby]//
[[/collapsible]]
[[/>]]

And here's the subscriber:

[[code type="C" title="Durable subscriber" name="durasub"]]
//
//  Durable subscriber
//
#include "zhelpers.h"

int main (int argc, char *argv[])
{
    void *context = zmq_init (1);

    //  Connect our subscriber socket
    void *subscriber = zmq_socket (context, ZMQ_SUB);
    zmq_setsockopt (subscriber, ZMQ_IDENTITY, "Hello", 5);
    zmq_setsockopt (subscriber, ZMQ_SUBSCRIBE, "", 0);
    zmq_connect (subscriber, "tcp://localhost:5565");

    //  Synchronize with publisher
    void *sync = zmq_socket (context, ZMQ_PUSH);
    zmq_connect (sync, "tcp://localhost:5564");
    s_send (sync, "");

    //  Get updates, expect random Ctrl-C death
    while (1) {
        char *string = s_recv (subscriber);
        printf ("%s\n", string);
        if (strcmp (string, "END") == 0) {
            free (string);
            break;
        }
        free (string);
    }
    zmq_close (subscriber);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/durasub.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/durasub.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/durasub.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/durasub.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/durasub.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/durasub.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/durasub.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/durasub.erl Erlang]//
**[*http://github.com/imatix/zguide/blob/master/examples/Go/durasub.go Go]**
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/durasub.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/durasub.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/durasub.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/durasub.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/durasub.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/durasub.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/durasub.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/durasub.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/durasub.rb Ruby]//
[[/collapsible]]
[[/>]]

To run this, start the publisher, then the subscriber, each in their own window.  Allow the subscriber to collect one or two messages, then Ctrl-C it.  Count to three, and restart it.  What you will see is something like this:

[[code]]
$ durasub
Update 0
Update 1
Update 2
^C
$ durasub
Update 3
Update 4
Update 5
Update 6
Update 7
^C
$ durasub
Update 8
Update 9
END
[[/code]]

Just to see the difference, comment out the line in the subscriber that sets the socket identity, and try again.  You will see that it loses messages.  Setting an identity turns a transient subscriber into a durable subscriber.  You would in practice want to choose identities carefully, either taking them from configuration files, or generating UUIDs and storing them somewhere.

When we set a high-water mark on the PUB socket, the publisher stores that many messages, but no more.  Let's test this by setting the publisher HWM to 2 messages, before we start publishing to the socket:

[[code]]
uint64_t hwm = 2;
zmq_setsockopt (publisher, ZMQ_HWM, &hwm, sizeof (hwm));
[[/code]]

Now running our test, killing and restarting the subscriber after a couple of seconds' pause will show something like this:

[[code]]
$ durasub
Update 0
Update 1
^C
$ durasub
Update 2
Update 3
Update 7
Update 8
Update 9
END
[[/code]]

Look carefully: we have two messages kept for us (2 and 3), then a gap of several messages, and then new updates again.  The HWM causes ØMQ to drop messages it can't put onto the queue, something the ØMQ Reference Manual calls an "exceptional condition".

In short, if you use subscriber identities, you must set the high-water mark on publisher sockets, or else you risk servers that run out of memory and crash.  However, there is a way out.  ØMQ provides something called a "swap", which is a disk file that holds messages we can't store to the queue.  It is very simple to enable:

[[code]]
//  Specify swap space in bytes
uint64_t swap = 25000000;
zmq_setsockopt (publisher, ZMQ_SWAP, &swap, sizeof (swap));
[[/code]]

We can put this together to make a cynical publisher that is immune to slow, blocked, or absent subscribers while still offering durable subscriptions to those that need it:

[[code type="C" title="Durable but cynical publisher" name="durapub2"]]
//
//  Publisher for durable subscriber
//
#include "zhelpers.h"

int main () {
    void *context = zmq_init (1);

    //  Subscriber tells us when it's ready here
    void *sync = zmq_socket (context, ZMQ_PULL);
    zmq_bind (sync, "tcp://*:5564");

    //  We send updates via this socket
    void *publisher = zmq_socket (context, ZMQ_PUB);
    zmq_bind (publisher, "tcp://*:5565");

    //  Prevent publisher overflow from slow subscribers
    uint64_t hwm = 1;
    zmq_setsockopt (publisher, ZMQ_HWM, &hwm, sizeof (hwm));

    //  Specify swap space in bytes, this covers all subscribers
    uint64_t swap = 25000000;
    zmq_setsockopt (publisher, ZMQ_SWAP, &swap, sizeof (swap));

    //  Wait for synchronization request
    char *string = s_recv (sync);
    free (string);

    //  Now broadcast exactly 10 updates with pause
    int update_nbr;
    for (update_nbr = 0; update_nbr < 10; update_nbr++) {
        char string [20];
        sprintf (string, "Update %d", update_nbr);
        s_send (publisher, string);
        sleep (1);
    }
    s_send (publisher, "END");

    sleep (1);              //  Give 0MQ/2.0.x time to flush output

    zmq_close (sync);
    zmq_close (publisher);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/durapub2.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/durapub2.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/durapub2.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/durapub2.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/durapub2.cpp C++]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%23/durapub2.cs C#]**
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/durapub2.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/durapub2.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/durapub2.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/durapub2.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/durapub2.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/durapub2.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/durapub2.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/durapub2.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/durapub2.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/durapub2.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/durapub2.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/durapub2.rb Ruby]//
[[/collapsible]]
[[/>]]

In practice, setting the HWM to 1 and shoving everything to disk will make a pubsub system very slow.  Here is a more reasonable 'best practice' for publishers that have to deal with unknown subscribers:

* Always set a HWM on the socket, based on the expected maximum number of subscribers, the amount of memory you are willing to dedicated to queuing, and the average size of a message.  For example if you expect up to 5,000 subscribers, and have 1GB of memory to play with, and messages of ~200 bytes, then a safe HWM would be (1,000,000,000 / 200 / 5,000) = 1,000.

* If you don't want slow or crashing subscribers to lose data, set a SWAP that's large enough to handle the peaks, based on the number of subscribers, peak message rate, average size of messages, and time you want to cover.  For example with 5,000 subscribers and messages of ~200 bytes coming in at 100,000 per second, you will need up to 100MB of disk space per second.  To cover an outage of up to 1 minute, therefore, you'd need 6GB of disk space, and it would have to be fast, but that's a different story.

Some notes on durable subscribers:

* Depending on how the subscriber dies, and the frequency of updates, and the size of network buffers, and the transport protocol you are using, data may be lost.  Durable subscribers will have //much better// reliability than transient ones, but they will not be perfect.

* The SWAP file is not recoverable, so if a publisher dies and restarts, it will lose data that was in its transmit buffers, and that was in the network I/O buffers.

Some notes on using the HWM option:

* This affects both the transmit and receive buffers of a single socket.  Some sockets (PUB, PUSH) only have transmit buffers.  Some (SUB, PULL, REQ, REP) only have receive buffers.  Some (XREQ, XREP, PAIR) have both transmit and receive buffers.

* Future versions of ØMQ might offer high-water marks on transmit and receive buffers separately.  However this isn't something people seem particularly worried about.

* When your socket reaches its high-water mark, it will either block or drop data depending on the socket type.  PUB sockets will drop data if they reach their high-water mark, while other socket types will block.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ A Bare Necessity
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

ØMQ is like a box of pieces that plug together, the only limitation being your imagination and sobriety.

The scalable elastic architecture you get should be an eye-opener.  You might need a coffee or two first.  Don't make the mistake I made once and buy exotic German coffee labeled //Entkoffeiniert//.  That does not mean "Delicious".  Scalable elastic architectures are not a new idea - [http://en.wikipedia.org/wiki/Flow-based_programming flow-based programming] and languages like [http://www.erlang.org/ Erlang] already worked like this - but ØMQ makes it easier to use than ever before.

As [http://permalink.gmane.org/gmane.network.zeromq.devel/2145 Gonzo Diethelm said], '//My gut feeling is summarized in this sentence: "if ØMQ didn't exist, it would be necessary to invent it". Meaning that I ran into ØMQ after years of brain-background processing, and it made instant sense... ØMQ simply seems to me a "bare necessity" nowadays.//'
