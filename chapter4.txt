.output chapter4.wd
++ Chapter 4 - Reliability

In Chapter Three we looked at advanced use of 0MQ's request-reply pattern with worked examples. In this chapter we'll look at the general question of reliability and learn how to build reliable messaging on top of 0MQ's patterns.

+++ Necessary Philosophy

++++ What is "Reliability"?

If you're using tcp://, ipc:// or inproc:// transports then messages don't just get lost. It's not as if messages need a GPS to get to their destination. Transports like TCP are explicitly designed to retry and resend, to the point of silliness sometimes. So to understand what reliability means, we have to look at its opposite, namely *failure*. If we can handle a certain set of failures, we are reliable with respect to those failures. No more, no less.

So let's look at the possible causes of failure in a distributed application, in descending order of probability:

* Application code is the worst offender. It can crash and exit, freeze and stop responding to input, run too slowly for its input, exhaust all memory, etc.
* System code - like brokers we write using 0MQ - can die. System code should be more reliable than application code but can still crash and burn, and especially run out of memory if it tries to compensate for slow clients.
* Message queues can overflow, typically in system code that has learned to deal brutally with slow clients. When a queue overflows, it starts to discard messages.
* Hardware can fail and take with it all the processes running on that box.
* Networks can fail in exotic ways, e.g. some ports on a switch may die and those parts of the network become inaccessible.
* Entire data centers can be struck by lightning, earthquakes, fire, or more mundane power or cooling failures.

To make a software system fully reliable against *all* of these possible failures is an enormously difficult and expensive job and goes beyond the scope of this modest guide.

Since the first three or four cases cover 99.9% of real world requirements outside large companies (according to a highly scientific study I just ran), that's what we'll look at. If you're a large company with money to spend on the last two cases, contact me immediately, there's a large hole behind my beach house waiting to be converted into a pool.

++++ The Slow Client Problem

In any high-volume architecture (data or workload distribution), applications need to be able to keep up with incoming data. The problem is that application developers too often don't have the skills to write fast code, or use languages that are inherently slow, or deploy to boxes that can easily run very slowly. Even a fast, well-written client application can appear to run "slowly" if the network is congested, or the application gets temporarily disconnected from the server.

Handling slow clients correctly is delicate. On the one hand, you really don't want to face application developers with an excuse like "sorry, our messaging layer lost your message somewhere". On the other hand, if you allow message queues to build up, especially in publishers that handle many clients, things just break.

0MQ does two things to handle the slow client problem:

* It moves messages as rapidly as possible to the client, and queues them there. In all the asynchronous messaging patterns (that is, all except synchronous request-reply), messages are sent to their destination without pause. By itself, this strategy avoids the bulk of queue overflow problems. If the client application runs out of memory, we don't really care.

* For cases where network congestion or client disconnection stops the sender getting rid of messages, 0MQ offers a "high water mark" that limits the size of a given socket queue. Since each use case has its own needs, 0MQ lets you set this per outgoing socket. When a queue hits the HWM, messages are just dropped. It's brutal, but there is no other sane strategy.

++++ Pattern Reliability

So to make things brutally simple, reliability is "keeping things working properly when code freezes or crashes", a situation we'll shorten to "dies". However the things we want to keep working properly are more complex than just messages. We need to take each 0MQ messaging pattern and see how to make it work (if we can) even when code dies.

Let's take them one by one:

* Request-reply: if the server dies (while processing a request), the client can figure that out since it won't get an answer back. Then it can give up in a huff, wait and try again later, find another server, etc. As for the client dying, we can brush that off as "someone else's problem" for now.

* Publish-subscribe: if the client dies (having gotten some data), the server doesn't know about it. Pubsub doesn't send any information back from client to server. But the client can contact the server out-of-band, e.g. via request-reply, and ask, "please resend everything I missed". As for the server dying, that's out of scope for here.

* Pipeline: if a worker dies (while working), the ventilator doesn't know about it. Pipelines, like pubsub, and the grinding gears of time, only work in one direction. But the downstream collector can detect that one task didn't get done, and send a message back to the ventilator saying, "hey, resend task 324!" If the ventilator or collector die, then whatever upstream client originally sent the work batch can get tired of waiting and resend the whole lot. It's not elegant but system code should really not die often enough to matter.

++++ Disk-based Reliability

You can, and people do, use spinning rust to store messages. It rather makes a mess of the idea of "performance" but we're usually more comfortable knowing a really important message (such as that transfer of $400M to my Cyprus account) is stored on disk rather than only in memory. Spinning rust only makes sense for some patterns, mainly request-reply. If we get bored in this chapter we'll play with that, but otherwise, just shove really critical messages into a database that all parties can access, and skip 0MQ for those parts of your dialog.

+++ Reliable Request-Reply - the Pirate Pattern

The basic request-reply pattern (a REQ socket talking to a REP socket) fails miserably if the  server or network dies, so it's basically unreliable. By default the REQ socket blocks on a recv, meaning the client just waits forever. If there's a problem down the line, too bad. I'd probably only use the basic request-reply pattern between two threads in the same process where there's no network or separate server process to die.

However, with a little extra work it becomes a good basis for real work across a distributed network, and we get a reliable pattern I like to call the "Pirate" pattern. RRR! The minimum Pirate sits just in the client, which needs to:

* Poll the REQ socket and only recv from it when it's sure a reply has arrived.
* Resend a request several times, it no reply arrived within a timeout period.
* Abandon the transaction if after several requests, there is still no reply.

Rather than rely on a single server, which may die and be unavailable for a long period, Pirate works better with a pool of servers. A good model is live-live redundant, i.e. a client can send a request to any server. If one server dies, the others take over. In practice that usually means servers need some common storage, e.g. shared access to a single database. To work with a pool of servers, we need to:

* Know a list of servers to connect to, rather than a single server. This makes configuration and management more work, but there are answers for that.
* Choose a server to send a request to. The least-recently-used routing from Chapter 3 is suitable here, it means we won't send a request to a server that is not up and running.
* Detect a server that dies while processing a request. We do this with a timeout: if the server does not reply within a certain time, we treat it as dead.
* Track dead servers so we don't send requests to them (again). A simple list will work.

With a pool of servers, we can make the client's retry strategy smarter:

* If there is just one server in the pool, the client waits with a timeout for the server to reply. If the server does not reply within the timeout, the client will retry a number of times before abandoning.
* If there are multiple servers in the pool, the client tries each server in succession, but does not retry the same server twice.
* If a server appears to be really dead (i.e. has not responded for some time), the client removes it from its pool.

While many Pirate use cases are *idempotent* (i.e. executing the same request more than once is safe), some are not. Examples of an idempotent Pirate include:

* Stateless task distribution, i.e. a collapsed pipeline where the client is both ventillator and sink, and the servers are stateless workers that compute a reply based purely on the state provided by a request. In such a case it's safe (though inefficient) to execute the same request many times.
* A name service that translates logical addresses into endpoints to bind or connect to. In such a case it's safe to make the same lookup request many times.

And here are examples of a non-idempotent Pirate pattern:

* A logging service. One does not want the same log information recorded more than once.
* Any service that has impact on downstream nodes, e.g. sends on information to other nodes. If that service gets the same request more than once, downstream nodes will get duplicate information.
* Any service that modifies shared data in some non-idempotent way. E.g. a service that debits a bank account is definitely not idempotent.

When our server is not idempotent, we have to think more carefully about when exactly a server can crash. If it dies when it's idle, or while it's processing a request, that's usually fine. We can use database transactions to make sure a debit and a credit are always done together, if at all. If the server dies while sending its reply, that's a problem, because as far as its concerned, it's done its work.

if the network dies just as the reply is making its way back to the client, the same situation arises. The client will think the server died, will resend the request, and the server will do the same work twice. Which is not what we want.

We use the standard solution of detecting and rejecting duplicate requests. This means:

* The client must stamp every request with a unique client identifier and a unique message number.
* The server, before sending back a reply, stores it using the client id + message number as a key.
* The server, when getting a request from a given client, first checks if it has a reply for that client id + message number. If so, it does not process the request but just resends the reply.

The final touch to a robust Pirate pattern is server heartbeating. This means getting the server to say "hello" every so often even when it's not doing any work. The smoothest design is where the client pings the server, which pongs back. We don't need to send heartbeats to a working server, only one that's idle. Knowing when an idle server has died means we don't uselessly send requests to dead servers, which improves response time in those cases.

We'll explore Pirates with running code later. For now, let's focus our attention on another

+++ Reliable Publish-Subscribe - the Cache Pattern

Pubsub is inherently unreliable. In fact it's so easy to lose messages with this pattern that you might wonder why 0MQ bothers to implement it at all. :-)


The basic request-reply pattern (a REQ socket talking to a REP socket) fails miserably if the  server or network dies, so it's basically unreliable. By default the REQ socket blocks on a recv, meaning the client just waits forever. If there's a problem down the line, too bad. I'd probably only use the basic request-reply pattern between two threads in the same process where there's no network or separate server process to die.


We'll look at the common question of getting a consistent state across a set of subscribers that come and go at any time. We will build a simple distributed cache:

* We have a set of peers that each hold a copy of the cache
* Any peer can update its copy of the cache at any time
* Updates are published to all other peers
* A peer can join the network at any time, and get the cache state

I'll leave the design for later. We'll need a forwarder to act as stable location in the network. Should be fun!







++++ Centralized (Queue-based) RPC

In the general case we have many clients and many servers. We could start with a single client and server but this is Chapter 4, and we're moving fast now. Grab another coffee. When we have N to N nodes, as a rule we stick a device in the middle, because it makes things simpler.

This is the architecture, which is a lot like the LRU (least-recently used) queue broker from Chapter 3:

[[code type="textdiagram"]]

    +-----------+   +-----------+   +-----------+
    |           |   |           |   |           |
    |  Client   |   |  Client   |   |  Client   |
    |           |   |           |   |           |
    +-----------+   +-----------+   +-----------+
    |    REQ    |   |    REQ    |   |    REQ    |
    \-----------/   \-----------/   \-----------/
          ^               ^               ^
          |               |               |
          \---------------+---------------/
                          |
                          v
                    /-----------\
                    |   XREP    |
                    +-----------+
                    |           |
                    |   Queue   |
                    |           |
                    +-----------+
                    |   XREP    |
                    \-----------/
                          ^
                          |
          /---------------+---------------\
          |               |               |
          v               v               v
    /-----------\   /-----------\   /-----------\
    |   XREQ    |   |   XREQ    |   |   XREQ    |
    +-----------+   +-----------+   +-----------+
    |           |   |           |   |           |
    |  Server   |   |  Server   |   |  Server   |
    |           |   |           |   |           |
    +-----------+   +-----------+   +-----------+


           Figure # - Centralized RPC
[[/code]]

The client connects its REQ socket to the queue and sends requests, one by one, waiting each time for an answer. If it doesn't get a reply within a certain time, say 1 second, it retries. After a few retries, it gives up in exasperation.

The server connects its XREQ socket to the queue and uses the LRU worker approach, i.e. it signals when it's ready for a new task by sending a request, and the queue then sends the task as a reply. It does its work, and sends its results back as a new "I'm ready (oh, and BTW here is the stuff I worked on)" request message. When waiting for work, the server sends a heartbeat message (which is an empty message) to the queue each second. This is why the server uses an XREQ socket instead of a REQ socket (which does not allow multiple requests to be sent before a response arrives).

All the complex work happens in the queue, which is another nice thing about the centralized RPC architecture. Clients and servers remain simple. The first nice thing was scalability, even with many servers and clients, you only need one DNS name everyone connects to.

The queue binds to XREP frontend and backend sockets, and handles requests and replies asynchronously on these using the LRU logic we developed in Chapter 3. It works with these data structures:

* A set (a hash map) of all known servers, which identify themselves using unique IDs.
* A list of servers that are ready for work.
* A list of servers that are busy doing work.
* A list of requests sent by clients but not yet successfully processed.

The queue polls all sockets for input and then processes all incoming messages. It queues tasks and distributes them to workers that are alive. Any replies from workers are sent back to their original clients, unless the worker is disabled, in which case the reply is dropped.

Idle workers must signal that they are alive with a ready message or a heartbeat, or they will be marked as disabled until they send a message again. This is to detect blocked and disconnected workers (since 0MQ does not report disconnections).

The queue detects a disabled worker in two ways: heartbeating, as explained, and timeouts on request processing. If a reply does not come back within (e.g.) 10ms, the queue marks the worker as disabled, and retries with the next worker.


The server will randomly simulate two problems when it receives a task:
1. A crash and restart while processing a request, i.e. close its socket, block for 5 seconds, reopen its socket and restart.
2. A temporary busy wait, i.e. sleep 1 second then continue as normal.

.end



++++ Distributed (Peer-to-peer) RPC







++++ Customized Publish-Subscribe

- use identity to route message explicitly to A or B
- not using PUBSUB at all but XREP/????
    - limitations: no multicast, only TCP
    - how to scale with devices...

When a client activates, it chooses a random port that is not in use and creates a SUB socket listening for all traffic on it. The client then sends a message via REQ to the publisher containing the port that it is listening on. The publisher receives this message, acknowledges it, and creates a new pub socket specific to that client. All published events specific to this client go out that socket.

When the client deactivates, it sends a message to the publisher with the port to deactivate and close.

You end up creating a lot more PUB sockets on your server end and doing all of the filtering at the server. This sounds acceptable to you.

I didn't need to do this to avoid network bandwidth bottlenecks; I created this to enforce some security and entitlements.


- chapter 4
  - heartbeating & presence detection
  - reliable request-reply
  -
