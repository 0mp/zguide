.output chapter5.wd
++ Chapter Five - Advanced Publish-Subscribe

In Chapters Three and Four we looked at advanced use of 0MQ's request-reply pattern. If you managed to digest all that, congratulations. In this chapter we'll focus on publish-subscribe, and extend 0MQ's core pub-sub pattern with higher-level patterns for performance, reliability, state distribution, and security.

We'll cover:

* How to handle too-slow subscribers (the //Suicidal Snail// pattern).
* How to design high-speed subscribers (the //Black Box// pattern).
* How to build a shared key-value cache (the //Clone// pattern).
* How to do atomic group multicast (the //Platoon// pattern).
* How to do secure publish-subscribe (the //Need to Know// pattern).

+++ Slow Subscriber Detection (Suicidal Snail Pattern)

A common problem you will hit when using the pub-sub pattern in real life is the slow subscriber. In an ideal world, we stream data at full speed from publishers to subscribers. In reality, subscriber applications are often written in interpreted languages, or just do a lot of work, or are just badly written, to the extent that they can't keep up with publishers.

How do we handle a slow subscriber? The ideal fix is to make the subscriber faster, but that might take work and time. Some of the classic strategies for handling a slow subscriber are:

* **Queue messages on the publisher**. This is what Gmail does when I don't read my email for a couple of hours. But in high-volume messaging, pushing queues upstream has the thrilling but unprofitable result of making publishers run out of memory and crash. Especially if there are lots of subscribers and it's not possible to flush to disk for performance reasons.

* **Queue messages on the subscriber**. This is much better, and it's what 0MQ does by default if the network can keep up with things. If anyone's going to run out of memory and crash, it'll be the subscriber rather than the publisher, which is fair. This is perfect for "peaky" streams where a subscriber can't keep up for a while, but can catch up when the stream slows down. However it's no answer to a subscriber that's simply too slow in general.

* **Stop queuing new messages after a while**. This is what Gmail does when my mailbox overflows its 7.554GB, no 7.555GB of space. New messages just get rejected or dropped. This is a great strategy from the perspective of the publisher, and it's what 0MQ does when the publisher sets a high water mark or HWM. However it still doesn't help us fix the slow subscriber. Now we just get gaps in our message stream.

* **Punish slow subscribers with disconnect**. This is what Hotmail does when I don't login for two weeks, which is why I'm on my fifteenth Hotmail account. It's a nice brutal strategy that forces subscribers to sit up and pay attention, and would be ideal, but 0MQ doesn't do this, and there's no way to layer it on top since subscribers are invisible to publisher applications.

None of these classic strategies fit. So we need to get creative. Rather than disconnect the publisher, let's convince the subscriber to kill itself. This is the Suicidal Snail pattern. When a subscriber detects that it's running too slowly (where "too slowly" is presumably a configured option that really means "so slowly that if you ever get here, shout really loudly because I need to know, so I can fix this!"), it croaks and dies.

How can a subscriber detect this? One way would be to sequence messages (number them in order), and use a HWM at the publisher. Now, if the subscriber detects a gap (i.e. the numbering isn't consecutive), it knows something is wrong. We then tune the HWM to the "croak and die if you hit this" level.

There are two problems with this solution. One, if we have many publishers, how do we sequence messages? The solution is to give each publisher a unique ID and add that to the sequencing. Second, if subscribers use ZMQ_SUBSCRIBE filters, they will get gaps by definition. Our precious sequencing will be for nothing.

Some use cases won't use filters, and sequencing will work for them. But a more general solution is that the publisher timestamps each message. When a subscriber gets a message it checks the time, and if the difference is more than, say, one second, it does the "croak and die" thing. Possibly firing off a squawk to some operator console first.

The Suicide Snail pattern works especially when subscribers have their own clients and service-level agreements and need to guarantee certain maximum latencies. Aborting a subscriber may not seem like a constructive way to guarantee a maximum latency, but it's the assertion model. Abort today, and the problem will be fixed. Allow late data to flow downstream, and the problem may cause wider damage and take longer to appear on the radar.

So here is a minimal example of a Suicidal Snail:

[[code type="example" title="Suicidal Snail" name="suisnail"]]
[[/code]]

Notes about this example:

* The message here consists simply of the current system clock as a number of milliseconds. In a realistic application you'd have at least a message header with the timestamp, and a message body with data.
* The example has subscriber and publisher in a single process, as two threads. In reality they would be separate processes. Using threads is just convenient for the demonstration.

+++ High-speed Subscribers (Black Box Pattern)

A common use case for pub-sub is distributing large data streams. For example, 'market data' coming from stock exchanges. A typical set-up would have a publisher connected to a stock exchange, taking price quotes, and sending them out to a number of subscribers. If there are a handful of subscribers, we could use TCP. If we have a larger number of subscribers, we'd probably use reliable multicast, i.e. {{pgm}}.

Let's imagine our feed has an average of 100,000 100-byte messages a second. That's a typical rate, after filtering market data we don't need to send on to subscribers. Now we decide to record a day's data (maybe 250 GB in 8 hours), and then replay it to a simulation network, i.e. a small group of subscribers. While 100K messages a second is easy for a 0MQ application, we want to replay //much faster//.

So we set-up our architecture with a bunch of boxes, one for the publisher, and one for each subscriber. These are well-specified boxes, eight cores, twelve for the publisher. (If you're reading this in 2015, which is when the Guide is scheduled to be finished, please add a zero to those numbers.)

And as we pump data into our subscribers, we notice two things:

# When we do even the slightest amount of work with a message, it slows down our subscriber to the point where it can't catch up with the publisher again.
# We're hitting a ceiling, at both publisher and subscriber, to around say 6M messages a second, even after careful optimization and TCP tuning.

The first thing we have to do is break our subscriber into a multithreaded design so that we can do work with messages in one set of threads, while reading messages in another. Typically we don't want to process every message the same way. Rather, the subscriber will filter some messages, perhaps by prefix key. When a message matches some criteria, the subscriber will call a worker to deal with it. In 0MQ terms this means sending the message to a worker thread.

So the subscriber looks something like a queue device. We could use various sockets to connect the subscriber and workers. If we assume one-way traffic, and workers that are all identical, we can use PUSH and PULL, and delegate all the routing work to 0MQ. This is the simplest and fastest approach:

[[code type="textdiagram"]]

                        +-----------+   
                        |           |   
                        | Publisher |   
                        |           |
                        +-----------+   
                        |    PUB    |   
                        \-----+-----/   
                              |         
  +---------------------------|---------------------------+
  :                           |                Fast box   :
  :                           v                           :
  :                     /-----------\                     :
  :                     |    SUB    |                     :
  :                     +-----------+                     :
  :                     |           |                     :
  :                     | Subscriber|                     :
  :                     |           |                     :
  :                     +-----------+                     :
  :                     |   PUSH    |                     :
  :                     \-----+-----/                     :
  :                           |                           :
  :                           |                           :
  :           /---------------+---------------\           :
  :           |               |               |           :
  :           v               v               v           :
  :     /-----------\   /-----------\   /-----------\     :
  :     |   PULL    |   |   PULL    |   |   PULL    |     :
  :     +-----------+   +-----------+   +-----------+     :
  :     |           |   |           |   |           |     :
  :     |  Worker   |   |  Worker   |   |  Worker   |     :
  :     |           |   |           |   |           |     :
  :     +-----------+   +-----------+   +-----------+     :
  :                                                       :
  +-------------------------------------------------------+

           Figure # - Simple Black Box Pattern
[[/code]]

The subscriber talks to the publisher over TCP or PGM. The subscriber talks to its workers, which are all in the same process, over inproc.

Now to break that ceiling. What happens is that the subscriber thread hits 100% of CPU, and since it is one thread, it cannot use more than one core. A single thread will always hit a ceiling, be it at 2M, 6M, or more messages per second. We want to split the work across multiple threads that can run in parallel.

The approach used by many high-performance products, which works here, is //sharding//, meaning we split the work into parallel and independent streams. E.g. half of the topic keys are in one stream, half in another. We could use many streams, but performance won't scale unless we have free cores.

So let's see how to shard into two streams:

[[code type="textdiagram"]]

                        +-----------+
                        |           |
                        | Publisher |
                        |           |
                        +-----+-----+
                        | PUB | PUB |
                        \--+--+--+--/
                           |     |   
  +------------------------|--=--|------------------------+
  :                        |     |             Fast box   :
  :                        v     v                        :
  :                     /-----+-----\                     :
  :                     | SUB | SUB |                     :
  :                     +-----+-----+                     :
  :                     |           |                     :
  :                     | Subscriber|                     :
  :                     |           |                     :
  :                     +-----+-----+                     :
  :                     | PUSH|PUSH |                     :
  :                     \--+--+--+--/                     :
  :                        |     |                        :
  :                        |     |                        :
  :           /------------+--+  +------------\           :
  :           |               |               |           :
  :           v               v               v           :
  :     /-----------\   /-----------\   /-----------\     :
  :     |   PULL    |   |   PULL    |   |   PULL    |     :
  :     +-----------+   +-----------+   +-----------+     :
  :     |           |   |           |   |           |     :
  :     |  Worker   |   |  Worker   |   |  Worker   |     :
  :     |           |   |           |   |           |     :
  :     +-----------+   +-----------+   +-----------+     :
  :                                                       :
  +-------------------------------------------------------+

              Figure # - Mad Black Box Pattern
[[/code]]

With two streams, working at full speed, we would configure 0MQ as follows:

* Two I/O threads, rather than one.
* Two network interfaces (NIC), one per subscriber.
* Each I/O thread bound to a specific NIC.
* Two subscriber threads, bound to specific cores.
* Two SUB sockets, one per subscriber thread.
* The remaining cores assigned to worker threads.
* Worker threads connected to both subscriber PUSH sockets.

With ideally, no more threads in our architecture than we had cores. Once we create more threads than cores, we get contention between threads, and diminishing returns. There would be no benefit, for example, in creating more I/O threads.

+++ A Shared Key-Value Cache (Clone Pattern)

Pub-sub is like a radio broadcast, you miss everything before you join, and then how much information you get depends on the quality of your reception. Surprisingly, for engineers who are used to aiming for "perfection", this model is useful and wide-spread, because it maps perfectly to real-world distribution of information. Think of Facebook and Twitter, the BBC World Service, and the sports results.

However, there are also a whole lot of cases where more reliable pub-sub would be valuable, if we could do it. As we did for request-reply, let's define ''reliability' in terms of what can go wrong. Here are the classic problems with pub-sub:

* Subscribers join late, so miss messages the server already sent.
* Subscriber connections are slow, and can lose messages during that time.
* Subscribers go away, and lose messages while they are away.

Less often, we see problems like these:

* Subscribers can crash, and restart, and lose whatever data they already received.
* Subscribers can fetch messages too slowly, so queues build up and then overflow.
* Networks can become overloaded and drop data (specifically, for PGM).
* Networks can become too slow, so publisher-side queues overflow, and publishers crash.

A lot more can go wrong but these are the typical failures we see in a realistic system.

We've already solved some of these, such as the slow subscriber, which we handle with the Suicidal Snail pattern. But for the rest, it would be nice to have a generic, reusable framework for reliable pub-sub.

The difficulty is that we have no idea what our target applications actually want to do with their data. Do they filter it, and process only a subset of messages? Do they log the data somewhere for later reuse? Do they distribute the data further to workers? There are dozens of plausible scenarios, and each will have its own ideas about what reliability means and how much it's worth in terms of effort and performance.

So we'll build an abstraction that we can implement once, and then reuse for many applications. This abstraction is a **shared value-key cache**, which stores a set of blobs indexed by unique keys.

Don't confuse this with //distributed hash tables//, which solve the wider problem of connecting peers in a distributed network, or with //distributed key-value tables//, which act like non-SQL databases. All we will build is a system that reliably clones some in-memory state from a server to a set of clients. We want to:

* Let a client join the network at any time, and reliably get the current server state.
* Let any client update the key-value cache (inserting new key-value pairs, updating existing ones, or deleting them).
* Reliably propagates changes to all clients, and does this with minimum latency overhead.
* Handle very large numbers of clients, e.g. tens of thousands or more.

The key aspect of the Clone pattern is that clients talk back to servers, which is more than we do in a simple pub-sub dialog. This is why I use the terms 'server' and 'client' instead of 'publisher' and 'subscriber'. We'll use pub-sub as the core of Clone but it is a bit more than that.

++++ Distributing Key-Value Updates

We'll develop Clone in stages, solving one problem at a time. First, let's look at how to distribute key-value updates from a server to a set of clients. We'll take our weather server from Chapter One and refactor it to send messages as key-value pairs. We'll modify our client to store these in a hash table:

[[code type="textdiagram"]]

                 +-------------+
                 |             |
                 |   Server    |
                 |             |
                 +-------------+
                 |     PUB     |
                 \-------------/
                        |
                        |
                     updates
                        |
        +---------------+---------------+
        |               |               |
        |               |               |
        v               v               v
  /------------\  /------------\  /------------\
  |    SUB     |  |    SUB     |  |    SUB     |
  +------------+  +------------+  +------------+
  |            |  |            |  |            |
  |   Client   |  |   Client   |  |   Client   |
  |            |  |            |  |            |
  +------------+  +------------+  +------------+


        Figure # - Simplest Clone Model
[[/code]]

This is the server:

[[code type="example" title="Clone server, Model One" name="clonesrv1"]]
[[/code]]

And here is the client:

[[code type="example" title="Clone client, Model One" name="clonecli1"]]
[[/code]]

Some notes about this code:

* All the hard work is done in the **kvmsg** class. This class works with key-value message objects, which are multipart 0MQ messages structured as three frames: a key (a 0MQ string), a sequence number (64-bit value, in network byte order), and a binary body (holds everything else).
* The server generates messages with a randomized 4-digit key, which lets us simulate a large but not enormous hash table (10K entries).
* The server does a 200 millisecond pause after binding its socket. This is to prevent "slow joiner syndrome" where the subscriber loses messages as it connects to the server's socket. We'll remove that in later models.
* We'll use the terms 'publisher' and 'subscriber' in the code to refer to sockets. This will help later when we have multiple sockets doing different things.

Here is the kvmsg class:

[[code type="example" title="Key-value message class " name="kvmsg"]]
[[/code]]

Both the server and client maintain hash tables, but this first model only works properly if we start all clients before the server, and the clients never crash. That's not 'reliability'.

++++ Getting a Snapshot

In order to allow a late (or recovering) client to catch up with a server it has to get a snapshot of the server's state. Just as we've reduced "message" to mean "a sequenced key-value pair", we can reduce "state" to mean "a hash table". To get the server state, a client opens a REQ socket and asks for it explicitly:

[[code type="textdiagram"]]

                 +-------------+
                 |             |
                 |   Server    |
                 |             |
                 +------+------+
                 |  PUB | XREP |
                 \---+--+------/
                     |     ^
                     |     |    state request
                  updates  +----------------+
                     |                      |
     +---------------+---------------+      |
     |               |               |      |
     |               |               |      |
     v               v               v      |
  /------+-----\  /------+-----\  /------+--+--\
  | SUB  |     |  | SUB  |     |  | SUB  | REQ |
  +------+-----+  +------+-----+  +------+-----+
  |            |  |            |  |            |
  |   Client   |  |   Client   |  |   Client   |
  |            |  |            |  |            |
  +------------+  +------------+  +------------+


          Figure # - State Replication
[[/code]]

To make this work, we have to solve the timing problem. Getting a state snapshot will take a certain time, possibly fairly long if the snapshot is large. We need to correctly apply updates to the snapshot. But the server won't know when to start sending us updates. One way would be to start susbcribing, get a first update, and then ask for "state for update N". This would require the server storing one snapshot for each update, which isn't practical.

So we will do the synchronization in the client, as follows:

* The client first subscribes to updates and then makes a state request. This guarantees that the state is going to be newer than the oldest update it has.
* The client waits for the server to reply with state, and meanwhile queues all updates. It does this simply by not reading them: 0MQ keeps them queued on the socket queue, since we don't set a HWM.
* When the client receives its state update, it begins once again to read updates. However it discards any updates that are older than the state update. So if the state update includes updates up to 200, the client will discard updates up to 201.
* The client then applies updates to its own state snapshot.

It's a simple model that exploits 0MQ's own internal queues. Here's the server:

[[code type="example" title="Clone server, Model Two" name="clonesrv2"]]
[[/code]]

And here is the client:

[[code type="example" title="Clone client, Model Two" name="clonecli2"]]
[[/code]]

Some notes about this code:

* The server uses two threads, for simpler design. One thread produces random updates, and the second thread handles state. The two communicate across PAIR sockets. You might like to use SUB sockets but you'd hit the "slow joiner" problem where the subscriber would randomly miss some messages while connecting. PAIR sockets let us explicitly synchronize the two threads.
* We set a HWM on the updates socket pair, since hash table insertions are relatively slow. Without this, the server runs out of memory. On {{inproc}} connections, the real HWM is the sum of the HWM of //both// sockets, so we set the HWM on each socket.
* The client is really simple. In C, under 60 lines of code. A lot of the heavy lifting is done in the kvmsg class, but still, the Clone pattern is easier to implement than it seemed at first.
* We don't use anything fancy for serializing the state. The hash table holds a set of kvmsg objects, and the server sends these, as a batch of messages, to the client requesting state. If multiple clients request state at once, each will get a different snapshot.
* We assume that the client has exactly one server to talk to. The server **must** be running; we do not try to solve the question of what happens if the server crashes.

Right now, these two programs don't do anything real, but they correctly synchronize state. It's a neat example of how to mix different patterns: PAIR-over-inproc, PUB-SUB, and ROUTER-DEALER.

++++ Republishing Updates

In our second model, changes to the key-value cache came from the server itself. This is a centralized model, useful for example if we have a central configuration file we want to distribute, with local caching on each node. A more interesting model takes updates from clients, not the server. The server thus becomes a stateless broker. This gives us some benefits:

* We're less worried about the reliability of the server. If it crashes, we can start a new instance, and feed it new values.
* We can use the key-value cache to share knowledge between dynamic peers.

Updates from clients go via a PUSH-PULL socket flow from client to server:

[[code type="textdiagram"]]

                 +-------------+
                 |             |
                 |   Server    |
                 |             |
                 +------+------+
                 |  PUB | PULL |
                 \---+--+------/
                     |     ^
                     |     |    state update
                  updates  +----------------+
                     |                      |
     +---------------+---------------+      |
     |               |               |      |
     |               |               |      |
     v               v               v      |
  /------+-----\  /------+-----\  /------+--+--\
  | SUB  |     |  | SUB  |     |  | SUB  | PUSH|
  +------+-----+  +------+-----+  +------+-----+
  |            |  |            |  |            |
  |   Client   |  |   Client   |  |   Client   |
  |            |  |            |  |            |
  +------------+  +------------+  +------------+


         Figure # - Republishing Updates
[[/code]]

Why don't we allow clients to publish updates directly to other clients? While this would reduce latency, it makes it impossible to sequence messages. Updates **must** pass through the server to make sense to other clients. There's a more subtle second reason. In many applications it's important that updates have a single order, across many clients. Forcing all updates through the server ensures that they have the same order when they finally get to clients.

With unique sequencing, clients can detect the nastier failures - network congestion and queue overflow. If a client discovers that its incoming message stream has a hole, it can take action. It seems sensible that the client contact the server and ask for the missing messages, but in practice that isn't useful. If there are holes, they're caused by network stress, and adding more stress to the network will make things worse. All the client can really do is warn its users "Unable to continue", and stop, and not restart until someone has manually checked the cause of the problem.

We'll now generate state updates in the client. Here's the server:

[[code type="example" title="Clone server, Model Three" name="clonesrv3"]]
[[/code]]

And here is the client:

[[code type="example" title="Clone client, Model Three" name="clonecli3"]]
[[/code]]

Some notes about this code:

* The server has collapsed to one thread, which collects updates from clients and redistributes them. It manages a PULL socket for incoming updates, a ROUTER socket for state requests, and a PUB socket for outgoing updates.
* The client uses a simple tickless timer to send a random update to the server once a second. In reality, updates would be driven by application code.

.end

++++ Clone Server Reliability

Clone models one to three are relatively simple. We're now going to get into unpleasantly complex territory here that has me getting up for another espresso. You should appreciate that making "reliable" messaging is complex enough that you always need to ask, "do we actually need this?" before jumping into it. If you can get away with unreliable, or "good enough" reliability, you can make a huge win in terms of cost and complexity. Sure, you may lose some data now and then. It is often a good trade-off.

As you play with model three, you'll stop and restart the server. It might look like it recovers, but of course it's applying updates to an empty state, instead of the proper current state. Any new client joining the network will get just the latest updates, instead of all of them. So let's work out a design for making Clone work despite server failures.

Let's list the failures we want to be able to handle:

* Clone server process crashes and is automatically or manually restarted. The process loses its state and has to get it back from somewhere.
* Clone server machine dies and is off-line for a significant time. Clients have to switch to an alternate server somewhere.
* Clone server process or machine gets disconnected from the network, e.g. a switch dies. It may come back at some point, but in the meantime clients need an alternate server.

To detect server failure in the client, we need some type of heartbeating. It's not enough to listen for silence, since the server will be silent if there are no updates from other clients. We can use the sub-pub flow for heartbeating. We also need to detect if the server is unresponsive when we start up and make a state request, so we can switch over to an alternate server.

Now, how to create an alternate server? At its simplest, an alternate server is a proxy that acts like a client to a main server, and a server to clients. This would work, except that when the primary server dies, we'll lose any updates that it had received from clients, but not yet published back to them.

The obvious solution is to synchronously transact updates between the two servers, so that primary will only publish updates when backup acknowledges "OK". Apart from increasing the latency of every update (a TCP round-trip costs several hundred microseconds), this won't in fact give reliability. The backup still doesn't know what updates clients //actually// got, when the primary crashed.

A better solution is to send out updates at full speed but after a primary server crash, have clients detect what updates were lost, and resend them. Only clients have both the knowledge of what updates they sent out, and what updates they received back. It's still not a reliable design, however, because a client can get an update before the backup server gets it. And in that case, the client will think, "this update is safe", when in fact it's not.

Any fully reliable solution has to involve hand-shaking between //all three parties//, client, primary, and backup. If there are N servers acting as a group, every single server in the group has to participate in some kind of asynchronous transaction.

What we need to make here is a form of atomic multicast, where we know that the whole group of servers receive a message, or none of them do. We'll look at a general design for this a bit later, for now we'll make it as simple as it can be for the Clone use case.

So here are the elements of our server failover design:

* Exactly two known servers, S1 and S2.
* Zero, one, or both servers can be running at any time.
* Both servers know about each other, all clients know about both servers.
* S1 and S2 are not symmetric; 
* First server to get a client request becomes head, second becomes tail.
* If head crashes, tail becomes head.
* If head rejoins, it becomes tail.
* Clients request status snapshots from tail.
* Clients push updates to head, and listen for updates from tail.

This is half of the design. The other half is where clients track which of their updates have hit both head and tail, and use that information to resend updates that appear to have gotten lost.

Let's eat the apple in two bites, then. First, server head/tail election. It's a classic primary/failover negotiation problem, and enormously difficult to get right. There are so many traps it's like /b/ on a bad weekend. Some of the ways failover can go wrong:

* //Split-brain syndrome// caused by network failure between the servers, so that both servers think they are in charge. This results in some of the clients thinking one server is head, other clients thinking the other server is head, causing loss of data and general misery.
* //Phantom failover syndrome//, caused by lost heartbeats or unexpected congestion, causing failover without reason. This is relatively OK if failover actually works, but often failover is badly tested, and clients simply don't fail over properly.
* //Bargepole syndrome//, where failover is so complex to understand and configure that system administrators simply refuse to use it, meaning it's not properly tested, meaning it doesn't work properly.
* //Thrashing//, where the two servers get into utter confusion, flipping between roles rapidly, and bringing the application to a halt.

And so on. Presumably a correct and simple model can avoid these traps, but that's much easier said than done. Our best tool to force a design to be correct and simple on a is to implement it in pieces. We can then understand each piece really well, test it to destruction, and have higher confidence in how it behaves under stress.

* server failover is Binary Star pattern from Chapter 4.

++++ Clone Protocol Specification

++++ Clone API

We don't need a lot of code to make a Clone client, but it's still subtle enough to be profitably hidden in a small API. 

While the code we need to implement Clone is short, it's subtle. You'd not want to implement this directly in your applications. The server is fine, this can be a generic process that doesn't include any application code. For the client, let's make an event-driven API.


Clone is complex enough in practice that you don't want to implement it directly in your applications. Instead, it makes a good basis for an application server framework, which talks to applications via the key-value table.

- event driven on changes?
- wrap as API cloneapi.h

++++ Synchronized Clients

- two or more clients asking for same state
- ...? how

Changes to the 

- clones will not have identical state, unless all started at once and cloned in parallel
- clients may send updates back to server, via pub/sub

++++ Practical Clone Example

- distributed name service
- ephemeral names

+++ Atomic Group Multicast (Platoon Pattern)

* N servers, where N is zero or more. In the degenerate case no servers are running. In the normal case, all servers are running.
* The servers know about each other, through a fixed configuration. That is, we do not attempt to create a dynamic pool of servers that can come and go.
* The clients know about all the servers, through a fixed configuration. Again, we do not attempt to allow clients to discover new servers.
* The servers are all exact copies of each other, and at any time hold the same state.
* The servers organize themselves into an ordered list, with a group "head" and a group "tail". Clients speak to the group head, and listen to the group tail.
* When a server joins or rejoins the group, it always becomes the new tail.
* If the group head crashes, the next server in the list, if any, becomes the new head.
* If the group tail crashes, the previous server in the list, if any, becomes the new tail.
* When any other server crashes, the previous server in the list links instead to the following server.

+++ Cluster-wide topic distribution

- anyone can publish
- anyone can subscribe
- reliable
- using 1 or two forwarders


+++ Secure Publish-Subscribe (Need to Know Pattern)
