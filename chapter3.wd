[[include :csi:include:css-brackets]]
[[div style="overflow:hidden"]]
[[div style="float:left"]]
++ Chapter Three - Advanced Request-Reply
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

In Chapter Two we worked through the basics of using ØMQ by developing a series of small applications, each time exploring new aspects of ØMQ. We'll continue this approach in this chapter, as we explore advanced aspects of ØMQ's request-reply pattern.

We'll cover:

* How to create and use message envelopes for request-reply.
* How to use the REQ, REP, XREQ, and XREP sockets.
* How to set manual reply addresses using identities.
* How to do custom random scatter routing.
* How to do custom least-recently used routing.
* How to build a higher-level message class.
* How to build a basic request-reply broker.
* How to choose good names for sockets.
* How to simulate a cluster of clients and workers.
* How to build a scalable cloud of request-reply clusters.
* How to use pipeline sockets for monitoring threads.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Request-Reply Envelopes
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

In the request-reply pattern, the envelope holds the return address for replies. It is how a ØMQ network with no state can create round-trip request-reply dialogs.

You don't in fact need to understand how request-reply envelopes work to use them for common cases. When you use REQ and REP, your sockets build and use envelopes automatically. When you write a device, and we covered this in the last chapter, you just need to read and write all the parts of a message. ØMQ implements envelopes using multipart data, so if you copy multipart data safely, you implicitly copy envelopes too.

However, getting under the hood and playing with request-reply envelopes is necessary for advanced request-reply work. It's time to explain how XREP works, in terms of envelopes:

* When you receive a message from an XREP socket, it shoves a brown paper envelope around the message and scribbles on with indelible ink, "This came from Lucy". Then it gives that to you. That is, the XREP socket gives you what came off the wire, wrapped up in an envelope with the reply address on it.

* when you send a message to an XREP socket, it rips off that brown paper envelope, tries to read its own handwriting, and if it knows who "Lucy" is, sends the contents back to Lucy. That is the reverse process of receiving a message.

If you leave the brown envelope alone, and then pass that message to another XREP socket (e.g. by sending to an XREQ connected to an XREP), the second XREP socket will in turn stick another brown envelope on it, and scribble the name of that XREQ on it.

The whole point of this is that each XREP knows how to send replies back to the right place. All you need to do, in your application, is respect the brown envelopes. Now the REP socket makes sense. It carefully slices open the brown envelopes, one by one, keeps them safely aside, and gives you (the application code that owns the REP socket) the original message. When you send the reply, it re-wraps the reply in the brown paper envelopes, so it can hand the resulting brown package back to the XREP sockets down the chain.

Which lets you insert XREP-XREQ devices into a request-reply pattern like this:

[[code]]
[REQ] <--> [REP]
[REQ] <--> [XREP--XREQ] <--> [REP]
[REQ] <--> [XREP--XREQ] <--> [XREP--XREQ] <--> [REP]
...etc.
[[/code]]

If you connect a REQ socket to an XREP socket, and send one request message, this is what you get when you receive from the XREP socket:

[[=image http://github.com/imatix/zguide/raw/master/images/fig29.png]]

Breaking this down:

* The data in frame 3 is what the sending application sends to the REQ socket.

* The empty message part in frame 2 is prepended by the REQ socket when it sends the message to the XREP socket.

* The reply address in frame 1 is prepended by the XREP before it passes the message to the receiving application.

Now if we extend this with a chain of devices, we get envelope on envelope, with the newest envelope always stuck at the beginning of the stack:

[[=image http://github.com/imatix/zguide/raw/master/images/fig30.png]]

Here now is a more detailed explanation of the four socket types we use for request-reply patterns:

* XREQ just load-balances the messages you send to all connected peers, and fair-queues the messages it receives. It is exactly like a PUSH and PULL socket combined.

* REQ prepends an empty message part to every message you send, and removes the empty message part from each message you receive. It then works like XREQ (and in fact is built on XREQ) except it also imposes a strict send / receive cycle.

* XREP prepends an envelope with reply address to each message it receives, before passing it to the application. It also chops off the envelope (the first message part) from each message it sends, and uses that reply address to decide which peer the message should go to.

* REP stores all the message parts up to the first empty message part, when you receive a message and it passes the rest (the data) to your application. When you send a reply, REP prepends the saved envelopes to the message and sends it back using the same semantics as XREP (and in fact REP is built on top of XREP), but matching REQ, imposes a strict receive / send cycle.

REP requires that the envelopes end with an empty message part. If you're not using REQ at the other end of the chain then you must add the empty message part yourself.

So the obvious question about XREP is, where does it get the reply addresses from? And the obvious answer is, it uses the socket's identity. As we already learned, a socket can be transient in which case the //other// socket (XREP in this case) generates an identity that it can associate with the socket. Or, the socket can be durable in which case it explicitly tells the other socket (XREP, again) its identity and XREP can use that rather than generating a temporary label.

This is what it looks like for transient sockets:

[[=image http://github.com/imatix/zguide/raw/master/images/fig31.png]]

This is what it looks like for durable sockets:

[[=image http://github.com/imatix/zguide/raw/master/images/fig32.png]]

Let's observe the above two cases in practice. This program dumps the contents of the message parts that an XREP socket receives from two REP sockets, one not using identities, and one using an identity 'Hello':

[[code type="C" title="Identity check" name="identity"]]
//
//  Demonstrate identities as used by the request-reply pattern.  Run this
//  program by itself.  Note that the utility functions s_ are provided by
//  zhelpers.h.  It gets boring for everyone to keep repeating this code.
//
//  Changes for 2.1:
//  - added close sink before terminating
//
#include "zhelpers.h"

int main () {
    void *context = zmq_init (1);

    void *sink = zmq_socket (context, ZMQ_XREP);
    zmq_bind (sink, "inproc://example");

    //  First allow 0MQ to set the identity
    void *anonymous = zmq_socket (context, ZMQ_REQ);
    zmq_connect (anonymous, "inproc://example");
    s_send (anonymous, "XREP uses a generated UUID");
    s_dump (sink);

    //  Then set the identity ourself
    void *identified = zmq_socket (context, ZMQ_REQ);
    zmq_setsockopt (identified, ZMQ_IDENTITY, "Hello", 5);
    zmq_connect (identified, "inproc://example");
    s_send (identified, "XREP socket uses REQ's socket identity");
    s_dump (sink);

    zmq_close (sink);
    zmq_close (anonymous);
    zmq_close (identified);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/identity.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/identity.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/identity.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/identity.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/identity.cpp C++]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/identity.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/identity.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/identity.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/identity.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/identity.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/identity.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/identity.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/identity.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/identity.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/identity.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/identity.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/identity.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/identity.rb Ruby]//
[[/collapsible]]
[[/>]]

Here is what the dump function prints:

[[code]]
----------------------------------------
[017] 00314F043F46C441E28DD0AC54BE8DA727
[000]
[026] XREP uses a generated UUID
----------------------------------------
[005] Hello
[000]
[038] XREP socket uses REQ's socket identity
[[/code]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Custom Request-Reply Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We already saw that XREP uses the message envelope to decide which client to route a reply back to. Now let me express that in another way: //XREP will route messages asynchronously to any peer connected to it, if you provide the correct routing address via a properly constructed envelope.//

So XREP is really a fully controllable router. Let's look at this magic in detail. But first, let's fix the parsing pain we feel when we try to distinguish "REP", "REQ", "XREP", and "XREQ" from each other. There should be a law against names that are so similar. :-)

For readability, and because we're going to go off-road into some rough and possibly illegal terrain now, let's rename these four socket types just for this section of the text:

* REQ is a **mama** socket, doesn't listen but always expects an answer. Mamas are strictly synchronous and if you use them they are always the 'request' end of a chain.
* REP is a **papa** socket, always answers, but never starts a conversation. Papas are strictly synchronous and if you use them, they are always the 'reply' end of a chain.
* XREQ is a **dealer** socket, shuffling messages to and fro. Dealers can deal requests evenly to N papas (sending requests, waiting for replies) and/or one router (waiting for requests, and sending back replies).
* XREP is a **router** socket, able to route messages to specific peers. Routers can talk to N peers of any kind, but they're naturally most at ease with mama sockets.

The thing about Mama sockets is, as we all learned as kids, you can't speak until spoken to. Mamas do not have simple open-mindedness of papas, nor the ambiguous "sure, whatever" shrugged-shoulder aloofness of a dealer. So to speak to a mama socket, you have to get the mama socket to talk to you first. The good part is mamas don't care if you reply now, or much later. Just bring a good sob story and a bag of laundry.

Papa sockets on the other hand are strong and silent, and pedantic. They do just one thing, which is to give you an answer to whatever you ask, perfectly framed and precise. Don't expect a papa socket to be chatty, or to pass a message on to someone else, this is just not going to happen.

Dealers are used to talking to papas, and treat a group of papas fairly by dealing each of them one card, round and round. You cannot bribe a dealer to treat any papa specially, they are immune to all forms of persuasion.

Routers are the diplomats of the ØMQ request-reply world, able to talk to all the other sockets. If you ask a router to do pass a message to someone it doesn't know, it won't complain or say "no", it'll just drop the message discretely into the trash. Whenever you need to talk to //specific// peers, you need a router.

While we usually think of request-reply as a to-and-fro pattern, in fact it can be fully asynchronous, as long as we understand that any mamas or papas will be at the end of a chain, never in the middle of it, and always synchronous. All we need to know is the address of the peer we want to talk to, and then we can then send it messages asynchronously, via a router. The router is the one and only ØMQ socket type capable of being told "send this message to X" where X is the address of a connected peer.

These are the ways we can know the address to send a message to, and you'll see most of these used in the examples of custom request-reply routing:

* If it's an anonymous peer, i.e. did not set any identity, the router will generate a UUID and use that to refer to the connection when it delivers you an incoming request envelope.

* If it is a peer with explicit identity, the router will give that identity when it delivers you an incoming request envelope.

* Peers with explicit identities can send them via some other mechanism, e.g. via some other sockets.

* Peers can have prior knowledge of each others' identities, e.g. via configuration files or some other magic.

There are four custom routing patterns, one for each of the socket types we can connect to a router:

* Router-to-dealer, also called XREP-to-XREQ.
* Router-to-mama, aka XREP-to-REQ.
* Router-to-papa, aka XREP-to-REP.
* Router-to-router, aka XREP-to-XREP.

In each of these cases we have total control over how we route messages, but the different patterns cover different use-cases and message flows. Let's break it down over the next sections with examples of different routing algorithms.

But first some warnings about custom routing:

* This goes against a fairly solid ØMQ rule: //delegate peer addressing to the socket//. The only reason we do it is because ØMQ lacks a wide range of routing algorithms.

* Future versions of ØMQ will probably do some of the routing we're going to build here. That means the code we design now may break, or become redundant in the future.

* While the built-in routing has certain guarantees of scalability, such as being friendly to devices, custom routing doesn't. You will need to make your own devices.

So overall, custom routing is more expensive and more fragile than delegating this to ØMQ. Only do it if you need it. Having said that, let's jump in, the water's great!

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Random Scatter Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

The router-to-dealer pattern is the simplest. You connect one router to many dealers, and then distribute messages to the dealers using any algorithm you like. The dealers can be sinks (process the messages without any response), proxies (send the messages on to other nodes), or services (send back replies).

If you expect the dealer to reply, there should only be one router talking to it. Dealers have no idea how to reply to a specific peer, so if they have multiple peers, they will load-balance between them, which would be weird. If the dealer is a sink, any number of routers can talk to it.

What kind of routing can you do with a router-to-dealer pattern?  If the dealers talk back to the router, e.g. telling the router when they finished a task, you can use that knowledge to route depending on how fast a dealer is. Since both router and dealer are asynchronous, it can get a little tricky. You'd need to use [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] at least.

We'll make an example where the dealers don't talk back, they're pure sinks. Our routing algorithm will be a weighted random scatter: we have two dealers and we send twice as many messages to one as to the other.

[[=image http://github.com/imatix/zguide/raw/master/images/fig33.png]]

Here's code that shows how this works:

[[code type="C" title="Router-to-dealer" name="rtdealer"]]
//
//  Custom routing Router to Dealer (XREP to XREQ)
//
//  Changes for 2.1:
//  - added version assertion
//  - use separate contexts for each thread
//  - close sockets in each child thread
//  - call zmq_term in each thread before ending
//  - removed sleep(1) at end of main thread
//
#include "zhelpers.h"

//  We have two workers, here we copy the code, normally these would
//  run on different boxes...
//
void *worker_a (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_XREQ);
    zmq_setsockopt (worker, ZMQ_IDENTITY, "A", 1);
    zmq_connect (worker, "ipc://routing.ipc");

    int total = 0;
    while (1) {
        //  We receive one part, with the workload
        char *request = s_recv (worker);
        int finished = (strcmp (request, "END") == 0);
        free (request);
        if (finished) {
            printf ("A received: %d\n", total);
            break;
        }
        total++;
    }
    zmq_close (worker);
    zmq_term (context);
    return (NULL);
}

void *worker_b (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_XREQ);
    zmq_setsockopt (worker, ZMQ_IDENTITY, "B", 1);
    zmq_connect (worker, "ipc://routing.ipc");

    int total = 0;
    while (1) {
        //  We receive one part, with the workload
        char *request = s_recv (worker);
        int finished = (strcmp (request, "END") == 0);
        free (request);
        if (finished) {
            printf ("B received: %d\n", total);
            break;
        }
        total++;
    }
    zmq_close (worker);
    zmq_term (context);
    return (NULL);
}

int main () {
    s_version_assert (2, 1);
    void *context = zmq_init (1);

    void *client = zmq_socket (context, ZMQ_XREP);
    zmq_bind (client, "ipc://routing.ipc");

    pthread_t worker;
    pthread_create (&worker, NULL, worker_a, NULL);
    pthread_create (&worker, NULL, worker_b, NULL);

    //  Wait for threads to connect, since otherwise the messages
    //  we send won't be routable.
    sleep (1);

    //  Send 10 tasks scattered to A twice as often as B
    int task_nbr;
    srandom ((unsigned) time (NULL));
    for (task_nbr = 0; task_nbr < 10; task_nbr++) {
        //  Send two message parts, first the address...
        if (randof (3) > 0)
            s_sendmore (client, "A");
        else
            s_sendmore (client, "B");

        //  And then the workload
        s_send (client, "This is the workload");
    }
    s_sendmore (client, "A");
    s_send     (client, "END");

    s_sendmore (client, "B");
    s_send     (client, "END");

    zmq_close (client);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/rtdealer.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rtdealer.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rtdealer.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rtdealer.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rtdealer.cpp C++]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/rtdealer.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rtdealer.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rtdealer.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rtdealer.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rtdealer.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rtdealer.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rtdealer.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rtdealer.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rtdealer.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rtdealer.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rtdealer.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rtdealer.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rtdealer.rb Ruby]//
[[/collapsible]]
[[/>]]

Some comments on this code:

* The router doesn't know when the dealers are ready, and it would be distracting for our example to add in the signaling to do that. So the router just does a "sleep (1)" after starting the dealer threads. Without this sleep, the router will send out messages that can't be routed, and ØMQ will discard them.
* Note that this behavior is specific to ROUTER sockets. PUB sockets will also discard messages if there are no subscribers, but all other socket types will queue sent messages until there's a peer to receive them.

To route to a dealer, we create an envelope like this:

[[=image http://github.com/imatix/zguide/raw/master/images/fig34.png]]

The router removes the first frame, routes the second frame, which the dealer gets as-is. If the dealer was to reply, we'd get back a similar envelope in two parts.

Something to note: if you use an invalid address, the router discards the message silently. There is not much else it can do usefully. In normal cases this either means the peer has gone away, or that there is a programming error somewhere and you're using a bogus address. ØMQ may in future report dropped messages via a sys://log bus, and may distinguish these two different cases. In any case you cannot ever assume a message will be routed successfully until and unless you get a reply of some sorts from the destination node. We'll come to creating reliable patterns later on.

Dealers look a bit like PULL sockets here and in fact they work exactly as PUSH and PULL combined. It's illegal to connect PULL or PUSH to a request-reply socket, and pointless, so don't do it.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Least-Recently Used Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Like we said, Mamas don't listen to you, and if you try to speak out of turn they'll ignore you. You have to wait for them to say something, //then// you can give a sarcastic answer. This is very useful for routing because it means we can keep a bunch of mamas waiting for answers. In effect, mamas tell us when they're ready.

You can connect one router to many mamas, and distribute messages as you would to dealers. Mamas will usually want to reply, but they will let you have the last word. However it's one thing at a time:

* Mama speaks to router
* Router replies to mama
* Mama speaks to router
* Router replies to mama
* etc.

Like dealers, mamas can only talk to one router and since mamas always start by talking to the router, you should never connect one mama to more than one router unless you are doing sneaky stuff like multi-pathway redundant routing. I'm not even going to explain that now, and hopefully the jargon is complex enough to stop you trying this until you need it.

[[=image http://github.com/imatix/zguide/raw/master/images/fig35.png]]

What kind of routing can you do with a router-to-mama pattern?  Probably the most obvious is "least-recently-used" (LRU), where we always route to the mama that's been waiting longest. Here is an example that does LRU routing to a set of mamas:

[[code type="C" title="Router-to-mama" name="rtmama"]]
//
//  Custom routing Router to Mama (XREP to REQ)
//
//  Changes for 2.1:
//  - added version assertion
//  - use separate contexts for each thread
//  - close sockets in each child thread
//  - call zmq_term in each thread before ending
//  - removed sleep(1) at end of main thread
//
#include "zhelpers.h"

#define NBR_WORKERS 10

static void *
worker_thread (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_REQ);

    //  We use a string identity for ease here
    s_set_id (worker);
    zmq_connect (worker, "ipc://routing.ipc");

    int total = 0;
    while (1) {
        //  Tell the router we're ready for work
        s_send (worker, "ready");

        //  Get workload from router, until finished
        char *workload = s_recv (worker);
        int finished = (strcmp (workload, "END") == 0);
        free (workload);
        if (finished) {
            printf ("Processed: %d tasks\n", total);
            break;
        }
        total++;

        //  Do some random work
        struct timespec t;
        t.tv_sec = 0;
        t.tv_nsec = randof (100000000) + 1;
        nanosleep (&t, NULL);
    }
    zmq_close (worker);
    zmq_term (context);
    return (NULL);
}

int main () {
    s_version_assert (2, 1);
    void *context = zmq_init (1);

    void *client = zmq_socket (context, ZMQ_XREP);
    zmq_bind (client, "ipc://routing.ipc");
    srandom ((unsigned) time (NULL));

    int worker_nbr;
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_thread, NULL);
    }
    int task_nbr;
    for (task_nbr = 0; task_nbr < NBR_WORKERS * 10; task_nbr++) {
        //  LRU worker is next waiting in queue
        char *address = s_recv (client);
        char *empty = s_recv (client);
        free (empty);
        char *ready = s_recv (client);
        free (ready);

        s_sendmore (client, address);
        s_sendmore (client, "");
        s_send (client, "This is the workload");
        free (address);
    }
    //  Now ask mamas to shut down and report their results
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        char *address = s_recv (client);
        char *empty = s_recv (client);
        free (empty);
        char *ready = s_recv (client);
        free (ready);

        s_sendmore (client, address);
        s_sendmore (client, "");
        s_send (client, "END");
        free (address);
    }
    zmq_close (client);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/rtmama.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rtmama.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rtmama.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rtmama.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rtmama.cpp C++]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/rtmama.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rtmama.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rtmama.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rtmama.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rtmama.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rtmama.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rtmama.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rtmama.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rtmama.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rtmama.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rtmama.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rtmama.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rtmama.rb Ruby]//
[[/collapsible]]
[[/>]]

For this example the LRU doesn't need any particular data structures above what ØMQ gives us (message queues) because we don't need to synchronize the workers with anything. A more realistic LRU algorithm would have to collect workers as they become ready, into a queue, and the use this queue when routing client requests. We'll do this in a later example.

To prove that the LRU is working as expected, the mamas print the total tasks they each did. Since the mamas do random work, and we're not load balancing, we expect each mama to do approximately the same amount but with random variation. And that is indeed what we see:

[[code]]
Processed: 8 tasks
Processed: 8 tasks
Processed: 11 tasks
Processed: 7 tasks
Processed: 9 tasks
Processed: 11 tasks
Processed: 14 tasks
Processed: 11 tasks
Processed: 11 tasks
Processed: 10 tasks
[[/code]]

Some comments on this code

* We don't need any settle time, since the mamas explicitly tell the router when they are ready.

* We're generating our own identities here, as printable strings, using the zhelpers.h s_set_id function. That's just to make our life a little simpler. In a realistic application the mamas would be fully anonymous and then you'd call [http://api.zeromq.org/zmq_recv.html zmq_recv(3)] and [http://api.zeromq.org/zmq_send.html zmq_send(3)] directly instead of the zhelpers s_recv() and s_send() functions, which can only handle strings.

* Worse, we're using //random// identities. Don't do this in real code, please. Randomized durable sockets are not good in real life.

* If you copy and paste example code without understanding it, you deserve what you get. It's like watching Spiderman leap off the roof and then trying that yourself.

To route to a mama, we must create a mama-friendly envelope like this:

[[=image http://github.com/imatix/zguide/raw/master/images/fig36.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Address-based Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Papas are, if we care about them at all, only there to answer questions. And to pay the bills, fix the car when mama drives it into the garage wall, put up shelves, and walk the dog when it's raining. But apart from that, papas are only there to answer questions.

In a classic request-reply pattern a router wouldn't talk to a papa socket at all, but rather would get a dealer to do the job for it. That's what dealers are for: to pass questions onto random papas and come back with their answers. Routers are generally more comfortable talking to mamas. OK, dear reader, you may stop the psychoanalysis. These are analogies, not life stories.

It's worth remembering with ØMQ that the classic patterns are the ones that work best, that the beaten path is there for a reason, and that when we go off-road we take the risk of falling off cliffs and getting eaten by zombies. Having said that, let's plug a router into a papa and see what the heck emerges.

The special thing about papas, all joking aside, is actually two things:

* One, they are strictly lockstep request-reply.
* Two, they accept an envelope stack of any size and will return that intact.

In the normal request-reply pattern, papas are anonymous and replaceable (wow, these analogies //are// scary), but we're learning about custom routing. So, in our use-case we have reason to send a request to papa A rather than papa B. This is essential if you want to keep some kind of a conversation going between you, at one end of a large network, and a papa sitting somewhere far away.

A core philosophy of ØMQ is that the edges are smart and many, and the middle is vast and dumb. This does mean the edges can address each other, and this also means we want to know how to reach a given papa. Doing routing across multiple hops is something we'll look at later but for now we'll look just at the final step: a router talking to a specific papa:

[[=image http://github.com/imatix/zguide/raw/master/images/fig37.png]]

This example shows a very specific chain of events:

* The client has a message that it expects to route back (via another router) to some node. The message has two addresses (a stack), an empty part, and a body.
* The client passes that to the router but specifies a papa address first.
* The router removes the papa address, uses that to decide which papa to send the message to.
* The papa receives the addresses, empty part, and body.
* It removes the addresses, saves them, and passes the body to the worker.
* The worker sends a reply back to the papa.
* The papa recreates the envelope stack and sends that back with the worker's reply to the router.
* The router prepends the papa's address and provides that to the client along with the rest of the address stack, empty part, and the body.

It's complex but worth working through until you understand it. Just remember a papa is garbage in, garbage out.

[[code type="C" title="Router-to-papa" name="rtpapa"]]
//
//  Custom routing Router to Papa (XREP to REP)
//
//  Changes for 2.1:
//  - close worker socket properly
//
#include "zhelpers.h"

//  We will do this all in one thread to emphasize the sequence
//  of events...
int main () {
    void *context = zmq_init (1);

    void *client = zmq_socket (context, ZMQ_XREP);
    zmq_bind (client, "ipc://routing.ipc");

    void *worker = zmq_socket (context, ZMQ_REP);
    zmq_setsockopt (worker, ZMQ_IDENTITY, "A", 1);
    zmq_connect (worker, "ipc://routing.ipc");

    //  Wait for the worker to connect so that when we send a message
    //  with routing envelope, it will actually match the worker...
    sleep (1);

    //  Send papa address, address stack, empty part, and request
    s_sendmore (client, "A");
    s_sendmore (client, "address 3");
    s_sendmore (client, "address 2");
    s_sendmore (client, "address 1");
    s_sendmore (client, "");
    s_send     (client, "This is the workload");

    //  Worker should get just the workload
    s_dump (worker);

    //  We don't play with envelopes in the worker
    s_send (worker, "This is the reply");

    //  Now dump what we got off the XREP socket...
    s_dump (client);

    zmq_close (client);
    zmq_close (worker);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/rtpapa.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rtpapa.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rtpapa.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rtpapa.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rtpapa.cpp C++]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/rtpapa.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rtpapa.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rtpapa.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rtpapa.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rtpapa.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rtpapa.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rtpapa.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rtpapa.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rtpapa.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rtpapa.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rtpapa.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rtpapa.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rtpapa.rb Ruby]//
[[/collapsible]]
[[/>]]

Run this program and it should show you this:

[[code]]
----------------------------------------
[020] This is the workload
----------------------------------------
[001] A
[009] address 3
[009] address 2
[009] address 1
[000]
[017] This is the reply
[[/code]]

Some comments on this code:

* In reality we'd have the papa and router in separate nodes. This example does it all in one thread because it makes the sequence of events really clear.

* [http://api.zeromq.org/zmq_connect.html zmq_connect(3)] doesn't happen instantly. When the papa socket connects to the router, that takes a certain time and happens in the background. In a realistic application the router wouldn't even know the papa existed until there had been some previous dialog. In our toy example we'll just {{sleep (1);}} to make sure the connection's done. If you remove the sleep, the papa socket won't get the message. (Try it.)

* We're routing using the papa's identity. Just to convince yourself this really is happening, try sending to a wrong address, like "B". The papa won't get the message.

* The s_dump and other utility functions (in the C code) come from the zhelpers.h header file. It becomes clear that we do the same work over and over on sockets, and there are interesting layers we can build on top of the ØMQ API. We'll come back to this later when we make a real application rather than these toy examples.

To route to a papa, we must create a papa-friendly envelope like this:

[[=image http://github.com/imatix/zguide/raw/master/images/fig38.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ A Request-Reply Message Broker
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We'll recap the knowledge we have so far about doing weird stuff with ØMQ message envelopes, and build the core of a generic custom routing queue device that we can properly call a //message broker//. Sorry for all the buzzwords. What we'll make is a //queue device// that connects a bunch of //clients// to a bunch of //workers//, and lets you use //any routing algorithm// you want. What we'll do is //least-recently used//, since it's the most obvious use-case apart from load-balancing.

To start with, let's look back at the classic request-reply pattern and then see how it extends over a larger and larger service-oriented network. The basic pattern is:

[[=image http://github.com/imatix/zguide/raw/master/images/fig39.png]]

This extends to multiple papas, but if we want to handle multiple mamas as well we need a device in the middle, which normally consists of a router and a dealer back to back, connected by a classic ZMQ_QUEUE device that just copies message parts between the two sockets as fast as it can:

[[=image http://github.com/imatix/zguide/raw/master/images/fig40.png]]

The key here is that the router stores the originating mama address in the request envelope, the dealer and papas don't touch that, and so the router knows which mama to send the reply back to. Papas are anonymous and not addressed in this pattern, all papas are assumed to provide the same service.

In the above design, we're using the built-in load balancing routing that the dealer socket provides. However we want for our broker to use a least-recently used (LRU) algorithm, so we take the router-mama pattern we learned, and apply that:

[[=image http://github.com/imatix/zguide/raw/master/images/fig41.png]]

Our broker - a router-to-router LRU queue - can't simply copy message parts blindly. Here is the code, it's fairly complex but the core logic is reusable in any request-reply broker that wants to do LRU routing:

[[code type="C" title="LRU queue broker" name="lruqueue"]]
//
//  Least-recently used (LRU) queue device
//  Clients and workers are shown here in-process
//
//  Changes for 2.1:
//  - added version assertion
//  - use separate contexts for each thread
//  - close sockets in each child thread
//  - call zmq_term in each thread before ending
//  - removed sleep(1) at end of main thread
//
#include "zhelpers.h"

#define NBR_CLIENTS 10
#define NBR_WORKERS 3

//  A simple dequeue operation for queue implemented as array
#define DEQUEUE(q) memmove (&(q)[0], &(q)[1], sizeof (q) - sizeof (q [0]))

//  Basic request-reply client using REQ socket
//
static void *
client_thread (void *args) {
    void *context = zmq_init (1);
    void *client = zmq_socket (context, ZMQ_REQ);
    s_set_id (client);          //  Makes tracing easier
    zmq_connect (client, "ipc://frontend.ipc");

    //  Send request, get reply
    s_send (client, "HELLO");
    char *reply = s_recv (client);
    printf ("Client: %s\n", reply);
    free (reply);
    zmq_close (client);
    zmq_term (context);
    return NULL;
}

//  Worker using REQ socket to do LRU routing
//
static void *
worker_thread (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_REQ);
    s_set_id (worker);          //  Makes tracing easier
    zmq_connect (worker, "ipc://backend.ipc");

    //  Tell broker we're ready for work
    s_send (worker, "READY");

    while (1) {
        //  Read and save all frames until we get an empty frame
        //  In this example there is only 1 but it could be more
        char *address = s_recv (worker);
        char *empty = s_recv (worker);
        assert (*empty == 0);
        free (empty);

        //  Get request, send reply
        char *request = s_recv (worker);
        printf ("Worker: %s\n", request);
        free (request);

        s_sendmore (worker, address);
        s_sendmore (worker, "");
        s_send     (worker, "OK");
        free (address);
    }
    zmq_close (worker);
    zmq_term (context);
    return NULL;
}

int main (int argc, char *argv[])
{
    s_version_assert (2, 1);

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    void *frontend = zmq_socket (context, ZMQ_XREP);
    void *backend  = zmq_socket (context, ZMQ_XREP);
    zmq_bind (frontend, "ipc://frontend.ipc");
    zmq_bind (backend,  "ipc://backend.ipc");

    int client_nbr;
    for (client_nbr = 0; client_nbr < NBR_CLIENTS; client_nbr++) {
        pthread_t client;
        pthread_create (&client, NULL, client_thread, NULL);
    }
    int worker_nbr;
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_thread, NULL);
    }
    //  Logic of LRU loop
    //  - Poll backend always, frontend only if 1+ worker ready
    //  - If worker replies, queue worker as ready and forward reply
    //    to client if necessary
    //  - If client requests, pop next worker and send request to it

    //  Queue of available workers
    int available_workers = 0;
    char *worker_queue [10];

    while (1) {
        //  Initialize poll set
        zmq_pollitem_t items [] = {
            //  Always poll for worker activity on backend
            { backend,  0, ZMQ_POLLIN, 0 },
            //  Poll front-end only if we have available workers
            { frontend, 0, ZMQ_POLLIN, 0 }
        };
        if (available_workers)
            zmq_poll (items, 2, -1);
        else
            zmq_poll (items, 1, -1);

        //  Handle worker activity on backend
        if (items [0].revents & ZMQ_POLLIN) {
            //  Queue worker address for LRU routing
            char *worker_addr = s_recv (backend);
            assert (available_workers < NBR_WORKERS);
            worker_queue [available_workers++] = worker_addr;

            //  Second frame is empty
            char *empty = s_recv (backend);
            assert (empty [0] == 0);
            free (empty);

            //  Third frame is READY or else a client reply address
            char *client_addr = s_recv (backend);

            //  If client reply, send rest back to frontend
            if (strcmp (client_addr, "READY") != 0) {
                empty = s_recv (backend);
                assert (empty [0] == 0);
                free (empty);
                char *reply = s_recv (backend);
                s_sendmore (frontend, client_addr);
                s_sendmore (frontend, "");
                s_send     (frontend, reply);
                free (reply);
                if (--client_nbr == 0)
                    break;      //  Exit after N messages
            }
            free (client_addr);
        }
        if (items [1].revents & ZMQ_POLLIN) {
            //  Now get next client request, route to LRU worker
            //  Client request is [address][empty][request]
            char *client_addr = s_recv (frontend);
            char *empty = s_recv (frontend);
            assert (empty [0] == 0);
            free (empty);
            char *request = s_recv (frontend);

            s_sendmore (backend, worker_queue [0]);
            s_sendmore (backend, "");
            s_sendmore (backend, client_addr);
            s_sendmore (backend, "");
            s_send     (backend, request);

            free (client_addr);
            free (request);

            //  Dequeue and drop the next worker address
            free (worker_queue [0]);
            DEQUEUE (worker_queue);
            available_workers--;
        }
    }
    zmq_close (frontend);
    zmq_close (backend);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/lruqueue.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/lruqueue.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/lruqueue.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/lruqueue.c C]**
**[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/lruqueue.cpp C++]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/lruqueue.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/lruqueue.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/lruqueue.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/lruqueue.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/lruqueue.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/lruqueue.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/lruqueue.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/lruqueue.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/lruqueue.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/lruqueue.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/lruqueue.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/lruqueue.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/lruqueue.rb Ruby]//
[[/collapsible]]
[[/>]]

The difficult part of this program is (a) the envelopes that each socket reads and writes, and (b) the LRU algorithm. We'll take these in turn, starting with the message envelope formats.

First, recall that a mama socket always puts on an empty part (the envelope delimiter) on sending and removes this empty part on reception. The reason for this isn't important, it's just part of the 'normal' request-reply pattern. What we care about here is just keeping mama happy by doing precisely what she needs. Second, the router always adds an envelope with the address of whomever the message came from.

We can now walk through a full request-reply chain from client to worker and back. In the code we set the identity of client and worker sockets to make it easier to print the message frames if we want to. Let's assume the client's identity is "CLIENT" and the worker's identity is "WORKER". The client sends a single frame:

[[=image http://github.com/imatix/zguide/raw/master/images/fig42.png]]

What the queue gets, when reading off the router frontend socket is this:

[[=image http://github.com/imatix/zguide/raw/master/images/fig43.png]]

The broker sends this to the worker, prefixed by the address of the worker, taken from the LRU queue, plus an additional empty part to keep the mama at the other end happy:

[[=image http://github.com/imatix/zguide/raw/master/images/fig44.png]]

This complex envelope stack gets chewed up first by the backend router socket, which removes the first frame. Then the mama socket in the worker removes the empty part, and provides the rest to the worker:

[[=image http://github.com/imatix/zguide/raw/master/images/fig45.png]]

Which is exactly the same as what the queue received on its frontend router socket. The worker has to save the envelope (which is all the parts up to and including the empty message part) and then it can do what's needed with the data part.

On the return path the messages are the same as when they come in, i.e. the backend socket gives the queue a message in five parts, and the queue sends the frontend socket a message in three parts, and the client gets a message in one part.

Now let's look at the LRU algorithm. It requires that both clients and workers use mama sockets, and that workers correctly store and replay the envelope on messages they get. The algorithm is:

* Create a pollset which polls the backend always, and the frontend only if there are one or more workers available.

* Poll for activity with infinite timeout.

* If there is activity on the backend, we either have a "ready" message or a reply for a client. In either case we store the worker address (the first part) on our LRU queue, and if the rest is a client reply we send it back to that client via the frontend.

* If there is activity on the frontend, we take the client request, pop the next worker (which is the least-recently used), and send the request to the backend. This means sending the worker address, empty part, and then the three parts of the client request.

You should now see that you can reuse and extend the LRU algorithm with variations based on the information the worker provides in its initial "ready" message. For example, workers might start up and do a performance self-test, then tell the broker how fast they are. The broker can then choose the fastest available worker rather than LRU or round-robin.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ A Multipart Message Class
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Reading and writing multipart messages rapidly gets tedious and error-prone. Look at the core of the worker thread from our LRU queue broker:

[[code]]
    while (1) {
        //  Read and save all frames until we get an empty frame
        //  In this example there is only 1 but it could be more
        char *address = s_recv (worker);
        char *empty = s_recv (worker);
        assert (*empty == 0);
        free (empty);

        //  Get request, send reply
        char *request = s_recv (worker);
        printf ("Worker: %s\n", request);
        free (request);

        s_sendmore (worker, address);
        s_sendmore (worker, "");
        s_send     (worker, "OK");
        free (address);
    }
[[/code]]

That code isn't even reusable, because it can only handle one envelope. And this code already does some wrapping around the ØMQ API. If we used the raw API carefully this is what we'd have to write:

[[code]]
    while (1) {
        //  Read and save all frames until we get an empty frame
        //  In this example there is only 1 but it could be more
        zmq_msg_t address;
        zmq_msg_init (&address);
        zmq_recv (worker, &address, 0);

        zmq_msg_t empty;
        zmq_msg_init (&empty);
        zmq_recv (worker, &empty, 0);

        //  Get request, send reply
        zmq_msg_t payload;
        zmq_msg_init (&payload);
        zmq_recv (worker, &payload, 0);
        int char_nbr;
        printf ("Worker: ");
        for (char_nbr = 0; char_nbr < zmq_msg_size (&payload); char_nbr++)
            printf ("%c", *(char *) (zmq_msg_data (&payload) + char_nbr));
        printf ("\n");
        zmq_msg_init_size (&payload, 2);
        memcpy (zmq_msg_data (&payload), "OK", 2);

        zmq_send (worker, &address, ZMQ_SNDMORE);
        zmq_close (&address);
        zmq_send (worker, &empty, ZMQ_SNDMORE);
        zmq_close (&empty);
        zmq_send (worker, &payload, 0);
        zmq_close (&payload);
    }
[[/code]]

What we want is an API that lets us receive and send an entire message in one shot, including all envelopes. One that lets us do what we want with the absolute least lines of code. The ØMQ API itself doesn't aim to do this, but nothing prevents us making layers on top, and part of learning to use ØMQ intelligently is to do exactly that.

The best way to design an API is to start by writing test code, i.e. see what the API would look like in real code. Here's how I'd like that worker code to look:

[[code]]
    while (1) {
        zmsg_t *zmsg = zmsg_recv (worker);
        printf ("Worker: %s\n", zmsg_body (zmsg));
        zmsg_body_set (zmsg, "OK");
        zmsg_send (&zmsg, worker);
    }
[[/code]]

Replacing 22 lines of code with 4 is a good deal, especially since the results are easy to read and understand. I'll use every sneaky assumption in the book to make this API minimalistic:

* We don't care about zero-copy performance, so messages hold copies of data.
* Receiving a message always calls the constructor so we don't need extra constructors.
* Sending a message always calls the destructor, and sets the message reference to null.
* Message parts (addresses and data) are always printable strings.

That last one is tricky because ØMQ uses a binary zero at the start of generated identities. It is painful in C to pass length-specified blobs in and out of APIs. So, our message class encodes and decodes ØMQ identities to make them usable internally as C strings. This is not a high-performance design but it is simple and safe. In other languages you'll be able to make an API that handles binary message parts without such hacks.

So here's the class:

[[code type="C" title="zmsg helper class" name="zmsg"]]
/*  =========================================================================
    zmsg.h

    Multipart message class for example applications.

    Follows the ZFL class conventions and is further developed as the ZFL
    zfl_msg class.  See http://zfl.zeromq.org for more details.

    -------------------------------------------------------------------------
    Copyright (c) 1991-2010 iMatix Corporation <www.imatix.com>
    Copyright other contributors as noted in the AUTHORS file.

    This file is part of the ZeroMQ Guide: http://zguide.zeromq.org

    This is free software; you can redistribute it and/or modify it under the
    terms of the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your option)
    any later version.

    This software is distributed in the hope that it will be useful, but
    WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABIL-
    ITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
    Public License for more details.

    You should have received a copy of the GNU Lesser General Public License
    along with this program. If not, see <http://www.gnu.org/licenses/>.
    =========================================================================
*/

#ifndef __ZMSG_H_INCLUDED__
#define __ZMSG_H_INCLUDED__

#include "zhelpers.h"

#ifdef __cplusplus
extern "C" {
#endif

//  Opaque class structure
typedef struct _zmsg_t zmsg_t;

//  Constructor and destructor
zmsg_t *zmsg_new      (void);
void    zmsg_destroy  (zmsg_t **self_p);

//  Receive and send message, wrapping new/destroy
zmsg_t *zmsg_recv     (void *socket);
void    zmsg_send     (zmsg_t **self, void *socket);

//  Report size of message
size_t  zmsg_parts    (zmsg_t *self);

//  Read and set message body part as C string
char   *zmsg_body     (zmsg_t *self);
void    zmsg_body_set (zmsg_t *self, char *body);
void    zmsg_body_fmt (zmsg_t *self, char *format, ...);

//  Generic push/pop message part off front
void    zmsg_push     (zmsg_t *self, char *part);
char   *zmsg_pop      (zmsg_t *self);

//  Read and set message envelopes
char   *zmsg_address  (zmsg_t *self);
void    zmsg_wrap     (zmsg_t *self, char *address, char *delim);
char   *zmsg_unwrap   (zmsg_t *self);

//  Dump message to stderr, for debugging and tracing
void    zmsg_dump     (zmsg_t *self);

//  Selftest for the class
int     zmsg_test     (int verbose);

#ifdef __cplusplus
}
#endif

#endif

//  Pretty arbitrary limit on complexity of a message
#define ZMSG_MAX_PARTS  255

//  Structure of our class
//  We access these properties only via class methods

struct _zmsg_t {
    //  Part data follows message recv/send order
    unsigned char
          *_part_data [ZMSG_MAX_PARTS];
    size_t _part_size [ZMSG_MAX_PARTS];
    size_t _part_count;
};


//  --------------------------------------------------------------------------
//  Constructor

zmsg_t *
zmsg_new (void)
{
    zmsg_t
        *self;

    self = malloc (sizeof (zmsg_t));
    memset (self, 0, sizeof (zmsg_t));
    return (self);
}


//  --------------------------------------------------------------------------
//  Destructor

void
zmsg_destroy (zmsg_t **self_p)
{
    assert (self_p);
    if (*self_p) {
        zmsg_t *self = *self_p;

        //  Free message parts, if any
        while (self->_part_count)
            free (zmsg_pop (self));

        //  Free object structure
        free (self);
        *self_p = NULL;
    }
}


//  --------------------------------------------------------------------------
//  Formats 17-byte UUID as 33-char string starting with '@'
//  Lets us print UUIDs as C strings and use them as addresses
//
static char *
s_encode_uuid (unsigned char *data)
{
    static char
        hex_char [] = "0123456789ABCDEF";

    assert (data [0] == 0);
    char *uuidstr = malloc (34);
    uuidstr [0] = '@';
    int byte_nbr;
    for (byte_nbr = 0; byte_nbr < 16; byte_nbr++) {
        uuidstr [byte_nbr * 2 + 1] = hex_char [data [byte_nbr + 1] >> 4];
        uuidstr [byte_nbr * 2 + 2] = hex_char [data [byte_nbr + 1] & 15];
    }
    uuidstr [33] = 0;
    return (uuidstr);
}


//  --------------------------------------------------------------------------
//  Formats 17-byte UUID as 33-char string starting with '@'
//  Lets us print UUIDs as C strings and use them as addresses
//
static unsigned char *
s_decode_uuid (char *uuidstr)
{
    static char
        hex_to_bin [128] = {
           -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*            */
           -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*            */
           -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*            */
            0, 1, 2, 3, 4, 5, 6, 7, 8, 9,-1,-1,-1,-1,-1,-1,    /*   0..9     */
           -1,10,11,12,13,14,15,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*   A..F     */
           -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*            */
           -1,10,11,12,13,14,15,-1,-1,-1,-1,-1,-1,-1,-1,-1,    /*   a..f     */
           -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1 };  /*            */

    assert (strlen (uuidstr) == 33);
    assert (uuidstr [0] == '@');
    unsigned char *data = malloc (17);
    int byte_nbr;
    data [0] = 0;
    for (byte_nbr = 0; byte_nbr < 16; byte_nbr++)
        data [byte_nbr + 1]
            = (hex_to_bin [uuidstr [byte_nbr * 2 + 1] & 127] << 4)
            + (hex_to_bin [uuidstr [byte_nbr * 2 + 2] & 127]);

    return (data);
}

//  --------------------------------------------------------------------------
//  Private helper function to store a single message part

static void
_set_part (zmsg_t *self, int part_nbr, unsigned char *data, size_t size)
{
    self->_part_size [part_nbr] = size;
    self->_part_data [part_nbr] = malloc (size + 1);
    memcpy (self->_part_data [part_nbr], data, size);
    //  Convert to C string if needed
    self->_part_data [part_nbr][size] = 0;
}


//  --------------------------------------------------------------------------
//  Receive message from socket
//  Creates a new message and returns it
//  Blocks on recv if socket is not ready for input

zmsg_t *
zmsg_recv (void *socket)
{
    assert (socket);

    zmsg_t *self = zmsg_new ();
    while (1) {
        zmq_msg_t message;
        zmq_msg_init (&message);
        if (zmq_recv (socket, &message, 0)) {
            if (errno != ETERM)
                printf ("E: %s\n", zmq_strerror (errno));
            exit (1);
        }
        //  We handle 0MQ UUIDs as printable strings
        unsigned char *data = zmq_msg_data (&message);
        size_t         size = zmq_msg_size (&message);
        if (size == 17 && data [0] == 0) {
            //  Store message part as string uuid
            char *uuidstr = s_encode_uuid (data);
            self->_part_size [self->_part_count] = strlen (uuidstr);
            self->_part_data [self->_part_count] = (unsigned char *) uuidstr;
            self->_part_count++;
        }
        else
            //  Store this message part
            _set_part (self, self->_part_count++, data, size);

        zmq_msg_close (&message);

        int64_t more;
        size_t more_size = sizeof (more);
        zmq_getsockopt (socket, ZMQ_RCVMORE, &more, &more_size);
        if (!more)
            break;      //  Last message part
    }
    return (self);
}


//  --------------------------------------------------------------------------
//  Send message to socket
//  Destroys message after sending

void
zmsg_send (zmsg_t **self_p, void *socket)
{
    assert (self_p);
    assert (*self_p);
    assert (socket);
    zmsg_t *self = *self_p;

    int part_nbr;
    for (part_nbr = 0; part_nbr < self->_part_count; part_nbr++) {
        //  Could be improved to use zero-copy since we destroy
        //  the message parts after sending anyhow...
        zmq_msg_t message;

        //  Unmangle 0MQ identities for writing to the socket
        unsigned char *data = self->_part_data [part_nbr];
        size_t         size = self->_part_size [part_nbr];
        if (size == 33 && data [0] == '@') {
            unsigned char *uuidbin = s_decode_uuid ((char *) data);
            zmq_msg_init_size (&message, 17);
            memcpy (zmq_msg_data (&message), uuidbin, 17);
            free (uuidbin);
        }
        else {
            zmq_msg_init_size (&message, size);
            memcpy (zmq_msg_data (&message), data, size);
        }
        int rc = zmq_send (socket, &message,
            part_nbr < self->_part_count - 1? ZMQ_SNDMORE: 0);
        assert (rc == 0);
        zmq_msg_close (&message);
    }
    zmsg_destroy (self_p);
}


//  --------------------------------------------------------------------------
//  Report size of message

size_t
zmsg_parts (zmsg_t *self)
{
    return (self->_part_count);
}


//  --------------------------------------------------------------------------
//  Return pointer to message body, if any
//  Caller should not modify the provided data

char *
zmsg_body (zmsg_t *self)
{
    assert (self);

    if (self->_part_count)
        return ((char *) self->_part_data [self->_part_count - 1]);
    else
        return (NULL);
}


//  --------------------------------------------------------------------------
//  Set message body as copy of provided string
//  If message is empty, creates a new message body

void
zmsg_body_set (zmsg_t *self, char *body)
{
    assert (self);
    assert (body);

    if (self->_part_count) {
        assert (self->_part_data [self->_part_count - 1]);
        free (self->_part_data [self->_part_count - 1]);
    }
    else
        self->_part_count = 1;

    _set_part (self, self->_part_count - 1, (void *) body, strlen (body));
}


//  --------------------------------------------------------------------------
//  Set message body using printf format
//  If message is empty, creates a new message body
//  Hard-coded to max. 255 characters for this simplified class

void
zmsg_body_fmt (zmsg_t *self, char *format, ...)
{
    char value [255 + 1];
    va_list args;

    assert (self);
    va_start (args, format);
    vsnprintf (value, 255, format, args);
    va_end (args);
    zmsg_body_set (self, value);
}


//  --------------------------------------------------------------------------
//  Push message part to front of message parts

void
zmsg_push (zmsg_t *self, char *part)
{
    assert (self);
    assert (part);
    assert (self->_part_count < ZMSG_MAX_PARTS - 1);

    //  Move part stack up one element and insert new part
    memmove (&self->_part_data [1], &self->_part_data [0],
        (ZMSG_MAX_PARTS - 1) * sizeof (unsigned char *));
    memmove (&self->_part_size [1], &self->_part_size [0],
        (ZMSG_MAX_PARTS - 1) * sizeof (size_t));
    _set_part (self, 0, (void *) part, strlen (part));
    self->_part_count += 1;
}


//  --------------------------------------------------------------------------
//  Pop message part off front of message parts
//  Caller should free returned string when finished with it

char *
zmsg_pop (zmsg_t *self)
{
    assert (self);
    assert (self->_part_count);

    //  Remove first part and move part stack down one element
    char *part = (char *) self->_part_data [0];
    memmove (&self->_part_data [0], &self->_part_data [1],
        (ZMSG_MAX_PARTS - 1) * sizeof (unsigned char *));
    memmove (&self->_part_size [0], &self->_part_size [1],
        (ZMSG_MAX_PARTS - 1) * sizeof (size_t));
    self->_part_count--;
    return (part);
}


//  --------------------------------------------------------------------------
//  Return pointer to outer message address, if any
//  Caller should not modify the provided data

char *
zmsg_address (zmsg_t *self)
{
    assert (self);

    if (self->_part_count)
        return ((char *) self->_part_data [0]);
    else
        return (NULL);
}


//  --------------------------------------------------------------------------
//  Wraps message in new address envelope
//  If delim is not null, creates two-part envelope

void
zmsg_wrap (zmsg_t *self, char *address, char *delim)
{
    assert (self);
    assert (address);

    //  Push optional delimiter and then address
    if (delim)
        zmsg_push (self, delim);
    zmsg_push (self, address);
}


//  --------------------------------------------------------------------------
//  Unwraps outer message envelope and returns address
//  Discards empty message part after address, if any
//  Caller should free returned string when finished with it

char *
zmsg_unwrap (zmsg_t *self)
{
    assert (self);

    char *address = zmsg_pop (self);
    if (*zmsg_address (self) == 0)
        free (zmsg_pop (self));
    return (address);
}


//  --------------------------------------------------------------------------
//  Dump message to stderr, for debugging and tracing

void
zmsg_dump (zmsg_t *self)
{
    int part_nbr;
    for (part_nbr = 0; part_nbr < self->_part_count; part_nbr++) {
        unsigned char *data = self->_part_data [part_nbr];
        size_t         size = self->_part_size [part_nbr];

        //  Dump the message as text or binary
        int is_text = 1;
        int char_nbr;
        for (char_nbr = 0; char_nbr < size; char_nbr++)
            if (data [char_nbr] < 32 || data [char_nbr] > 127)
                is_text = 0;

        fprintf (stderr, "[%03d] ", (int) size);
        for (char_nbr = 0; char_nbr < size; char_nbr++) {
            if (is_text)
                fprintf (stderr, "%c", data [char_nbr]);
            else
                fprintf (stderr, "%02X", (unsigned char) data [char_nbr]);
        }
        fprintf (stderr, "\n");
    }
    fflush (stderr);
}



//  --------------------------------------------------------------------------
//  Runs self test of class

int
zmsg_test (int verbose)
{
    zmsg_t
        *zmsg;
    int rc;

    printf (" * zmsg: ");

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    void *output = zmq_socket (context, ZMQ_XREQ);
    rc = zmq_bind (output, "ipc://zmsg_selftest.ipc");
    assert (rc == 0);
    void *input = zmq_socket (context, ZMQ_XREP);
    rc = zmq_connect (input, "ipc://zmsg_selftest.ipc");
    assert (rc == 0);

    //  Test send and receive of single-part message
    zmsg = zmsg_new ();
    assert (zmsg);
    zmsg_body_set (zmsg, "Hello");
    assert (strcmp (zmsg_body (zmsg), "Hello") == 0);
    zmsg_send (&zmsg, output);
    assert (zmsg == NULL);

    zmsg = zmsg_recv (input);
    assert (zmsg_parts (zmsg) == 2);
    if (verbose)
        zmsg_dump (zmsg);
    assert (strcmp (zmsg_body (zmsg), "Hello") == 0);

    //  Test send and receive of multi-part message
    zmsg = zmsg_new ();
    zmsg_body_set (zmsg, "Hello");
    zmsg_wrap     (zmsg, "address1", "");
    zmsg_wrap     (zmsg, "address2", NULL);
    assert (zmsg_parts (zmsg) == 4);
    zmsg_send (&zmsg, output);

    zmsg = zmsg_recv (input);
    if (verbose)
        zmsg_dump (zmsg);
    assert (zmsg_parts (zmsg) == 5);
    assert (strlen (zmsg_address (zmsg)) == 33);
    free (zmsg_unwrap (zmsg));
    assert (strcmp (zmsg_address (zmsg), "address2") == 0);
    zmsg_body_fmt (zmsg, "%c%s", 'W', "orld");
    zmsg_send (&zmsg, output);

    zmsg = zmsg_recv (input);
    free (zmsg_unwrap (zmsg));
    assert (zmsg_parts (zmsg) == 4);
    assert (strcmp (zmsg_body (zmsg), "World") == 0);
    char *part;
    part = zmsg_unwrap (zmsg);
    assert (strcmp (part, "address2") == 0);
    free (part);

    //  Pull off address 1, check that empty part was dropped
    part = zmsg_unwrap (zmsg);
    assert (strcmp (part, "address1") == 0);
    assert (zmsg_parts (zmsg) == 1);
    free (part);

    //  Check that message body was correctly modified
    part = zmsg_pop (zmsg);
    assert (strcmp (part, "World") == 0);
    assert (zmsg_parts (zmsg) == 0);

    zmsg_destroy (&zmsg);
    assert (zmsg == NULL);

    printf ("OK\n");
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/zmsg.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/zmsg.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/zmsg.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/zmsg.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/zmsg.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/zmsg.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/zmsg.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/zmsg.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/zmsg.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/zmsg.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/zmsg.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/zmsg.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/zmsg.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/zmsg.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/zmsg.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/zmsg.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/zmsg.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/zmsg.rb Ruby]//
[[/collapsible]]
[[/>]]

Here is a test wrapper:

[[code type="C" title="zmsg test wrapper" name="zmsg_test"]]
//
//  Test zmsg class
//
#include "zhelpers.h"
#include "zmsg.c"

int main (int argc, char *argv[])
{
    zmsg_test (1);
    return EXIT_SUCCESS;
}
[[/code]]
[[>]]
examples/C/zmsg_test.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/zmsg_test.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/zmsg_test.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/zmsg_test.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/zmsg_test.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/zmsg_test.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/zmsg_test.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/zmsg_test.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/zmsg_test.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/zmsg_test.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/zmsg_test.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/zmsg_test.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/zmsg_test.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/zmsg_test.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/zmsg_test.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/zmsg_test.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/zmsg_test.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/zmsg_test.rb Ruby]//
[[/collapsible]]
[[/>]]

Build and run the test wrapper, and it should print something like this, including a couple of message dumps that show ØMQ-generated socket identities:

[[code]]
----------------------------------------
[017] 20B60BC50DC16542C2A9AB7B01872F359D
[005] Hello
----------------------------------------
[017] 20B60BC50DC16542C2A9AB7B01872F359D
[008] address2
[008] address1
[000]
[005] Hello
 * zmsg: OK
[[/code]]

And here is the LRU queue broker rewritten to use the zmsg class. Compare the code that works with messages, you'll see it's much shorter and 'just works':

[[code type="C" title="LRU queue broker using zmsg" name="lruqueue2"]]
//
//  Least-recently used (LRU) queue device
//  Demonstrates use of the zmsg class
//
//  Changes for 2.1:
//  - added version assertion
//  - use separate contexts for each thread
//  - close sockets in each child thread
//  - call zmq_term in each thread before ending
//  - removed sleep(1) at end of main thread
//
#include "zhelpers.h"
#include "zmsg.c"

#define NBR_CLIENTS 10
#define NBR_WORKERS 3

//  A simple dequeue operation for queue implemented as array
#define DEQUEUE(q) memmove (&(q)[0], &(q)[1], sizeof (q) - sizeof (q [0]))

//  Basic request-reply client using REQ socket
//
static void *
client_thread (void *args) {
    void *context = zmq_init (1);
    void *client = zmq_socket (context, ZMQ_REQ);
    s_set_id (client);          //  Makes tracing easier
    zmq_connect (client, "ipc://frontend.ipc");

    //  Send request, get reply
    s_send (client, "HELLO");
    char *reply = s_recv (client);
    printf ("Client: %s\n", reply);
    free (reply);

    zmq_close (client);
    zmq_term (context);
    return NULL;
}

//  Worker using REQ socket to do LRU routing
//
static void *
worker_thread (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_REQ);
    s_set_id (worker);          //  Makes tracing easier
    zmq_connect (worker, "ipc://backend.ipc");

    //  Tell broker we're ready for work
    s_send (worker, "READY");

    while (1) {
        zmsg_t *zmsg = zmsg_recv (worker);
        printf ("Worker: %s\n", zmsg_body (zmsg));
        zmsg_body_set (zmsg, "OK");
        zmsg_send (&zmsg, worker);
    }
    zmq_close (worker);
    zmq_term (context);
    return NULL;
}

int main (int argc, char *argv[])
{
    s_version_assert (2, 1);

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    void *frontend = zmq_socket (context, ZMQ_XREP);
    void *backend  = zmq_socket (context, ZMQ_XREP);
    zmq_bind (frontend, "ipc://frontend.ipc");
    zmq_bind (backend,  "ipc://backend.ipc");

    int client_nbr;
    for (client_nbr = 0; client_nbr < NBR_CLIENTS; client_nbr++) {
        pthread_t client;
        pthread_create (&client, NULL, client_thread, NULL);
    }
    int worker_nbr;
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_thread, NULL);
    }
    //  Logic of LRU loop
    //  - Poll backend always, frontend only if 1+ worker ready
    //  - If worker replies, queue worker as ready and forward reply
    //    to client if necessary
    //  - If client requests, pop next worker and send request to it

    //  Queue of available workers
    int available_workers = 0;
    char *worker_queue [NBR_WORKERS];

    while (1) {
        //  Initialize poll set
        zmq_pollitem_t items [] = {
            //  Always poll for worker activity on backend
            { backend,  0, ZMQ_POLLIN, 0 },
            //  Poll front-end only if we have available workers
            { frontend, 0, ZMQ_POLLIN, 0 }
        };
        if (available_workers)
            zmq_poll (items, 2, -1);
        else
            zmq_poll (items, 1, -1);

        //  Handle worker activity on backend
        if (items [0].revents & ZMQ_POLLIN) {
            zmsg_t *zmsg = zmsg_recv (backend);
            //  Use worker address for LRU routing
            assert (available_workers < NBR_WORKERS);
            worker_queue [available_workers++] = zmsg_unwrap (zmsg);

            //  Forward message to client if it's not a READY
            if (strcmp (zmsg_address (zmsg), "READY") == 0)
                zmsg_destroy (&zmsg);
            else {
                zmsg_send (&zmsg, frontend);
                if (--client_nbr == 0)
                    break;      //  Exit after N messages
            }
        }
        if (items [1].revents & ZMQ_POLLIN) {
            //  Now get next client request, route to next worker
            zmsg_t *zmsg = zmsg_recv (frontend);
            zmsg_wrap (zmsg, worker_queue [0], "");
            zmsg_send (&zmsg, backend);

            //  Dequeue and drop the next worker address
            free (worker_queue [0]);
            DEQUEUE (worker_queue);
            available_workers--;
        }
    }
    zmq_close (frontend);
    zmq_close (backend);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/lruqueue2.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/lruqueue2.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/lruqueue2.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/lruqueue2.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/lruqueue2.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/lruqueue2.cs C#]//
//[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/lruqueue2.lisp Common Lisp]//
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/lruqueue2.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/lruqueue2.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/lruqueue2.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/lruqueue2.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/lruqueue2.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/lruqueue2.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/lruqueue2.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/lruqueue2.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/lruqueue2.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/lruqueue2.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/lruqueue2.rb Ruby]//
[[/collapsible]]
[[/>]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Router-to-Router (N-to-N) Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

We've seen XREP/router sockets talking to dealers, mamas, and papas. The last case is routers talking to routers. One use-case for this is a web farm that has redundant HTTP front-ends talking to an array of asynchronous back-end workers. Each worker accepts requests from any of the front-end HTTP servers, and processes them asynchronously, sending asynchronous replies back. A fully asynchronous worker has some internal concurrency but we don't really care about that here. What interests us is how N workers can talk to N front-ends.

[[=image http://github.com/imatix/zguide/raw/master/images/fig46.png]]

Here's a simplified example with a single front-end and a single worker, cross connected and routing to each other. We just send a message each way, and dump the message envelopes:

[[code type="C" title="Cross-connected routers" name="rtrouter"]]
//
//  Cross-connected XREP sockets addressing each other
//
#include "zhelpers.h"

int main () {
    void *context = zmq_init (1);

    void *worker = zmq_socket (context, ZMQ_XREP);
    zmq_setsockopt (worker, ZMQ_IDENTITY, "WORKER", 6);
    zmq_bind (worker, "ipc://rtrouter.ipc");

    void *server = zmq_socket (context, ZMQ_XREP);
    zmq_setsockopt (server, ZMQ_IDENTITY, "SERVER", 6);
    zmq_connect (server, "ipc://rtrouter.ipc");

    //  Wait for the worker to connect so that when we send a message
    //  with routing envelope, it will actually match the worker...
    sleep (1);

    s_sendmore (server, "WORKER");
    s_sendmore (server, "");
    s_send     (server, "send to worker");
    s_dump     (worker);

    s_sendmore (worker, "SERVER");
    s_sendmore (worker, "");
    s_send     (worker, "send to server");
    s_dump     (server);

    zmq_close (worker);
    zmq_close (server);
    zmq_term (context);
    return 0;
}
[[/code]]
[[>]]
examples/C/rtrouter.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/rtrouter.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/rtrouter.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/rtrouter.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/rtrouter.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/rtrouter.cs C#]//
**[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/rtrouter.lisp Common Lisp]**
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/rtrouter.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/rtrouter.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/rtrouter.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/rtrouter.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/rtrouter.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/rtrouter.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/rtrouter.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/rtrouter.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/rtrouter.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/rtrouter.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/rtrouter.rb Ruby]//
[[/collapsible]]
[[/>]]

The program produces this output:

[[code]]
----------------------------------------
[008] SERVER
[000]
[014] send to worker
----------------------------------------
[006] WORKER
[000]
[014] send to server
[[/code]]

Some comments on this code:

* We need to give the two sockets time to connect and exchange identities. If we don't, then they will discard the messages you try to send to them, not recognizing the address. Try commenting out the sleep(1), and then trying again.

* We can set and use identities on both bound and connected sockets, as this example shows.

* ØMQ currently has [http://github.com/zeromq/zeromq2/issues/issue/82 a bug] when cross-connecting XREP sockets over {{inproc}}. The binding socket does not use any identity you set, and always uses a generated UUID.

Although the router-to-router pattern looks ideal for asynchronous N-to-N routing, it has some pitfalls. First, any design with N-to-N connections will not scale beyond a small number of clients and servers. You should really create a device in the middle that turns it into two 1-to-N patterns. This gives you a structure like the LRU queue broker, though you would use XREQ at the front-end and worker sides to get streaming.

Second, it may become confusing if you try to put two XREP sockets at the same logical level. One must bind, one must connect, and request-reply is inherently asymmetric. However, the next point takes care of this.

Third, one side of the connection must know the identity of the other. You cannot do xrep-to-xrep flows between two anonymous sockets. In practice this means you need a name service, configuration data, or some other magic to define and share the identities of one of the peers. It's convenient therefore to treat the more static side of the flow as 'server', and give it a fixed, known identity, and then treat the dynamic side as 'client'. The client will have to connect to the server, then send it a message using the server's known identity as address, and then the server can respond to the client.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
+++ Worked Example: Inter-Broker Routing
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Let's take everything we've seen so far, and scale things up. Our best client calls us urgently and asks for a design of a large cloud computing facility. He has this vision of a cloud that spans many data centers, each a cluster of clients and workers, and that works together as a whole.

Because we're smart enough to know that practice always beats theory, we propose to make a working simulation using ØMQ. Our client, eager to lock down the budget before his own boss changes his mind, and having read great things about ØMQ on Twitter, agrees.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Establishing the Details
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Several espressos later, we want to jump into writing code but a little voice tells us to get more details before making a sensational solution to entirely the wrong problem. What kind of work is the cloud doing?, we ask. The client explains:

* Workers run on various kinds of hardware, but they are all able to handle any task. There are several hundred workers per cluster, and as many as a dozen clusters in total.

* Clients create tasks for workers. Each task is an independent unit of work and all the client wants is to find an available worker, and send it the task, as soon as possible. There will be a lot of clients and they'll come and go arbitrarily.

* The real difficulty is to be able to add and remove clusters at any time. A cluster can can leave or join the cloud instantly, bringing all its workers and clients with it.

* If there are no workers in their own cluster, clients' tasks will go off to other available workers in the cloud.

* Clients send out one task at a time, waiting for a reply. If they don't get an answer within X seconds they'll just send out the task again. This ain't our concern, the client API does it already.

* Workers process one task at a time, they are very simple beasts. If they crash, they get restarted by whatever script started them.

So we double check to make sure that we understood this correctly:

* There will be some kind of super-duper network interconnect between clusters, right?  The client says, yes, of course, we're not idiots.

* What kind of volumes are we talking about, we ask?  The client replies, up to a thousand clients per cluster, each doing max. ten requests per second. Requests are small, and replies are also small, no more than 1K bytes each.

So we do a little calculation and see that this will work nicely over plain TCP. 2,500 clients x 10/second x 1,000 bytes x 2 directions = 50MB/sec or 400Mb/sec, not a problem for a 1Gb network.

It's a straight-forward problem that requires no exotic hardware or protocols, just some clever routing algorithms and careful design. We start by designing one cluster (one data center) and then we figure out how to connect clusters together.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Architecture of a Single Cluster
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Workers and clients are synchronous. We want to use the least-recently used algorithm to route tasks to workers. Workers are all identical, our facility has no notion of different services. Workers are anonymous, clients never address them directly. We make no attempt here to provide guaranteed delivery, retry, etc.

For reasons we already looked at, clients and workers won't speak to each other directly. It makes it impossible to add or remove nodes dynamically. So our basic model consists of the request-reply message broker we saw earlier:

[[=image http://github.com/imatix/zguide/raw/master/images/fig47.png]]

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Scaling to Multiple Clusters
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Now we scale this out to more than one cluster. Each cluster has a set of clients and workers, and a broker that joins these together:

[[=image http://github.com/imatix/zguide/raw/master/images/fig48.png]]

The question is: how do we get the clients of each cluster talking to the workers of the other cluster?  There are a few possibilities, each with pros and cons:

* Clients could connect directly to both brokers. The advantage is that we don't need to modify brokers or workers. But clients get more complex, and become aware of the overall topology. If we want to add, e.g. a third or forth cluster, all the clients are affected. In effect we have to move routing and failover logic into the clients and that's not nice.

* Workers might connect directly to both brokers. But mama workers can't do that, they can only reply to one broker. We might use papas but papas don't give us customizable broker-to-worker routing like LRU, only the built-in load balancing. That's a fail, if we want to distribute work to idle workers: we precisely need LRU. One solution would be to use router sockets for the worker nodes. Let's label this "Idea #1".

* Brokers could connect to each other. This looks neatest because it creates the fewest additional connections. We can't add clusters on the fly but that is probably out of scope. Now clients and workers remain ignorant of the real network topology, and brokers tell each other when they have spare capacity. Let's label this "Idea #2".

Let's explore Idea #1. Workers connecting to both brokers and accepting jobs from either:

[[=image http://github.com/imatix/zguide/raw/master/images/fig49.png]]

It looks feasible. However it doesn't provide what we wanted, which was that clients get local workers if possible and remote workers only if it's better than waiting. Also workers will signal "ready" to both brokers and can get two jobs at once, while other workers remain idle. It seems this design fails because again we're putting routing logic at the edges.

So idea #2 then. We interconnect the brokers and don't touch the clients or workers, which are mamas like we're used to:

[[=image http://github.com/imatix/zguide/raw/master/images/fig50.png]]

This design is appealing because the problem is solved in one place, invisibly to the rest of the world. Basically, brokers open secret channels to each other and whisper, like camel traders, "//hey, I've got some spare capacity, if you have too many clients give me a shout and we'll deal".//

It is in effect just a more sophisticated routing algorithm: brokers become subcontractors for each other. Other things to like about this design, even before we play with real code:

* It treats the common case (clients and workers on the same cluster) as default and does extra work for the exceptional case (shuffling jobs between clusters).

* It lets us use different message flows for the different types of work. That means we can handle them differently, e.g. using different types of network connection.

* It feels like it would scale smoothly. Interconnecting three, or more brokers doesn't get over-complex. If we find this to be a problem, it's easy to solve by adding a super-broker.

We'll now make a worked example. We'll pack an entire cluster into one process. That is obviously not realistic but it makes it simple to simulate, and the simulation can accurately scale to real processes. This is the beauty of ØMQ, you can design at the microlevel and scale that up to the macro level. Thread become processes, become boxes and the patterns and logic remain the same. Each of our 'cluster' processes contains client threads, worker threads, and a broker thread.

We know the basic model well by now:

* The client (mama/REQ) threads create workloads and pass them to the broker (router/XREP).
* The worker (mama/REQ) threads process workloads and return the results to the broker (router/XREP).
* The broker queues and distributes workloads using the LRU routing model.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Federation vs. Peering
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

There are several possible way to interconnecting brokers. What we //want// is to be able to tell other brokers, "we have capacity", and then receive multiple tasks. We also need to be able to tell other brokers "stop, we're full". It doesn't need to be perfect: sometimes we may accept jobs we can't process immediately, then we'll do them as soon as possible.

The simplest interconnect is //federation// in which brokers simulate clients and workers for each other. We would do this by connecting our frontend to the other broker's backend socket. Note that it is legal to both bind a socket to an endpoint and connect it to other endpoints.

[[=image http://github.com/imatix/zguide/raw/master/images/fig51.png]]

This would give us simple logic in both brokers and a reasonably good mechanism: when there are no clients, tell the other broker 'ready', and accept one job from it. The problem is also that it is too simple for this problem. A federated broker would be able to handle only one task at once. If the broker emulates a lock-step client and worker, it is by definition also going to be lock-step and if it has lots of available workers they won't be used. Our brokers need to be connected in a fully asynchronous fashion.

The federation model is perfect for other kinds of routing, especially service-oriented architectures or SOAs (which route by service name and proximity rather than LRU or load-balancing or random scatter). So don't dismiss it as useless, it's just not right for least-recently used and cluster load-balancing.

So instead of federation, let's look at a //peering// approach in which brokers are explicitly aware of each other and talk over privileged channels. Let's break this down, assuming we want to interconnect N brokers. Each broker has (N - 1) peers, and all brokers are using exactly the same code and logic. There are two distinct flows of information between brokers:

* Each broker needs to tell its peers how many workers it has available at any time. This can be fairly simple information, just a quantity that is updated regularly. The obvious (and correct) socket pattern for this is publish-subscribe. So every broker opens a PUB socket and publishes state information on that, and every broker also opens a SUB socket and connects that to the PUB socket of every other broker, to get state information from its peers.

* Each broker needs a way to delegate tasks to a peer and get replies back, asynchronously. We'll do this using router/router (XREP/XREP) sockets, no other combination works. Each broker has two such sockets: one for tasks it receives, one for tasks it delegates. If we didn't use two sockets it would be more work to know whether we were reading a request or a reply each time. That would mean adding more information to the message envelope.

And there is also the flow of information between a broker and its local clients and workers.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ The Naming Ceremony
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Three flows x two sockets for each flow = six sockets that we have to manage in the broker.   Choosing good names is vital to keeping a multi-socket juggling act reasonably coherent in our minds. Sockets //do// something and what they do should form the basis for their names. It's about being able to read the code several weeks later on a cold Monday morning before coffee, and not feeling pain.

Let's do a shamanistic naming ceremony for the sockets. The three flows are:

* A //local// request-reply flow between the broker and its clients and workers.
* A //cloud// request-reply flow between the broker and its peer brokers.
* A //state// flow between the broker and its peer brokers.

Finding meaningful names that are all the same length means our code will align beautifully. It may seem irrelevant but such attention to such details turn ordinary code into something more like art.

For each flow the broker has two sockets that we can orthogonally call the "frontend" and "backend". We've used these names quite often. A frontend receives information or tasks. A backend sends those out to other peers. The conceptual flow is from front to back (with replies going in the opposite direction from back to front).

So in all the code we write for this tutorial will use these socket names:

* //localfe// and //localbe// for the local flow.
* //cloudfe// and //cloudbe// for the cloud flow.
* //statefe// and //statebe// for the state flow.

For our transport we'll use {{ipc}} for everything. This has the advantage of working like {{tcp}} in terms of connectivity (i.e. it's a disconnected transport, unlike {{inproc}}), yet we don't need IP addresses or DNS names, which would be a pain here. Instead, we will use {{ipc}} endpoints called //something//-{{local}}, //something//-{{cloud}}, and //something//-{{state}}, where //something// is the name of our simulated cluster.

You may be thinking that this is a lot of work for some names. Why not call them s1, s2, s3, s4, etc.?  The answer is that if your brain is not a perfect machine, you need a lot of help when reading code, and we'll see that these names do help. It is a lot easier to remember "three flows, two directions" than "six different sockets".

Here is the broker socket arrangement, then:

[[=image http://github.com/imatix/zguide/raw/master/images/fig52.png]]

Note that we connect the cloudbe in each broker to the cloudfe in every other broker, and likewise we connect the statebe in each broker to the statefe in every other broker.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Prototyping the State Flow
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Since each socket flow has its own little traps for the unwary, we will test them in real code one by one, rather than try to throw the whole lot into code in one go. When we're happy with each flow, we can put them together into a full program. We'll start with the state flow:

[[=image http://github.com/imatix/zguide/raw/master/images/fig53.png]]

Here is how this works in code:

[[code type="C" title="Prototype state flow" name="peering1"]]
//
//  Broker peering simulation (part 1)
//  Prototypes the state flow
//
#include "zhelpers.h"
#include "zmsg.c"

int main (int argc, char *argv [])
{
    //  First argument is this broker's name
    //  Other arguments are our peers' names
    //
    if (argc < 2) {
        printf ("syntax: peering1 me {you}...\n");
        exit (EXIT_FAILURE);
    }
    char *self = argv [1];
    printf ("I: preparing broker at %s...\n", self);
    srandom ((unsigned) time (NULL));

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    char endpoint [256];

    //  Bind statebe to endpoint
    void *statebe = zmq_socket (context, ZMQ_PUB);
    snprintf (endpoint, 255, "ipc://%s-state.ipc", self);
    int rc = zmq_bind (statebe, endpoint);
    assert (rc == 0);

    //  Connect statefe to all peers
    void *statefe = zmq_socket (context, ZMQ_SUB);
    zmq_setsockopt (statefe, ZMQ_SUBSCRIBE, "", 0);

    int argn;
    for (argn = 2; argn < argc; argn++) {
        char *peer = argv [argn];
        printf ("I: connecting to state backend at '%s'\n", peer);
        snprintf (endpoint, 255, "ipc://%s-state.ipc", peer);
        rc = zmq_connect (statefe, endpoint);
        assert (rc == 0);
    }
    //  Send out status messages to peers, and collect from peers
    //  The zmq_poll timeout defines our own heartbeating
    //
    while (1) {
        //  Initialize poll set
        zmq_pollitem_t items [] = {
            { statefe, 0, ZMQ_POLLIN, 0 }
        };
        //  Poll for activity, or 1 second timeout
        rc = zmq_poll (items, 1, 1000000);
        assert (rc >= 0);

        //  Handle incoming status message
        if (items [0].revents & ZMQ_POLLIN) {
            zmsg_t *zmsg = zmsg_recv (statefe);
            printf ("%s - %s workers free\n",
                zmsg_address (zmsg), zmsg_body (zmsg));
            zmsg_destroy (&zmsg);
        }
        else {
            //  Send random value for worker availability
            zmsg_t *zmsg = zmsg_new ();
            zmsg_body_fmt (zmsg, "%d", randof (10));
            //  We stick our own address onto the envelope
            zmsg_wrap (zmsg, self, NULL);
            zmsg_send (&zmsg, statebe);
        }
    }
    //  We never get here but clean up anyhow
    zmq_close (statefe);
    zmq_term (context);
    return EXIT_SUCCESS;
}
[[/code]]
[[>]]
examples/C/peering1.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/peering1.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/peering1.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/peering1.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/peering1.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/peering1.cs C#]//
//[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/peering1.lisp Common Lisp]//
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/peering1.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/peering1.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/peering1.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/peering1.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/peering1.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/peering1.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/peering1.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/peering1.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/peering1.php PHP]**
**[*http://github.com/imatix/zguide/blob/master/examples/Python/peering1.py Python]**
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/peering1.rb Ruby]//
[[/collapsible]]
[[/>]]

Notes about this code:

* Each broker has an identity that we use to construct {{ipc}} endpoint names. A real broker would need to work with TCP and a more sophisticated configuration scheme. We'll look at such schemes later in this book but for now, using generated {{ipc}} names lets us ignore the problem of where to get TCP/IP addresses or names from.

* We use a [http://api.zeromq.org/zmq_poll.html zmq_poll(3)] loop as the core of the program. This processes incoming messages and sends out state messages. We send a state message //only// if we did not get any incoming messages //and// we waited for a second. If we send out a state message each time we get one in, we'll get message storms.

* We use a two-part pubsub message consisting of sender address and data. Note that we will need to know the address of the publisher in order to send it tasks, and the only way is to send this explicitly as a part of the message.

* We don't set identities on subscribers, because if we did then we'd get out of date state information when connecting to running brokers.

* We don't set a HWM on the publisher, since subscribers are transient. We might set a HWM of 1 but it's extra work for nothing here.

We can build this little program and run it three times to simulate three clusters. Let's call them DC1, DC2, and DC3 (the names are arbitrary). We run these three commands, each in a separate window:

[[code]]
peering1 DC1 DC2 DC3  #  Start DC1 and connect to DC2 and DC3
peering1 DC2 DC1 DC3  #  Start DC2 and connect to DC1 and DC3
peering1 DC3 DC1 DC2  #  Start DC3 and connect to DC1 and DC2
[[/code]]

You'll see each cluster report the state of its peers, and after a few seconds they will all happily be printing random numbers once per second. Try this and satisfy yourself that the three brokers all match up and synchronize to per-second state updates.

In real life we'd not send out state messages at regular intervals but rather whenever we had a state change, i.e. whenever a worker becomes available or unavailable. That may seem like a lot of traffic but state messages are small and we've established that the inter-cluster connections are super-fast.

If we wanted to send state messages at precise intervals we'd create a child thread and open the statebe socket in that thread. We'd then send irregular state updates to that child thread from our main thread, and allow the child thread to conflate them into regular outgoing messages. This is more work than we need here.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Prototyping the Local and Cloud Flows
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Let's now prototype at the flow of tasks via the local and cloud sockets. This code pulls requests from clients and then distributes them to local workers and cloud peers on a random basis:

[[=image http://github.com/imatix/zguide/raw/master/images/fig54.png]]

Before we jump into the code, which is getting a little complex, let's sketch the core routing logic and break it down into a simple but robust design.

We need two queues, one for requests from local clients and one for requests from cloud clients. One option would be to pull messages off the local and cloud frontends, and pump these onto their respective queues. But this is kind of pointless because ØMQ sockets //are// queues already. So let's use the ØMQ socket buffers as queues.

This was the technique we used in the LRU queue broker, and it worked nicely. We only read from the two frontends when there is somewhere to send the requests. We can always read from the backends, since they give us replies to route back. As long as the backends aren't talking to us, there's no point in even looking at the frontends.

So our main loop becomes:

* Poll the backends for activity. When we get a message, it may be "READY" from a worker or it may be a reply. If it's a reply, route back via the local or cloud frontend.

* If a worker replied, it became available, so we queue it and count it.

* While there are workers available, take a request, if any, from either frontend and route to a local worker, or randomly, a cloud peer.

Randomly sending tasks to a peer broker rather than a worker simulates work distribution across the cluster. It's idiot but that is fine for this stage.

We use broker identities to route messages between brokers. Each broker has a name, which we provide on the command line in this simple prototype. As long as these names don't overlap with the ØMQ-generated UUIDs used for client nodes, we can figure out whether to route a reply back to a client or to a broker.

Here is how this works in code. The interesting part starts around the comment "Interesting part".

[[code type="C" title="Prototype local and cloud flow" name="peering2"]]
//
//  Broker peering simulation (part 2)
//  Prototypes the request-reply flow
//
#include "zmsg.c"

#define NBR_CLIENTS 10
#define NBR_WORKERS 3

//  A simple dequeue operation for queue implemented as array
#define DEQUEUE(q) memmove (&(q)[0], &(q)[1], sizeof (q) - sizeof (q [0]))

//  Request-reply client using REQ socket
//
static void *
client_thread (void *context) {
    void *client = zmq_socket (context, ZMQ_REQ);
    zmq_connect (client, "inproc://localfe");

    zmsg_t *zmsg = zmsg_new ();
    while (1) {
        //  Send request, get reply
        zmsg_body_set (zmsg, "HELLO");
        zmsg_send (&zmsg, client);
        zmsg = zmsg_recv (client);
        printf ("I: client status: %s\n", zmsg_body (zmsg));
    }
    return (NULL);
}

//  Worker using REQ socket to do LRU routing
//
static void *
worker_thread (void *context) {
    void *worker = zmq_socket (context, ZMQ_REQ);
    zmq_connect (worker, "inproc://localbe");

    //  Tell broker we're ready for work
    zmsg_t *zmsg = zmsg_new ();
    zmsg_body_set (zmsg, "READY");
    zmsg_send (&zmsg, worker);

    while (1) {
        zmsg = zmsg_recv (worker);
        //  Do some 'work'
        sleep (1);
        zmsg_body_fmt (zmsg, "OK - %04x", randof (0x10000));
        zmsg_send (&zmsg, worker);
    }
    return (NULL);
}


int main (int argc, char *argv [])
{
    //  First argument is this broker's name
    //  Other arguments are our peers' names
    //
    if (argc < 2) {
        printf ("syntax: peering2 me {you}...\n");
        exit (EXIT_FAILURE);
    }
    char *self = argv [1];
    printf ("I: preparing broker at %s...\n", self);
    srandom ((unsigned) time (NULL));

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    char endpoint [256];

    //  Bind cloud frontend to endpoint
    void *cloudfe = zmq_socket (context, ZMQ_XREP);
    snprintf (endpoint, 255, "ipc://%s-cloud.ipc", self);
    zmq_setsockopt (cloudfe, ZMQ_IDENTITY, self, strlen (self));
    int rc = zmq_bind (cloudfe, endpoint);
    assert (rc == 0);

    //  Connect cloud backend to all peers
    void *cloudbe = zmq_socket (context, ZMQ_XREP);
    zmq_setsockopt (cloudbe, ZMQ_IDENTITY, self, strlen (self));

    int argn;
    for (argn = 2; argn < argc; argn++) {
        char *peer = argv [argn];
        printf ("I: connecting to cloud frontend at '%s'\n", peer);
        snprintf (endpoint, 255, "ipc://%s-cloud.ipc", peer);
        rc = zmq_connect (cloudbe, endpoint);
        assert (rc == 0);
    }

    //  Prepare local frontend and backend
    void *localfe = zmq_socket (context, ZMQ_XREP);
    zmq_bind (localfe, "inproc://localfe");
    void *localbe = zmq_socket (context, ZMQ_XREP);
    zmq_bind (localbe, "inproc://localbe");

    //  Get user to tell us when we can start...
    printf ("Press Enter when all brokers are started: ");
    getchar ();

    //  Start local workers
    int worker_nbr;
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_thread, context);
    }
    //  Start local clients
    int client_nbr;
    for (client_nbr = 0; client_nbr < NBR_CLIENTS; client_nbr++) {
        pthread_t client;
        pthread_create (&client, NULL, client_thread, context);
    }

    //  Interesting part
    //  -------------------------------------------------------------
    //  Request-reply flow
    //  - Poll backends and process local/cloud replies
    //  - While worker available, route localfe to local or cloud

    //  Queue of available workers
    int capacity = 0;
    char *worker_queue [NBR_WORKERS];

    while (1) {
        zmq_pollitem_t backends [] = {
            { localbe, 0, ZMQ_POLLIN, 0 },
            { cloudbe, 0, ZMQ_POLLIN, 0 }
        };
        //  If we have no workers anyhow, wait indefinitely
        rc = zmq_poll (backends, 2, capacity? 1000000: -1);
        assert (rc >= 0);

        //  Handle reply from local worker
        zmsg_t *zmsg = NULL;
        if (backends [0].revents & ZMQ_POLLIN) {
            zmsg = zmsg_recv (localbe);

            assert (capacity < NBR_WORKERS);
            //  Use worker address for LRU routing
            worker_queue [capacity++] = zmsg_unwrap (zmsg);
            if (strcmp (zmsg_address (zmsg), "READY") == 0)
                zmsg_destroy (&zmsg);   //  Don't route it
        }
        //  Or handle reply from peer broker
        else
        if (backends [1].revents & ZMQ_POLLIN) {
            zmsg = zmsg_recv (cloudbe);
            //  We don't use peer broker address for anything
            free (zmsg_unwrap (zmsg));
        }
        //  Route reply to cloud if it's addressed to a broker
        for (argn = 2; zmsg && argn < argc; argn++) {
            if (strcmp (zmsg_address (zmsg), argv [argn]) == 0)
                zmsg_send (&zmsg, cloudfe);
        }
        //  Route reply to client if we still need to
        if (zmsg)
            zmsg_send (&zmsg, localfe);

        //  Now route as many clients requests as we can handle
        //
        while (capacity) {
            zmq_pollitem_t frontends [] = {
                { localfe, 0, ZMQ_POLLIN, 0 },
                { cloudfe, 0, ZMQ_POLLIN, 0 }
            };
            rc = zmq_poll (frontends, 2, 0);
            assert (rc >= 0);
            int reroutable = 0;
            //  We'll do peer brokers first, to prevent starvation
            if (frontends [1].revents & ZMQ_POLLIN) {
                zmsg = zmsg_recv (cloudfe);
                reroutable = 0;
            }
            else
            if (frontends [0].revents & ZMQ_POLLIN) {
                zmsg = zmsg_recv (localfe);
                reroutable = 1;
            }
            else
                break;      //  No work, go back to backends

            //  If reroutable, send to cloud 20% of the time
            //  Here we'd normally use cloud status information
            //
            if (reroutable && argc > 2 && randof (5) == 0) {
                //  Route to random broker peer
                int random_peer = randof (argc - 2) + 2;
                zmsg_wrap (zmsg, argv [random_peer], NULL);
                zmsg_send (&zmsg, cloudbe);
            }
            else {
                zmsg_wrap (zmsg, worker_queue [0], "");
                zmsg_send (&zmsg, localbe);

                //  Dequeue and drop the next worker address
                free (worker_queue [0]);
                DEQUEUE (worker_queue);
                capacity--;
            }
        }
    }
    //  We never get here but clean up anyhow
    zmq_close (localbe);
    zmq_close (cloudbe);
    zmq_term (context);
    return EXIT_SUCCESS;
}
[[/code]]
[[>]]
examples/C/peering2.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/peering2.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/peering2.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/peering2.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/peering2.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/peering2.cs C#]//
//[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/peering2.lisp Common Lisp]//
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/peering2.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/peering2.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/peering2.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/peering2.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/peering2.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/peering2.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/peering2.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/peering2.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/peering2.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/peering2.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/peering2.rb Ruby]//
[[/collapsible]]
[[/>]]

Run this by, for instance, starting two instance of the broker in two windows:

[[code]]
peering2 me you
peering2 you me
[[/code]]

Some comments on this code:

* Using the zmsg class makes life much easier, and our code much shorter. It's obviously a abstraction that works, and which should form part of your toolbox as a ØMQ programmer.

* Since we're not getting any state information from peers, we naively assume they are running. The code prompts you to confirm when you've started all the brokers. In the real case we'd not send anything to brokers who had not told us they exist.

You can satisfy yourself that the code works by watching it run forever. If there were any misrouted messages, clients would end up blocking, and the brokers would stop printing trace information. You can prove that by killing either of the brokers. The other broker tries to send requests to the cloud, and one by one its clients block, waiting for an answer.

[[div style="overflow:hidden"]]
[[div style="float:left"]]
++++ Putting it All Together
[[/div]]
[[div style="float:right; vertical-align:middle"]]
[/chapter:all# ↑top]
[[/div]]
[[/div]]

Let's put this together into a single package. As before, we'll run an entire cluster as one process. We're going to take the two previous examples and merge them into one properly working design that lets you simulate any number of clusters.

This code is the size of both previous prototypes together, at 270 LoC. That's pretty good for a simulation of a cluster that includes clients and workers and cloud workload distribution. Here is the code:

[[code type="C" title="Full cluster simulation" name="peering3"]]
//
//  Broker peering simulation (part 3)
//  Prototypes the full flow of status and tasks
//
//  Changes for 2.1:
//  - added version assertion
//  - use separate contexts for each thread
//  - use ipc:// instead of inproc://
//  - close sockets in each child thread
//  - call zmq_term in each thread before ending
//
#include "zhelpers.h"
#include "zmsg.c"

#define NBR_CLIENTS 10
#define NBR_WORKERS 5

//  A simple dequeue operation for queue implemented as array
#define DEQUEUE(q) memmove (&(q)[0], &(q)[1], sizeof (q) - sizeof (q [0]))

//  Request-reply client using REQ socket
//  To simulate load, clients issue a burst of requests and then
//  sleep for a random period.
//
static void *
client_thread (void *args) {
    int rc;
    void *context = zmq_init (1);
    void *client = zmq_socket (context, ZMQ_REQ);
    rc = zmq_connect (client, "ipc://localfe.ipc");
    assert (rc == 0);
    void *monitor = zmq_socket (context, ZMQ_PUSH);
    rc = zmq_connect (monitor, "ipc://monitor.ipc");
    assert (rc == 0);

    zmsg_t *zmsg = zmsg_new ();
    while (1) {
        sleep (randof (5));

        int burst = randof (15);
        while (burst--) {
            //  Send request with random hex ID
            char task_id [5];
            sprintf (task_id, "%04X", randof (0x10000));
            zmsg_body_set (zmsg, task_id);
            zmsg_send (&zmsg, client);

            //  Wait max ten seconds for a reply, then complain
            zmq_pollitem_t pollset [1] = {
                { client, 0, ZMQ_POLLIN, 0 }
            };
            rc = zmq_poll (pollset, 1, 10 * 1000000);
            assert (rc >= 0);
            if (pollset [0].revents & ZMQ_POLLIN) {
                zmsg = zmsg_recv (client);
                //  Worker is supposed to answer us with our task id
                assert (strcmp (zmsg_body (zmsg), task_id) == 0);
            }
            else {
                zmsg = zmsg_new ();
                zmsg_body_fmt (zmsg,
                    "E: CLIENT EXIT - lost task %s", task_id);
                zmsg_send (&zmsg, monitor);
                return (NULL);
            }
        }
    }
    //  We never get here but if we did, this is how we'd exit cleanly
    zmq_close (client);
    zmq_close (monitor);
    zmq_term (context);
    return (NULL);
}

//  Worker using REQ socket to do LRU routing
//
static void *
worker_thread (void *args) {
    void *context = zmq_init (1);
    void *worker = zmq_socket (context, ZMQ_REQ);
    int rc = zmq_connect (worker, "ipc://localbe.ipc");
    assert (rc == 0);

    //  Tell broker we're ready for work
    zmsg_t *zmsg = zmsg_new ();
    zmsg_body_set (zmsg, "READY");
    zmsg_send (&zmsg, worker);

    while (1) {
        //  Workers are busy for 0/1/2 seconds
        zmsg = zmsg_recv (worker);
        sleep (randof (2));
        zmsg_send (&zmsg, worker);
    }
    //  We never get here but if we did, this is how we'd exit cleanly
    zmq_close (worker);
    zmq_term (context);
    return (NULL);
}

int main (int argc, char *argv [])
{
    //  First argument is this broker's name
    //  Other arguments are our peers' names
    //
    s_version_assert (2, 1);
    if (argc < 2) {
        printf ("syntax: peering3 me {you}...\n");
        exit (EXIT_FAILURE);
    }
    char *self = argv [1];
    printf ("I: preparing broker at %s...\n", self);
    srandom ((unsigned) time (NULL));

    //  Prepare our context and sockets
    void *context = zmq_init (1);
    char endpoint [256];

    //  Bind cloud frontend to endpoint
    void *cloudfe = zmq_socket (context, ZMQ_XREP);
    snprintf (endpoint, 255, "ipc://%s-cloud.ipc", self);
    zmq_setsockopt (cloudfe, ZMQ_IDENTITY, self, strlen (self));
    int rc = zmq_bind (cloudfe, endpoint);
    assert (rc == 0);

    //  Bind state backend / publisher to endpoint
    void *statebe = zmq_socket (context, ZMQ_PUB);
    snprintf (endpoint, 255, "ipc://%s-state.ipc", self);
    rc = zmq_bind (statebe, endpoint);
    assert (rc == 0);

    //  Connect cloud backend to all peers
    void *cloudbe = zmq_socket (context, ZMQ_XREP);
    zmq_setsockopt (cloudbe, ZMQ_IDENTITY, self, strlen (self));

    int argn;
    for (argn = 2; argn < argc; argn++) {
        char *peer = argv [argn];
        printf ("I: connecting to cloud frontend at '%s'\n", peer);
        snprintf (endpoint, 255, "ipc://%s-cloud.ipc", peer);
        rc = zmq_connect (cloudbe, endpoint);
        assert (rc == 0);
    }

    //  Connect statefe to all peers
    void *statefe = zmq_socket (context, ZMQ_SUB);
    zmq_setsockopt (statefe, ZMQ_SUBSCRIBE, "", 0);

    for (argn = 2; argn < argc; argn++) {
        char *peer = argv [argn];
        printf ("I: connecting to state backend at '%s'\n", peer);
        snprintf (endpoint, 255, "ipc://%s-state.ipc", peer);
        rc = zmq_connect (statefe, endpoint);
        assert (rc == 0);
    }
    //  Prepare local frontend and backend
    void *localfe = zmq_socket (context, ZMQ_XREP);
    zmq_bind (localfe, "ipc://localfe.ipc");
    void *localbe = zmq_socket (context, ZMQ_XREP);
    zmq_bind (localbe, "ipc://localbe.ipc");

    //  Prepare monitor socket
    void *monitor = zmq_socket (context, ZMQ_PULL);
    zmq_bind (monitor, "ipc://monitor.ipc");

    //  Start local workers
    int worker_nbr;
    for (worker_nbr = 0; worker_nbr < NBR_WORKERS; worker_nbr++) {
        pthread_t worker;
        pthread_create (&worker, NULL, worker_thread, NULL);
    }
    //  Start local clients
    int client_nbr;
    for (client_nbr = 0; client_nbr < NBR_CLIENTS; client_nbr++) {
        pthread_t client;
        pthread_create (&client, NULL, client_thread, NULL);
    }

    //  Interesting part
    //  -------------------------------------------------------------
    //  Publish-subscribe flow
    //  - Poll statefe and process capacity updates
    //  - Each time capacity changes, broadcast new value
    //  Request-reply flow
    //  - Poll primary and process local/cloud replies
    //  - While worker available, route localfe to local or cloud

    //  Queue of available workers
    int local_capacity = 0;
    int cloud_capacity = 0;
    char *worker_queue [10];

    while (1) {
        zmq_pollitem_t primary [] = {
            { localbe, 0, ZMQ_POLLIN, 0 },
            { cloudbe, 0, ZMQ_POLLIN, 0 },
            { statefe, 0, ZMQ_POLLIN, 0 },
            { monitor, 0, ZMQ_POLLIN, 0 }
        };
        //  If we have no workers anyhow, wait indefinitely
        rc = zmq_poll (primary, 4, local_capacity? 1000000: -1);
        assert (rc >= 0);

        //  Track if capacity changes during this iteration
        int previous = local_capacity;

        //  Handle reply from local worker
        zmsg_t *zmsg = NULL;

        if (primary [0].revents & ZMQ_POLLIN) {
            assert (local_capacity < NBR_WORKERS);
            //  Use worker address for LRU routing
            zmsg = zmsg_recv (localbe);
            worker_queue [local_capacity++] = zmsg_unwrap (zmsg);
            if (strcmp (zmsg_address (zmsg), "READY") == 0)
                zmsg_destroy (&zmsg);   //  Don't route it
        }
        //  Or handle reply from peer broker
        else
        if (primary [1].revents & ZMQ_POLLIN) {
            zmsg = zmsg_recv (cloudbe);
            //  We don't use peer broker address for anything
            free (zmsg_unwrap (zmsg));
        }
        //  Route reply to cloud if it's addressed to a broker
        for (argn = 2; zmsg && argn < argc; argn++) {
            if (strcmp (zmsg_address (zmsg), argv [argn]) == 0)
                zmsg_send (&zmsg, cloudfe);
        }
        //  Route reply to client if we still need to
        if (zmsg)
            zmsg_send (&zmsg, localfe);

        //  Handle capacity updates
        if (primary [2].revents & ZMQ_POLLIN) {
            zmsg = zmsg_recv (statefe);
            cloud_capacity = atoi (zmsg_body (zmsg));
            zmsg_destroy (&zmsg);
        }
        //  Handle monitor message
        if (primary [3].revents & ZMQ_POLLIN) {
            zmsg_t *zmsg = zmsg_recv (monitor);
            printf ("%s\n", zmsg_body (zmsg));
            zmsg_destroy (&zmsg);
        }

        //  Now route as many clients requests as we can handle
        //  - If we have local capacity we poll both localfe and cloudfe
        //  - If we have cloud capacity only, we poll just localfe
        //  - Route any request locally if we can, else to cloud
        //
        while (local_capacity + cloud_capacity) {
            zmq_pollitem_t secondary [] = {
                { localfe, 0, ZMQ_POLLIN, 0 },
                { cloudfe, 0, ZMQ_POLLIN, 0 }
            };
            if (local_capacity)
                rc = zmq_poll (secondary, 2, 0);
            else
                rc = zmq_poll (secondary, 1, 0);
            assert (rc >= 0);

            if (secondary [0].revents & ZMQ_POLLIN)
                zmsg = zmsg_recv (localfe);
            else
            if (secondary [1].revents & ZMQ_POLLIN)
                zmsg = zmsg_recv (cloudfe);
            else
                break;      //  No work, go back to primary

            if (local_capacity) {
                zmsg_wrap (zmsg, worker_queue [0], "");
                zmsg_send (&zmsg, localbe);

                //  Dequeue and drop the next worker address
                free (worker_queue [0]);
                DEQUEUE (worker_queue);
                local_capacity--;
            }
            else {
                //  Route to random broker peer
                printf ("I: route request %s to cloud...\n",
                    zmsg_body (zmsg));
                int random_peer = randof (argc - 2) + 2;
                zmsg_wrap (zmsg, argv [random_peer], NULL);
                zmsg_send (&zmsg, cloudbe);
            }
        }
        if (local_capacity != previous) {
            //  Broadcast new capacity
            zmsg_t *zmsg = zmsg_new ();
            zmsg_body_fmt (zmsg, "%d", local_capacity);
            //  We stick our own address onto the envelope
            zmsg_wrap (zmsg, self, NULL);
            zmsg_send (&zmsg, statebe);
        }
    }
    //  We never get here but clean up anyhow
    zmq_close (localbe);
    zmq_close (cloudbe);
    zmq_close (statefe);
    zmq_close (monitor);
    zmq_term (context);
    return EXIT_SUCCESS;
}
[[/code]]
[[>]]
examples/C/peering3.c
[[collapsible show="All languages" hide="Hide languages"]]
//[*http://github.com/imatix/zguide/blob/master/examples/Ada/peering3.ada Ada]//
//[*http://github.com/imatix/zguide/blob/master/examples/Basic/peering3.bas Basic]//
**[*http://github.com/imatix/zguide/blob/master/examples/C/peering3.c C]**
//[*http://github.com/imatix/zguide/blob/master/examples/C%2B%2B/peering3.cpp C++]//
//[*http://github.com/imatix/zguide/blob/master/examples/C%23/peering3.cs C#]//
//[*http://github.com/imatix/zguide/blob/master/examples/Common%20Lisp/peering3.lisp Common Lisp]//
//[*http://github.com/imatix/zguide/blob/master/examples/Erlang/peering3.erl Erlang]//
//[*http://github.com/imatix/zguide/blob/master/examples/Go/peering3.go Go]//
//[*http://github.com/imatix/zguide/blob/master/examples/Haskell/peering3.hs Haskell]//
//[*http://github.com/imatix/zguide/blob/master/examples/Java/peering3.java Java]//
//[*http://github.com/imatix/zguide/blob/master/examples/Lua/peering3.lua Lua]//
//[*http://github.com/imatix/zguide/blob/master/examples/Objective-C/peering3.m Objective-C]//
//[*http://github.com/imatix/zguide/blob/master/examples/ooc/peering3.ooc ooc]//
//[*http://github.com/imatix/zguide/blob/master/examples/Perl/peering3.pl Perl]//
**[*http://github.com/imatix/zguide/blob/master/examples/PHP/peering3.php PHP]**
//[*http://github.com/imatix/zguide/blob/master/examples/Python/peering3.py Python]//
//[*http://github.com/imatix/zguide/blob/master/examples/Ruby/peering3.rb Ruby]//
[[/collapsible]]
[[/>]]

It's a non-trivial program and took about a day to get working. These are the highlights:

* The client threads detect and report a failed request. They do this by polling for a response and if none arrives after a while (10 seconds), printing an error message.

* Client threads don't print directly, but instead send a message to a 'monitor' socket (PUSH) that the main loop collects (PULL) and prints off. This is the first case we've seen of using ØMQ sockets for monitoring and logging; this is a big use case we'll come back to later.

* Clients simulate varying loads to get the cluster 100% at random moments, so that tasks are shifted over to the cloud. The number of clients and workers, and delays in the client and worker threads control this. Feel free to play with them to see if you can make a more realistic simulation.

* The main loop uses two pollsets. It could in fact use three: information, backends, and frontends. As in the earlier prototype, there is no point in taking a frontend message if there is no backend capacity.

These are some of the problems that hit during development of this program:

* Clients would freeze, due to requests or replies getting lost somewhere. Recall that the ØMQ XREP/router socket drops messages it can't route. The first tactic here was to modify the client thread to detect and report such problems. Secondly, I put zmsg_dump() calls after every recv() and before every send() in the main loop, until it was clear what the problems were.

* The main loop was mistakenly reading from more than one ready socket. This caused the first message to be lost. Fixed that by reading only from the first ready socket.

* The zmsg class was not properly encoding UUIDs as C strings. This caused UUIDs that contain 0 bytes to be corrupted. Fixed by modifying zmsg to encode UUIDs as printable hex strings.

This simulation does not detect disappearance of a cloud peer. If you start several peers and stop one, and it was broadcasting capacity to the others, they will continue to send it work even if it's gone. You can try this, and you will get clients that complain of lost requests. The solution is twofold: first, only keep the capacity information for a short time so that if a peer does disappear, its capacity is quickly set to 'zero'. Second, add reliability to the request-reply chain. We'll look at reliability in the next chapter.
