.set SELF=chapter:3
.output chapter3.wd
++ Chapter Three - Advanced Request-Reply

In Chapter Two we worked through the basics of using 0MQ by developing a series of small applications, each time exploring new aspects of 0MQ.  We'll continue this approach in this chapter, as we explore advanced aspects of 0MQ.

We'll cover:

* Transient vs durable sockets, identities, and queues.
* How to do make durable subscribers that can recover from crashes.
* Using the high-water mark (HWM) to protect against memory overflows.
* How to create and use message envelopes for request-reply.
* How the request-reply pattern works, in detail, including XREQ and XREP.
* Automatic vs. manual reply addresses (using identities).
* How to do custom routing instead of 0MQ's load-balancing.

+++ Transient vs. Durable Sockets

The concept of a durable socket is one more of those surprisingly obvious 0MQ inventions that when you see, you wonder why no-one ever thought of it before.  In classic networking, sockets are API objects, and their lifespan is never longer than the code that uses them.  But if you look at a socket you see that it collects a bunch of resources - network buffers - and at some stage, a 0MQ user asked, "isn't there some way these could hang around if my program crashes, so I can get them back?"

This turns out to be very useful.  It's not foolproof, but it gives 0MQ a kind of "better than a kick in the nuts" reliability, particularly for pubsub cases.  We'll look at that shortly.

Here is the general model of two sockets happily chatting about the weather, and who kissed who and where and when precisely, cause I heard something different, at the last staff party, not to mention did you see that new family up the road who do they think they are with that car and what's with the prices at the shops these days don't they know it's a crisis?

[[code type="textdiagram"]]
        +-----------+
        |           |
        |  Sender   |
        |           |
        +-----------+
        |  Socket   |
        \-----------/
         +---------+
         +---------+
         +---------+    0MQ Transmit buffer
         +---------+
         +---------+
              |
              |
              v
         +---------+
         +---------+    Network I/O buffers
         +---------+
              |
              |
              v
         +---------+
         +---------+
         +---------+    0MQ Receive buffer
         +---------+
         +---------+
        /-----------\
        |  Socket   |
        +-----------+
        |           |
        | Receiver  |
        |           |
        +-----------+


Figure # - Sender boring the pants of receiver
[[/code]]

What durable sockets give you is the promise that the 0MQ //transmit buffer// is kept alive as long as the sender exists.  If the receiver crashes, it will lose its receive buffer, and the network will lose its I/O buffers, but the sender can continue to push data into its transmit buffer and the receiver can then pick this up.

Note that 0MQ's transmit and receive buffers are invisible and automatic, just like TCP's buffers are.

All the sockets we've used so far were transient.  To turn a transient socket into a durable one you give it an explicit //identity//.  All 0MQ sockets have identities but by default they are generated 'unique universal identifiers' (UUIDs) that the peer uses to recall who it's talking to.

Behind the scenes, and invisibly to you, when one socket connects to another, the two sockets exchange identities.  Normally sockets don't tell their peers their identity, so peers invent random identities for each other:

[[code type="textdiagram"]]
        +-----------+
        |           |
        | Receiver  |
        |           |
        +-----------+
        |  Socket   |
        \-----------/
              |  "Not telling you my name!"
              |
              |
              v  "Fine, I'll call you Luv"
        /-----------\
        |  Socket   |
        +-----------+
        |           |
        |  Sender   |
        |           |
        +-----------+


         Figure # - Transient socket
[[/code]]

But a socket can also tell the other its identity, and then the next time the two meet, it'll be "so as I was saying what I heard was quite different but anyhow you know how it goes at the office, they're all tattletales I'd never say anything about anyone that wasn't true or at least based on a sure thing".

[[code type="textdiagram"]]
        +-----------+
        |           |
        | Receiver  |
        |           |
        +-----------+
        |  Socket   |
        \-----------/
              |  "My name's Lucy"
              |
              |
              v  "Lucy! Nice to see you again..."
        /-----------\
        |  Socket   |
        +-----------+
        |           |
        |  Sender   |
        |           |
        +-----------+


       Figure # - Durable socket aka. Can't Escape
[[/code]]

Here's how you set the identity of a socket, to create a durable socket:

[[code]]
zmq_setsockopt (socket, ZMQ_IDENTITY, "Lucy", 4);
[[/code]]

Some comments on setting a socket identity:

* If you want to set an identity you must do it //before// connecting or binding the socket.
* Identities are binary strings: identities starting with a zero byte are reserved for 0MQ use.
* Do not use the same identity for more than one socket.  On 0MQ/2.0.9 and earlier versions this will cause an assertion failure in the //other// socket.  Yes, that's a bug, and yes, it'll be fixed.
* Do not modify the identity of a socket after binding, or if you restart a peer, this will cause an assertion failure in sockets connected to it.  Yes, that's also a bug and yes, it'll also be fixed.
* Do not use random identities in applications that create lots of sockets.  What this will do is cause lots and lots of durable sockets to pile up, eventually crashing nodes.

See zmq_setsockopt[3] for a summary of the ZMQ_IDENTITY socket option.

About these assertion failures, 0MQ shouldn't assert when it's misused, only when it thinks it's actually hitting an internal bug.  Code should be full of assertions that //never// fail.  There are ongoing discussions about how these situations should be handled.  As a general rule, if you get assertions while using 0MQ, report them to the zeromq-dev list.

+++ Making a (Semi-)Durable Subscriber

Identities work on all socket types.  If you have a PUB and a SUB socket, and the subscriber gives the publisher its identity, the publisher holds onto data until it can deliver it to the subscriber.

This is both wonderful and terrible at the same time.  It's wonderful because it means updates can wait for you in the publisher's transmit buffer, until you connect and collect them.  It's terrible because by default this will rapidly kill a publisher and lock up your system.

**If you use durable subscriber sockets (i.e. if you set the identity on a SUB socket) you //must// also guard against queue explosion by using the //high water mark// or HWM, on the publisher socket.**

If you want to prove this, take the wuclient and wuserver from Chapter 1, and add this line to the wuclient before it connects:

[[code]]
    zmq_setsockopt (subscriber, ZMQ_IDENTITY, "Hello", 5);
[[/code]]

Build and run the two programs.  It all looks normal.  But keep an eye on the memory used by the publisher, and you'll see that as the subscriber finishes, the publisher memory grows and grows.  If you restart the subscriber, the publisher queues stop growing.  As soon as the subscriber goes away, they grow again.  It'll rapidly overwhelm your system.

We'll first look at how to do this, and then at how to do it properly.  Here are a publisher and subscriber that use the 'node coordination' technique from Chapter 2 to synchronize.  The publisher then sends ten messages, waiting a second between each one.  That wait is for you to kill the subscriber using Ctrl-C, wait a few seconds, and restart it.

Here's the publisher:

[[code type="C" title="Durable publisher" name="durapub"]]
[[/code]]

And here's the subscriber:

[[code type="C" title="Durable subscriber" name="durasub"]]
[[/code]]

To run this, start the publisher, then the subscriber, each in their own window.  Allow the subscriber to collect one or two messages, then Ctrl-C it.  Count to three, and restart it.  What you will see is something like this:

[[code]]
$ durasub
Update 0
Update 1
Update 2
^C
$ durasub
Update 3
Update 4
Update 5
Update 6
Update 7
^C
$ durasub
Update 8
Update 9
END
[[/code]]

Just to see the difference, comment out the line in the subscriber that sets the socket identity, and try again.  You will see that it loses messages.  Setting an identity turns a transient subscriber into a durable subscriber.  You would in practice want to choose identities carefully, either taking them from configuration files, or generating UUIDs and storing them somewhere.

When we set a high-water mark on the PUB socket, the publisher stores that many messages, but no more.  Let's test this by setting the publisher HWM to 2 messages, before we start publishing to the socket:

[[code]]
uint64_t hwm = 2;
zmq_setsockopt (publisher, ZMQ_HWM, &hwm, sizeof (hwm));
[[/code]]

Now running our test, killing and restarting the subscriber after a couple of seconds' pause will show something like this:

[[code]]
$ durasub
Update 0
Update 1
^C
$ durasub
Update 2
Update 3
Update 7
Update 8
Update 9
END
[[/code]]

Look carefully: we have two messages kept for us (2 and 3), then a gap of several messages, and then new updates again.  The HWM causes 0MQ to drop messages it can't put onto the queue, something the 0MQ Reference Manual calls an "exceptional condition".

In short, if you use subscriber identities, you must set the high-water mark on publisher sockets, or else you risk servers that run out of memory and crash.  However, there is a way out.  0MQ provides something called a "swap", which is a disk file that holds messages we can't store to the queue.  It is very simple to enable:

[[code]]
//  Specify swap space in bytes
uint64_t swap = 25000000;
zmq_setsockopt (publisher, ZMQ_SWAP, &swap, sizeof (swap));
[[/code]]

We can put this together to make a cynical publisher that is immune to slow, blocked, or absent subscribers while still offering durable subscriptions to those that need it:

[[code type="C" title="Durable but cynical publisher" name="durapub2"]]
[[/code]]

In practice, setting the HWM to 1 and shoving everything to disk will make a pubsub system very slow.  Here is a more reasonable 'best practice' for publishers that have to deal with unknown subscribers:

* Always set a HWM on the socket, based on the expected maximum number of subscribers, the amount of memory you are willing to dedicated to queuing, and the average size of a message.  For example if you expect up to 5,000 subscribers, and have 1GB of memory to play with, and messages of ~200 bytes, then a safe HWM would be (1,000,000,000 / 200 / 5,000) = 1,000.
* If you don't want slow or crashing subscribers to lose data, set a SWAP that's large enough to handle the peaks, based on the number of subscribers, peak message rate, average size of messages, and time you want to cover.  For example with 5,000 subscribers and messages of ~200 bytes coming in at 100,000 per second, you will need up to 100MB of disk space per second.  To cover an outage of up to 1 minute, therefore, you'd need 6GB of disk space, and it would have to be fast, but that's a different story.

Some notes on durable subscribers:

* Depending on how the subscriber dies, and the frequency of updates, and the size of network buffers, and the transport protocol you are using, data may be lost.  Durable subscribers will have //much better// reliability than transient ones, but they will not be perfect.

* The SWAP file is not recoverable, so if a publisher dies and restarts, it will lose data that was in its transmit buffers, and that was in the network I/O buffers.

Some notes on using the HWM option:

* This affects both the transmit and receive buffers of a single socket.  Some sockets (PUB, PUSH) only have transmit buffers.  Some (SUB, PULL, REQ, REP) only have receive buffers.  Some (XREQ, XREP, PAIR) have both transmit and receive buffers.

* Future versions of 0MQ might offer high-water marks on transmit and receive buffers separately.  However this isn't something people seem particularly worried about.

* When your socket reaches its high-water mark, it will either block or drop data depending on the socket type.  PUB sockets will drop data if they reach their high-water mark, while other socket types will block.

+++ Request-Reply Envelopes

We looked briefly at multipart messages in Chapter Two.  Let's now look at what we use them most often, which is to create //message envelopes//.  An envelope is a way of safely packaging up data with an address, without touching the data itself.

In the request-reply pattern, the envelope holds the return address for replies.  It is how a 0MQ network with no state can create round-trip request-reply dialogs.

You don't in fact need to understand how request-reply envelopes work to use them for common cases.  When you use REQ and REP, your sockets build and use envelopes automatically.  When you write a device, and we covered this in the last chapter, you just need to read and write all the parts of a message.  0MQ implements envelopes using multipart data, so if you copy multipart data safely, you implicitly copy envelopes too.

However, getting under the hood and playing with request-reply envelopes is necessary for advanced request-reply work.  It's time to explain how XREP works, in terms of envelopes:

* When you receive a message from an XREP socket, it shoves a brown paper envelope around the message and scribbles on with indelible ink, "This came from Lucy".  Then it gives that to you.  That is, the XREP socket gives you what came off the wire, wrapped up in an envelope with the reply address on it.

* when you send a message to an XREP socket, it rips off that brown paper envelope, tries to read its own handwriting, and if it knows who "Lucy" is, sends the contents back to Lucy.  That is the reverse process of receiving a message.

If you leave the brown envelope alone, and then pass that message to another XREP socket (e.g. by sending to an XREQ connected to an XREP), the second XREP socket will in turn stick another brown envelope on it, and scribble the name of that XREQ on it.

The whole point of this is that each XREP knows how to send replies back to the right place.  All you need to do, in your application, is respect the brown envelopes.  Now the REP socket makes sense.  It carefully slices open the brown envelopes, one by one, keeps them safely aside, and gives you (the application code that owns the REP socket) the original message.  When you send the reply, it re-wraps the reply in the brown paper envelopes, so it can hand the resulting brown package back to the XREP sockets down the chain.

Which lets you insert XREP-XREQ devices into a request-reply pattern like this:

[[code]]
[REQ] <--> [REP]
[REQ] <--> [XREP--XREQ] <--> [REP]
[REQ] <--> [XREP--XREQ] <--> [XREP--XREQ] <--> [REP]
...etc.
[[/code]]

If you connect a REQ socket to an XREP socket, and send one request message, this is what you get when you receive from the XREP socket:

[[code type="textdiagram"]]
            +---------------+
  Frame 1   | Reply address |   <----- Envelope
            +---+-----------+
  Frame 2   |   |   <------ Empty message part
            +---+-------------------------------------+
  Frame 3   |     Data                                |
            +-----------------------------------------+


       Figure # - Single-hop request-reply envelope
[[/code]]

Breaking this down:

* The data in frame 3 is what the sending application sends to the REQ socket.
* The empty message part in frame 2 is prepended by the REQ socket when it sends the message to the XREP socket.
* The reply address in frame 1 is prepended by the XREP before it passes the message to the receiving application.

Now if we extend this with a chain of devices, we get envelope on envelope, with the newest envelope always stuck at the beginning of the stack:

[[code type="textdiagram"]]

       (Next envelope will go here)

            +---------------+
  Frame 1   | Reply address |   <----- Envelope (XREP)
            +---------------+
  Frame 2   | Reply address |   <----- Envelope (XREP)
            +---------------+
  Frame 3   | Reply address |   <----- Envelope (XREP)
            +---+-----------+
  Frame 4   |   |   <------ Empty message part (REQ)
            +---+-------------------------------------+
  Frame 5   |     Data                                |
            +-----------------------------------------+


       Figure # - Multihop request-reply envelope
[[/code]]

Here now is a more detailed explanation of the four socket types we use for request-reply patterns:

* XREQ just load-balances the messages you send to all connected peers, and fair-queues the messages it receives.  It is exactly like a PUSH and PULL socket combined.

* REQ prepends an empty message part to every message you send, and removes the empty message part from each message you receive.  It then works like XREQ (and in fact is built on XREQ) except it also imposes a strict send / receive cycle.

* XREP prepends an envelope with reply address to each message it receives, before passing it to the application.  It also chops off the envelope (the first message part) from each message it sends, and uses that reply address to decide which peer the message should go to.

* REP stores all the message parts up to the first empty message part, when you receive a message and it passes the rest (the data) to your application.  When you send a reply, REP prepends the saved envelopes to the message and sends it back using the same semantics as XREP (and in fact REP is built on top of XREP), but matching REQ, imposes a strict receive / send cycle.

REP requires that the envelopes end with an empty message part.  If you're not using REQ at the other end of the chain then you must add the empty message part yourself.

So the obvious question about XREP is, where does it get the reply addresses from? And the obvious answer is, it uses the socket's identity.  As we already learned, a socket can be transient in which case the //other// socket (XREP in this case) generates an identity that it can associate with the socket.  Or, the socket can be durable in which case it explicitly tells the other socket (XREP, again) its identity and XREP can use that rather than generating a temporary label.

This is what it looks like for transient sockets:

[[code type="textdiagram"]]
        +-----------+
        |           |
        |   Client  |
        |           |
        +-----------+       +---------+
        |    REQ    |       |  Data   |     Client sends this
        \-----------/       +---------+
              |
              |  "My identity is empty"
              v
        /-----------\       +---------+
        |   XREP    |       |  UUID   |     XREP invents UUID to
        +-----------+       +-+-------+     use as reply address
        |           |       | |
        |  Service  |       +-+-------+
        |           |       |  Data   |
        +-----------+       +---------+


          Figure # - XREP invents a UUID for transient sockets
[[/code]]

This is what it looks like for durable sockets:

[[code type="textdiagram"]]
        +-----------+
        |           |       zmq_setsockopt (socket,
        |   Client  |           ZMQ_IDENTITY, "Lucy", 4);
        |           |
        +-----------+       +---------+
        |    REQ    |       |  Data   |     Client sends this
        \-----------/       +---------+
              |
              |  "Hi, my name is Lucy"
              v
        /-----------\       +---------+
        |   XREP    |       | 'Lucy'  |     XREP uses identity of
        +-----------+       +-+-------+     client as reply address
        |           |       | |
        |  Service  |       +-+-------+
        |           |       |  Data   |
        +-----------+       +---------+


           Figure # - XREP uses identity if it knows it
[[/code]]

Let's observe the above two cases in practice.  This program dumps the contents of the message parts that an XREP socket receives from two REP sockets, one not using identities, and one using an identity 'Hello':

[[code type="C" title="Identity check" name="identity"]]
[[/code]]

Here is what the dump function prints:

[[code]]
----------------------------------------
[017] 00314F043F46C441E28DD0AC54BE8DA727
[000]
[026] XREP uses a generated UUID
----------------------------------------
[005] Hello
[000]
[038] XREP socket uses REQ's socket identity
[[/code]]

+++ Custom Request-Reply Routing

We already saw that XREP uses the message envelope to decide which client to route a reply back to.  Now let me express that in another way: //XREP will route messages asynchronously to any peer connected to it, if you provide the correct routing address via a properly constructed envelope.//

So XREP is really a fully controllable router.  Let's look at this magic in detail.  But first, let's fix the parsing pain we feel when we try to distinguish "REP", "REQ", "XREP", and "XREQ" from each other.  There should be a law against names that are so similar. :-)

For readability, and because we're going to go off-road into some rough and possibly illegal terrain now, let's rename these four socket types just for this section of the text:

* REQ is a **mama** socket, doesn't listen but always expects an answer.
* REP is a **papa** socket, always answers, but never starts a conversation.
* XREQ is a **dealer** socket, shuffling messages to and fro.
* XREP is a **router** socket, able to route messages to specific peers.

The thing about Mama sockets is, as we all learned as kids, you can't speak until spoken to.  Mamas do not have simple open-mindedness of papas, nor the ambiguous "sure, whatever" shrugged-shoulder aloofness of a dealer.  So to speak to a mama socket, you have to get the mama socket to talk to you first.  The good part is mamas don't care if you reply now, or much later.  Just bring a good sob story and a bag of laundry.

Papa sockets on the other hand are strong and silent, and pedantic.  They do just one thing, which is to give you an answer to whatever you ask, perfectly framed and precise.  Don't expect a papa socket to be chatty, or to pass a message on to someone else, this is just not going to happen.

While we usually think of request-reply as a to-and-fro pattern, in fact it can be fully asynchronous, as long as we understand that any mamas or papas will be at the end of a chain, never in the middle of it, and always synchronous.  All we need to know is the address of the peer we want to talk to, and then we can then send it messages asynchronously, via a router.  The router is the one and only 0MQ socket type capable of being told "send this message to X" where X is the address of a connected peer.

These are the ways we can know the address to send a message to, and you'll see most of these used in the examples of custom request-reply routing:

* If it's an anonymous peer, i.e. did not set any identity, the router will generate a UUID and use that to refer to the connection when it delivers you an incoming request envelope.
* If it is a peer with explicit identity, the router will give that identity when it delivers you an incoming request envelope.
* Peers with explicit identities can send them via some other mechanism, e.g. via some other sockets.
* Peers can have prior knowledge of each others' identities, e.g. via configuration files or some other magic.

There are three custom routing patterns, one for each of the socket types we can connect to a router:

* Router-to-dealer, also called XREP-to-XREQ.
* Router-to-mama, aka XREP-to-REQ.
* Router-to-papa, aka XREP-to-REP.

You might be tempted to try to connect a router to a router but since request-reply is essentially asymmetric, that really makes no sense.  In the advanced cases we'll see we do router-to-router routing by creating two asymmetric connections rather than one bidirectional connection.

In each of these cases we have total control over how we route messages, but the different patterns cover different use cases and message flows.  Let's break it down over the next sections with examples of different routing algorithms.

But first some warnings about custom routing:

* This goes against a fairly solid 0MQ rule: delegate peer addressing to the socket.  The only reason we do it is because 0MQ lacks a wide range of routing algorithms.
* Future versions of 0MQ will probably do some of the routing we're going to build here.  That means the code we design now may break, or become redundant in the future.
* While the built-in routing has certain guarantees of scalability, such as being friendly to devices, custom routing doesn't.  You will need to make your own devices.

So overall, custom routing is more expensive and more fragile than delegating this to 0MQ.  Only do it if you need it.  Having said that, let's jump in, the water's great!

+++ Random Scatter Routing

The router-to-dealer pattern is the simplest.  You connect one router to many dealers, and then distribute messages to the dealers using any algorithm you like.  The dealers can be sinks (process the messages without any response), proxies (send the messages on to other nodes), or services (send back replies).

If you expect the dealer to reply, there should only be one router talking to it.  Dealers have no idea how to reply to a specific peer, so if they have multiple peers, they will load-balance between them, which would be weird.  If the dealer is a sink, any number of routers can talk to it.

What kind of routing can you do with a router-to-dealer pattern?  If the dealers talk back to the router, e.g. telling the router when they finished a task, you can use that knowledge to route depending on how fast a dealer is.  Since both router and dealer are asynchronous, it can get a little tricky.  You'd need to use zmq_poll[3] at least.

We'll make an example where the dealers don't talk back, they're pure sinks.  Our routing algorithm will be a weighted random scatter: we have two dealers and we send twice as many messages to one as to the other.

[[code type="textdiagram"]]
          +-------------+
          |             |
          |   Client    |   Send to "A" or "B"
          |             |
          +-------------+
          |   ROUTER    |   OK, it's really XREP
          \-------------/
                 |
                 |
         +-------+-------+
         |               |
         |               |
         v               v
   /-----------\   /-----------\
   |  DEALER   |   |  DEALER   |   Aka. XREQ
   |    "A"    |   |    "B"    |
   +-----------+   +-----------+
   |           |   |           |
   |  Worker   |   |   Worker  |
   |           |   |           |
   +-----------+   +-----------+


Figure # - Router to dealer custom routing
[[/code]]

Here's code that shows how this works:

[[code type="C" title="Router-to-dealer" name="rtdealer"]]
[[/code]]

Some comments on this code:

* The router doesn't know when the dealers are ready, and it would be distracting for our example to add in the signaling to do that.  So the router just does a "sleep (1)" after starting the dealer threads.

To route to a dealer, we create an envelope like this:

[[code type="textdiagram"]]
            +---------------+
  Frame 1   |   Address     |
            +---------------+-------------------------+
  Frame 2   |   Data                                  |
            +-----------------------------------------+


       Figure # - Routing envelope for dealer
[[/code]]

The router removes the first frame, routes the second frame, which the dealer gets as-is.  If the dealer was to reply, we'd get back a similar envelope in two parts.

Something to note: if you use an invalid address, the router discards the message silently.  There is not much else it can do usefully.  In normal cases this either means the peer has gone away, or that there is a programming error somewhere and you're using a bogus address.  0MQ may in future report dropped messages via a sys://log bus, and may distinguish these two different cases.  In any case you cannot ever assume a message will be routed successfully until and unless you get a reply of some sorts from the destination node.  We'll come to creating reliable patterns later on.

Dealers look a bit like PULL sockets here and in fact they work exactly as PUSH and PULL combined.  It's illegal to connect PULL or PUSH to a request-reply socket, and pointless, so don't do it.

+++ Least-Recently Used Routing

Like we said, Mamas don't listen to you, and if you try to speak out of turn they'll ignore you.  You have to wait for them to say something, //then// you can give a sarcastic answer.  This is very useful for routing because it means we can keep a bunch of mamas waiting for answers.  In effect, mamas tell us when they're ready.

You can connect one router to many mamas, and distribute messages as you would to dealers.  Mamas will usually want to reply, but they will let you have the last word.  However it's one thing at a time:

* Mama speaks to router
* Router replies to mama
* Mama speaks to router
* Router replies to mama
* etc.

Like dealers, mamas can only talk to one router and since mamas always start by talking to the router, you should never connect one mama to more than one router unless you are doing sneaky stuff like multi-pathway redundant routing.  I'm not even going to explain that now, and hopefully the jargon is complex enough to stop you trying this until you need it.

[[code type="textdiagram"]]
          +-------------+
          |             |
          |   Client    |   Send to "A" or "B"
          |             |
          +-------------+
          |   ROUTER    |   OK, it's really XREP
          \-------------/
                 ^
                 |  (1) Mama says Hi
                 |
         +-------+-------+
         |               |
         |               |   (2) Router gives laundry
         v               v
   /-----------\   /-----------\
   |   MAMA    |   |   MAMA    |   Aka. REQ
   |    "A"    |   |    "B"    |
   +-----------+   +-----------+
   |           |   |           |
   |  Worker   |   |   Worker  |
   |           |   |           |
   +-----------+   +-----------+


Figure # - Router to mama custom routing
[[/code]]

What kind of routing can you do with a router-to-mama pattern?  Probably the most obvious is "least-recently-used" (LRU), where we always route to the mama that's been waiting longest.  Here is an example that does LRU routing to a set of mamas:

[[code type="C" title="Router-to-mama" name="rtmama"]]
[[/code]]

For this example the LRU doesn't need any particular data structures above what 0MQ gives us (message queues) because we don't need to synchronize the workers with anything.  A more realistic LRU algorithm would have to collect workers as they become ready, into a queue, and the use this queue when routing client requests.  We'll do this in a later example.

To prove that the LRU is working as expected, the mamas print the total tasks they each did.  Since the mamas do random work, and we're not load balancing, we expect each mama to do approximately the same amount but with random variation.  And that is indeed what we see:

[[code]]
Processed: 8 tasks
Processed: 8 tasks
Processed: 11 tasks
Processed: 7 tasks
Processed: 9 tasks
Processed: 11 tasks
Processed: 14 tasks
Processed: 11 tasks
Processed: 11 tasks
Processed: 10 tasks
[[/code]]

Some comments on this code

* We don't need any settle time, since the mamas explicitly tell the router when they are ready.
* We're generating our own identities here, as printable strings, using the zhelpers.h s_set_id function.  That's just to make our life a little simpler.  In a realistic application the mamas would be fully anonymous and then you'd call zmq_recv[3] and zmq_send[3] directly instead of the zhelpers s_recv() and s_send() functions, which can only handle strings.
* Worse, we're using //random// identities.  Don't do this in real code, please.  Randomized durable sockets are not good in real life.
* If you copy and paste example code without understanding it, you deserve what you get.  It's like watching Spiderman leap off the roof and then trying that yourself.

To route to a mama, we must create a mama-friendly envelope like this:

[[code type="textdiagram"]]
            +---------------+
  Frame 1   |   Address     |
            +---+-----------+
  Frame 2   |   |   <------ Empty message part
            +---+-------------------------------------+
  Frame 3   |   Data                                  |
            +-----------------------------------------+


       Figure # - Routing envelope for mama
[[/code]]

+++ Address-based Routing

Papas are, if we care about them at all, only there to answer questions.  And to pay the bills, fix the car when mama drives it into the garage wall, put up shelves, and walk the dog when it's raining.  But apart from that, papas are only there to answer questions.

In a classic request-reply pattern a router wouldn't talk to a papa socket at all, but rather would get a dealer to do the job for it.  That's what dealers are for: to pass questions onto random papas and come back with their answers.  Routers are generally more comfortable talking to mamas.  OK, dear reader, you may stop the psychoanalysis.  These are analogies, not life stories.

It's worth remembering with 0MQ that the classic patterns are the ones that work best, that the beaten path is there for a reason, and that when we go off-road we take the risk of falling off cliffs and getting eaten by zombies.  Having said that, let's plug a router into a papa and see what the heck emerges.

The special thing about papas, all joking aside, is actually two things:

* One, they are strictly lockstep request-reply.
* Two, they accept an envelope stack of any size and will return that intact.

In the normal request-reply pattern, papas are anonymous and replaceable (wow, these analogies //are// scary), but we're learning about custom routing.  So, in our use case we have reason to send a request to papa A rather than papa B.  This is essential if you want to keep some kind of a conversation going between you, at one end of a large network, and a papa sitting somewhere far away.

A core philosophy of 0MQ is that the edges are smart and many, and the middle is vast and dumb.  This does mean the edges can address each other, and this also means we want to know how to reach a given papa.  Doing routing across multiple hops is something we'll look at later but for now we'll look just at the final step: a router talking to a specific papa:

[[code type="textdiagram"]]
          +-------------+
          |             |
          |   Client    |   Send to "A" or "B"
          |             |
          +-------------+
          |   ROUTER    |   Yes, it's still XREP
          \-------------/
                 ^
                 |
                 |
         +-------+-------+
         |               |
         |               |
         v               v
   /-----------\   /-----------\
   |   PAPA    |   |   PAPA    |   REP, naturally
   |    "A"    |   |    "B"    |
   +-----------+   +-----------+
   |           |   |           |
   |  Worker   |   |   Worker  |
   |           |   |           |
   +-----------+   +-----------+


Figure # - Router to papa custom routing
[[/code]]

This example shows a very specific chain of events:

* The client has a message that it expects to route back (via another router) to some node.  The message has two addresses (a stack), an empty part, and a body.
* The client passes that to the router but specifies a papa address first.
* The router removes the papa address, uses that to decide which papa to send the message to.
* The papa receives the addresses, empty part, and body.
* It removes the addresses, saves them, and passes the body to the worker.
* The worker sends a reply back to the papa.
* The papa recreates the envelope stack and sends that back with the worker's reply to the router.
* The router prepends the papa's address and provides that to the client along with the rest of the address stack, empty part, and the body.

It's complex but worth working through until you understand it.  Just remember a papa is garbage in, garbage out.

[[code type="C" title="Router-to-papa" name="rtpapa"]]
[[/code]]

Run this program and it should show you this:

[[code]]
----------------------------------------
[020] This is the workload
----------------------------------------
[001] A
[009] address 3
[009] address 2
[009] address 1
[000]
[017] This is the reply
[[/code]]

Some comments on this code:

* In reality we'd have the papa and router in separate nodes.  This example does it all in one thread because it makes the sequence of events really clear.

* zmq_connect[3] doesn't happen instantly.  When the papa socket connects to the router, that takes a certain time and happens in the background.  In a realistic application the router wouldn't even know the papa existed until there had been some previous dialog.  In our toy example we'll just {{sleep (1);}} to make sure the connection's done.  If you remove the sleep, the papa socket won't get the message. (Try it.)

* We're routing using the papa's identity.  Just to convince yourself this really is happening, try sending to a wrong address, like "B".  The papa won't get the message.

* The s_dump and other utility functions (in the C code) come from the zhelpers.h header file.  It becomes clear that we do the same work over and over on sockets, and there are interesting layers we can build on top of the 0MQ API.  We'll come back to this later when we make a real application rather than these toy examples.

To route to a papa, we must create a papa-friendly envelope like this:

[[code type="textdiagram"]]
            +---------------+
  Frame 1   |   Address     |  <--- Zero or more of these
            +---+-----------+
  Frame 2   |   |   <------ Exactly one empty message part
            +---+-------------------------------------+
  Frame 3   |   Data                                  |
            +-----------------------------------------+


       Figure # - Routing envelope for papa
[[/code]]

+++ A Generic Routing Device

We'll recap the knowledge we have so far about doing weird stuff with 0MQ message envelopes, and build the core of a generic custom routing queue device.  Sorry for all the buzzwords.  What we'll make is a //queue device// that connects a bunch of //clients// to a bunch of //workers//, and lets you use //any routing algorithm// you want.  What we'll do is //least-recently used//, since it's the most obvious use case apart from load-balancing.

To start with, let's look back at the classic request-reply pattern and then see how it extends over a larger and larger service-oriented network.  The basic pattern is:

[[code type="textdiagram"]]
            +--------+
            | Client |
            +--------+
            |  Mama  |
            +--------+
                |
                |
    +-----------+-----------+
    |           |           |
    |           |           |
+--------+  +--------+  +--------+
|  Papa  |  |  Papa  |  |  Papa  |
+--------+  +--------+  +--------+
| Worker |  | Worker |  | Worker |
+--------+  +--------+  +--------+


  Figure # - Basic request-reply
[[/code]]

This extends to multiple papas, but if we want to handle multiple mamas as well we need a device in the middle, which normally consists of a router and a dealer back to back, connected by a classic ZMQ_QUEUE device that just copies message parts between the two sockets as fast as it can:

[[code type="textdiagram"]]
+--------+  +--------+  +--------+
| Client |  | Client |  | Client |
+--------+  +--------+  +--------+
|  Mama  |  |  Mama  |  |  Mama  |
+--------+  +--------+  +--------+
    |           |           |
    +-----------+-----------+
                |
            +--------+
            | Router |
            +--------+
            | Device |
            +--------+
            | Dealer |
            +--------+
                |
    +-----------+-----------+
    |           |           |
+--------+  +--------+  +--------+
|  Papa  |  |  Papa  |  |  Papa  |
+--------+  +--------+  +--------+
| Worker |  | Worker |  | Worker |
+--------+  +--------+  +--------+


Figure # - Extended request-reply
[[/code]]

The key here is that the router stores the originating mama address in the request envelope, the dealer and papas don't touch that, and so the router knows which mama to send the reply back to.  Papas are anonymous and not addressed in this pattern, all papas are assumed to provide the same service.

In this design, we're using the built-in load balancing routing that the dealer socket provides. If we want, for example to have least-recently used, we can take the router-mama pattern we learned, and apply that:

[[code type="textdiagram"]]
    +--------+  +--------+  +--------+
    | Client |  | Client |  | Client |
    +--------+  +--------+  +--------+
    |  Mama  |  |  Mama  |  |  Mama  |
    +--------+  +--------+  +--------+
        |           |           |
        +-----------+-----------+
                    |
                +--------+
                | Router |  Frontend
                +--------+
                | Device |  LRU queue
                +--------+
                | Router |  Backend
                +--------+
                    |
        +-----------+-----------+
        |           |           |
    +--------+  +--------+  +--------+
    |  Mama  |  |  Mama  |  |  Mama  |
    +--------+  +--------+  +--------+
    | Worker |  | Worker |  | Worker |
    +--------+  +--------+  +--------+


Figure # - Extended request-reply with LRU
[[/code]]

This router-to-router LRU queue device can't simply copy message parts blindly.  Here is the code, it's fairly complex but the core logic is reusable in any LRU queuing device:

[[code type="C" title="LRU queue device" name="lruqueue"]]
[[/code]]

The difficult part of this program is (a) the envelopes that each socket reads and writes, and (b) the LRU algorithm.  We'll take these in turn, starting with the message envelope formats.

First, recall that a mama socket always puts on an empty part (the envelope delimiter) on sending and removes this empty part on reception.  The reason for this isn't important, it's just part of the 'normal' request-reply pattern.  What we care about here is just keeping mama happy by doing precisely what she needs.  Second, the router always adds an envelope with the address of whomever the message came from.

We can now walk through a full request-reply chain from client to worker and back.  In the code we set the identity of client and worker sockets to make it easier to print the message frames if we want to.  Let's assume the client's identity is "CLIENT" and the worker's identity is "WORKER".  The client sends a single frame:

[[code type="textdiagram"]]
             +---+-------+
   Frame 1   | 5 | HELLO |       Data part
             +---+-------+


    Figure # - Message that client sends
[[/code]]

What the queue gets, when reading off the router frontend socket is this:

[[code type="textdiagram"]]
             +---+--------+
   Frame 1   | 6 | CLIENT |    Identity of client
             +---+--------+
   Frame 2   | 0 |               Empty message part
             +---+-------+
   Frame 3   | 5 | HELLO |       Data part
             +---+-------+


       Figure # - Message coming in on frontend
[[/code]]

The queue device sends this to the worker, prefixed by the address of the worker, taken from the LRU queue, plus an additional empty part to keep the mama at the other end happy:

[[code type="textdiagram"]]
             +---+--------+
   Frame 1   | 6 | WORKER |     Identity of worker
             +---+--------+
   Frame 2   | 0 |               Empty message part
             +---+--------+
   Frame 3   | 6 | CLIENT |    Identity of client
             +---+--------+
   Frame 4   | 0 |               Empty message part
             +---+-------+
   Frame 5   | 5 | HELLO |       Data part
             +---+-------+


          Figure # - Message sent to backend
[[/code]]

This complex envelope stack gets chewed up first by the backend router socket, which removes the first frame.  Then the mama socket in the worker removes the empty part, and provides the rest to the worker:

[[code type="textdiagram"]]
             +---+--------+
   Frame 1   | 6 | CLIENT |    Identity of client
             +---+--------+
   Frame 2   | 0 |               Empty message part
             +---+-------+
   Frame 3   | 5 | HELLO |       Data part
             +---+-------+


        Figure # - Message delivered to worker
[[/code]]

Which is exactly the same as what the queue received on its frontend router socket.  The worker has to save the envelope (which is all the parts up to and including the empty message part) and then it can do what's needed with the data part.

On the return path the messages are the same as when they come in, i.e. the backend socket gives the queue a message in five parts, and the queue sends the frontend socket a message in three parts, and the client gets a message in one part.

Now let's look at the LRU algorithm.  This is a reusable algorithm.  It requires that both clients and workers use mama sockets, and that workers correctly store and replay the envelope on messages they get.

The algorithm is:

* Create a pollset which polls the backend always, and the frontend only if there are one or more workers available.
* Poll for activity with infinite timeout.
* If there is activity on the backend, we either have a "ready" message or a reply for a client.  In either case we store the worker address (the first part) on our LRU queue, and if the rest is a client reply we send it back to that client via the frontend.
* If there is activity on the frontend, we take the client request, pop the next worker (which is the least-recently used), and send the request to the backend.  This means sending the worker address, empty part, and then the three parts of the client request.

You should now see that you can extend the LRU algorithm with variations based on the information the worker provides in its initial "ready" message.  For example, workers might start up and do a performance self-test, then tell the queue device how fast they are.  The queue can then choose the fastest available worker rather than LRU or round-robin.

.end

+++ Router-to-Router Connections

Our last case, router sockets talking to router sockets, is complex.  If the other peer is a sink, you'd just use a dealer.  If the other peer talks back but doesn't do routing, again a dealer would be enough.  So router-to-router only makes sense if the other router is going to talk back to you, and has to route between multiple "YOUs".  The difficulty is that routers can't initiate a dialog, only respond to one, because they need to know the identity of the peer they should speak to.  If two routers are both sitting waiting for the other to initiate a dialog... well, you see the problem.

We'll find a way to do this, but let's make it concrete with a realistic use case that we'll then solve.

We're going to build a large cloud computing facility (or rather, a tiny simulation of a large cloud).  It's composed of data centers that each have a bunch of workers and a bunch of clients.  The workers are fast boxes able to compute results.  The clients collect jobs from the outside world, pass them to workers and send the replies back out.

We don't connect clients and workers to each other because that makes it painful to add or remove clients and workers dynamically.  So our basic model consists of the generic router-to-mama routing device we just built:

[[code type="textdiagram"]]
    +--------+  +--------+  +--------+
    | Client |  | Client |  | Client |
    +--------+  +--------+  +--------+
    |  Mama  |  |  Mama  |  |  Mama  |
    +--------+  +--------+  +--------+
        |           |           |
        +-----------+-----------+
                    |
              +------------+
              |   Router   |
              +------------+
              | LRU Queue  |
              +------------+
              |   Router   |
              +------------+
                    |
        +-----------+-----------+
        |           |           |
    +--------+  +--------+  +--------+
    |  Mama  |  |  Mama  |  |  Mama  |
    +--------+  +--------+  +--------+
    | Worker |  | Worker |  | Worker |
    +--------+  +--------+  +--------+


    Figure # - Datacenter Architecture
[[/code]]

Now we scale this out to more than one data center.  Let's illustrate with two data centers, the most common case (one primary, one backup), but keep in mind that we want to solve the problem for N data centers, not just two.  Each data center has a set of clients and workers, and a router that joins these together as a LRU queue:

[[code type="textdiagram"]]

    Primary Data Center     :     Backup Data Center
                            :
                            :
    +---+  +---+  +---+     :     +---+  +---+  +---+
    | C |  | C |  | C |     :     | C |  | C |  | C |
    +---+  +---+  +---+     :     +---+  +---+  +---+
      |      |      |       :       |      |      |
      |      |      |       :       |      |      |
    +-----------------+     :     +-----------------+
    |     Router      |     :     |     Router      |
    +-----------------+     :     +-----------------+
      |      |      |       :       |      |      |
      |      |      |       :       |      |      |
    +---+  +---+  +---+     :     +---+  +---+  +---+
    | W |  | W |  | W |     :     | W |  | W |  | W |
    +---+  +---+  +---+     :     +---+  +---+  +---+
                            :

        Figure # - Primary and Backup Datacenters
[[/code]]

The question is: how do we get the clients of each data center talking to the workers of the other data center?  There are a few possibilities, each with pros and cons:

* Clients could connect directly to both routers.  The advantage is that we don't need to modify routers or workers.  But clients get more complex, and become aware of the overall topology.  If we want to add, e.g. a third or forth data center, all the clients are affected.  In effect we have to move routing and failover logic into the clients and that's not nice.
* Workers might connect directly to both routers.  But mama workers can't do that, they can only reply to one router.  We might use papas but papas don't give us customizable router-to-worker routing like LRU, only the built-in load balancing.  That's a fail, if we want to distribute work to idle workers: we precisely need LRU.  One solution would be to use router sockets for the worker nodes.  Let's label this "Idea #1".
* Routers could connect to each other.  This looks neatest because it creates the fewest additional connections.  We can't add data centers on the fly but that is probably out of scope.  Now clients and workers remain ignorant of the real network topology, and routers tell each other when they have spare capacity.  Let's label this "Idea #2".

Let's explore Idea #1.  Workers connecting to both routers and accepting jobs from either:

[[code type="textdiagram"]]

               Data center 1       :       Data center 2
                                   :
                                   :
              |            |               |            |
              +------------+               +------------+
              |   Router   |               |   Router   |
              +------------+               +------------+
                    |                            |
          +---------|-+--=--------+--------------+
          :         | :           :
        +-----------+-----------+ :
        | :         | :         | :
        | :         | :         | :
    +--------+  +--------+  +--------+
    | Router |  | Router |  | Router |
    +--------+  +--------+  +--------+
    | Worker |  | Worker |  | Worker |
    +--------+  +--------+  +--------+


         Figure # - Idea 1 - cross-connected workers
[[/code]]

It looks feasible.  However it doesn't provide what we wanted, which was that clients get local workers if possible and remote workers only if it's better than waiting.  Also workers will signal "ready" to both routers and can get two jobs at once, while other workers remain idle.  It seems this design fails because again we're putting routing logic at the edges.

So idea #2 then.  We interconnect the routers and don't touch the clients or workers, which are mamas like we're used to:

[[code type="textdiagram"]]

    Primary Data Center     :     Backup Data Center
                            :
                            :
    +---+  +---+  +---+     :     +---+  +---+  +---+
    | C |  | C |  | C |     :     | C |  | C |  | C |
    +---+  +---+  +---+     :     +---+  +---+  +---+
      |      |      |       :       |      |      |
      |      |      |       :       |      |      |
    +-----------------+     :     +-----------------+
    |     Router      |<--------->|     Router      |
    +-----------------+     :     +-----------------+
      |      |      |       :       |      |      |
      |      |      |       :       |      |      |
    +---+  +---+  +---+     :     +---+  +---+  +---+
    | W |  | W |  | W |     :     | W |  | W |  | W |
    +---+  +---+  +---+     :     +---+  +---+  +---+
                            :

    Figure # - Idea 2 - routers talking to each other
[[/code]]

This design is appealing because the problem is solved in one place, invisibly to the rest of the world.  Basically, routers open secret channels to each other and whisper, like camel traders, "//hey, I've got some spare capacity, if you have too many clients give me a shout and we'll deal".//

It is in effect just a more sophisticated routing algorithm: routers become subcontractors for each other.  Other things to like about this design, even before we play with real code:

* It treats the common case (clients and workers on the same cluster) as default and does extra work for the exceptional case (shuffling jobs between data centers).
* It lets us use different message flows for the different types of work.  That means we can handle them differently, e.g. using different types of network connection.
* It feels like it would scale smoothly.  Interconnecting three, or more routers doesn't get over-complex.  If we find this to be a problem, it's easy to solve by adding a super-router.

We'll now make a worked example.  We'll pack an entire data center into one process.  That is obviously not realistic but it makes it simple to simulate, and the simulation can accurately scale to real processes.  This is the beauty of 0MQ, you can design at the microlevel and scale that up to the macro level.  Thread become processes, become boxes and the patterns and logic remain the same.  Each of our 'data center' processes contains client threads, worker threads, and a router thread.

We know the basic model well by now:

* The client threads create workloads and pass them to the router.  It uses a mama (REQ) socket and a request-reply flow.
* The worker threads process workloads and return the results to the router.  It uses a mama (REQ) socket and a ready-go flow as we saw in the LRU routing model.
* The router queues and distributes workloads using the LRU routing model.

There are several ways of interconnecting the routers.  What we //want// is to be able to tell other routers, "we have capacity", and then receive multiple tasks.  We also need to be able to tell other routers "stop, we're full".  It doesn't need to be perfect: sometimes we may accept jobs we can't process immediately, then we'll do them as soon as possible.

The simplest interconnect is //federation// in which each broker acts as a worker for the other.  We would do this by connecting our frontend to the other broker's backend socket.  Note that it is legal to both bind a socket to an endpoint and connect it to other endpoints.

[[code type="textdiagram"]]

    Primary Data Center     :     Backup Data Center
                            :
                            :
    +---+  +---+            :             +---+  +---+
    | C |  | C |            :             | C |  | C |
    +---+  +---+    +----+  :  +-----+    +---+  +---+
      |      |      |    |  :  |     |      |      |
      |      |      |    |  :  |     |      |      |
    +-----------------+  |  :  |   +-----------------+
    |     Router      |  |  :  |   |     Router      |
    +-----------------+  |  :  |   +-----------------+
      |      |      ^    |  :  |     ^      |      |
      |      |      |    |  :  |     |      |      |
    +---+  +---+    |    +-----------+    +---+  +---+
    | W |  | W |    |       :  |          | W |  | W |
    +---+  +---+    +----------+          +---+  +---+
                            :

   Figure # - Cross-connected routers in federation model
[[/code]]

This would give us simple logic in both routers and a reasonably good mechanism: when there are no clients, tell the other broker 'ready', and accept one job from it.  The problem is also that it's too simple, able to handle only one task at once.  If we have lots of available workers they won't be used.

The federation model is perfect for other kinds of routing, especially server-oriented architectures (routing by service name and proximity rather than LRU or load-balancing or random scatter) and we'll come back to that later.  So don't dismiss it as useless, it's just not right for least-recently used and cluster load-balancing.

What we need is an asynchronous routable flow of information between brokers.  This means we need a separate set of sockets, capable of asynchronous routing, and this router sockets:

[[code type="textdiagram"]]

       Primary Data Center        :        Backup Data Center
                                  :
                                  :
    +---+  +---+  +---+           :           +---+  +---+  +---+
    | C |  | C |  | C |           :           | C |  | C |  | C |
    +---+  +---+  +---+           :           +---+  +---+  +---+
      |      |      |             :             |      |      |
      |      |      |             :             |      |      |
    +-----------------+-----+     :     +-----+-----------------+
    |     Router      | ??? |<--------->| ??? |     Router      |
    +-----------------+-----+     :     +-----+-----------------+
      |      |      |             :             |      |      |
      |      |      |             :             |      |      |
    +---+  +---+  +---+           :           +---+  +---+  +---+
    | W |  | W |  | W |           :           | W |  | W |  | W |
    +---+  +---+  +---+           :           +---+  +---+  +---+
                                  :

            Figure # - And Router shall speak unto Router...
[[/code]]

With two data centers we could make an asymmetric flow, with one router binding and the other connecting.  But this does not scale.  It would be nicer that we have two sockets, one for incoming data, that we bind to, and one for outgoing data, that we connect.  It gives us a client-server model that is consistent with clients connecting, and servers binding.

Each router now manages //four// XREP router sockets.  Clear naming conventions will save us from categorical confusion, leaving just enough background bafflement to make this magic worth doing:

* The //frontend// socket talks to the clients;
* The //backend// socket talks to the workers;
* The //inward// socket talks to master routers and accepts tasks;
* The //onward// socket talks to slave routers and passes on tasks.

When a router starts up it binds its onward socket, and connects its inward socket to all other routers.  When it has idle workers, it scatters this out to



It now needs to tell each of its masters that it's ready.  But it doesn't know their identities.

/






- worked eexample
- single point of failure
- presence detection

- router trees
 (federation)
-



.end

+++ Presence Detection

- peers come and go
- if we want to route to them explicitly, we need to know if they are present
- heartbeating, liveness, etc.
- fresh data, failover, etc.
- purging old identities from routing tables
- example of eight robots and console
- robots come and go...


+++ A Service-Oriented Queue Device

- how to route to workers based on service names
- kind of like custom pubsub but with answers going back to clients


+++ A ZeroMQ Name Service

- name service
    translate logical name into connect or bind string
    service runs locally, connect via icp://zns
    gets name updates asynchronously from central server
    also local zns lookup file
    using zpl syntax
    pubsub state / refresh example

how to map names?
    - XXX -> tcp://lo:5050 if I'm on server 1
    - XXX -> tcp://somename:5050
    -> does ZMQ do host lookup?  Doesn't seem like it...
    -> resolve host...







+++ Generating Identities

-> need zfl here

- generic LRU device
    - accept input from REQs
    - load balance to bunch of mamas

lrudevice.c
    - imitates a queue device except...




 >> [/chapter:2 Return to Chapter 2 of The Guide]
 >> [/chapter:4 Continue reading Chapter 4 of The Guide]
